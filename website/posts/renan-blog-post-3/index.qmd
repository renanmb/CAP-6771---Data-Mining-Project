---
title: "Project 2 - Data Cubbing and Binning"
description: "In this post apply binning to pre-process the dataset"
author:
  - name: Renan monteiro barbosa
    url: https://github.com/renanmb
    affiliation: Master of Data Science Program @ The University of West Florida (UWF)
    # affiliation-url: https://ucsb-meds.github.io/
# date: 10-24-2022
categories: [renan]
# citation:
#   url: https://samanthacsik.github.io/posts/2022-10-24-my-blog-post/
image: images/spongebob-imagination.jpg
draft: false
bibliography: references.bib
link-citations: true
---

For the Project 2 we further processed our dataset by leveraging binning and a **data cube** structure built on **hierarchies** .


## 1. Introduction

Analyzing the `State_time_series.csv` dataset, which contains granular real estate metrics over time, benefits significantly from data transformation and aggregation. **Binning** is essential for this process, as it converts continuous variables, such as `MedianListingPrice_AllHomes`, into discrete, manageable categories (e.g., '$150k-$200k', '$200k-$250k'). This discretization simplifies complex data, making it easier to summarize, visualize, and identify trends.

Furthermore, leveraging a **data cube** structure built on **hierarchies** allows for powerful, multidimensional analysis. For instance, the `Date` field isn't just a single point in time; it's part of a hierarchy that can be "rolled up" from a specific day to a **Month**, **Quarter**, or **Year**. Similarly, the `RegionName` (State) could be aggregated into broader geographical regions (e.g., "Northeast", "West Coast"). By combining these binned and hierarchical dimensions, we can quickly "slice and dice" the data to answer complex questions, such as "How many homes in the $200k-$250k price bin were available in the *Northeast* region during *Q3 2018*?" This turns a massive, raw dataset into a flexible tool for gaining actionable insights.


The Zillow Home Value Index or ZHVI is a smoothed, seasonally adjusted measure of the typical home value and market changes across a given region and housing type. It reflects the typical value for homes in the 35th to 65th percentile range.

The Dataset [Zillow Economics Data](https://www.kaggle.com/datasets/zillow/zecon) @ZHVI, can be downloaded:

```{{bash}}
#!/bin/bash
curl -L -o ~/Downloads/zecon.zip\
  https://www.kaggle.com/api/v1/datasets/download/zillow/zecon
```

<!-- TODO Notes -->

<!-- 
Talk about how this relates to the Association rules and how the reocurrence of some features indicates paths of interest.
ZHVI is already a binned value. 
-->


### 1.1 Load packages

```{r}
#| output: false

# install.packages("fpp3")
library(fpp3)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggtime)
library(patchwork)
library(DT)
```

Possible errors makings notes:

```{{bash}}
Attaching package: ‘ggtime’

The following objects are masked from ‘package:feasts’:

    gg_arma, gg_irf, gg_lag, gg_season, gg_subseries, gg_tsdisplay,
    gg_tsresiduals
```

## 2. Load the Dataset

The following code will locate the folder **datasets** and then it will give back the variable **datasets_path** which you can use to build the path to the desired data to be loaded.

```{r}
#| code-fold: true
#| output: false
find_git_root <- function(start = getwd()) {
  path <- normalizePath(start, winslash = "/", mustWork = TRUE)
  while (path != dirname(path)) {
    if (dir.exists(file.path(path, ".git"))) return(path)
    path <- dirname(path)
  }
  stop("No .git directory found — are you inside a Git repository?")
}

repo_root <- find_git_root()
datasets_path <- file.path(repo_root, "datasets")
zillow_economics_data_path <- file.path(datasets_path, "zillow-economics-data-01")

state_time_series <- file.path(zillow_economics_data_path, "State_time_series.csv")
all_states_data <- read.csv(state_time_series)
```

## 3. Data Exploration and Processing

The 'Date' column is a character and should be converted to a Date object.

```{r}
all_states_data$Date <- as.Date(all_states_data$Date)
# str(all_states_data)
```

Exploring the N/As and the structure of the dataset:

```{r}
# Get the total number of rows for calculating percentages
total_rows <- nrow(all_states_data)

# Create a pretty summary table
na_summary <- all_states_data %>%
  # 1. Count NAs for every column
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  
  # 2. Pivot the data from wide to long
  pivot_longer(everything(),
               names_to = "Column",
               values_to = "NA_Count") %>%
  
  # 3. (Optional) Filter to only show columns that HAVE NAs
  filter(NA_Count > 0) %>%
  
  # 4. (Optional) Add a percentage column
  mutate(NA_Percentage = (NA_Count / total_rows) * 100) %>%

  # 5. Sort by the highest NA count
  arrange(desc(NA_Count))

# Print the clean table
print(na_summary)
```

Sample of the Dataset in the form of a data table to make visually clear how the dataset looks like and what features are available.

```{r}
print("all_states_data (tsibble):")
DT::datatable(all_states_data, options = list(pageLength = 5))
```

```{r}
library(skimr)
skim(all_states_data)
```

<!-- 
⣿⣿⡇⢩⠘⣴⣿⣥⣤⢦⢁⠄⠉⡄⡇⠛⠛⠛⢛⣭⣾⣿⣿⡏
⣿⣿⣿⡇⠹⢇⡹⣿⣿⣛⣓⣿⡿⠞⠑⣱⠄⢀⣴⣿⣿⣿⣿⡟
⣿⣿⣿⣧⣸⡄⣿⣪⡻⣿⠿⠋⠄⠄⣀⣀⢡⣿⣿⣿⣿⡿⠋
⠘⣿⣿⣿⣿⣷⣭⣓⡽⡆⡄⢀⣤⣾⣿⣿⣿⣿⣿⡿⠋
⠄⢨⡻⡇⣿⢿⣿⣿⣭⡶⣿⣿⣿⣜⢿⡇⡿⠟⠉
⠄⠸⣷⡅⣫⣾⣿⣿⣿⣷⣙⢿⣿⣿⣷⣦⣚⡀
⠄⠄⢉⣾⡟⠙⠶⠖⠈⢻⣿⣷⣅⢻⣿⣿⣿⣿⣿⣶⣶⡆⠄⣤⡀
⠄⢠⣿⣿⣧⣀⣀⣀⣀⣼⣿⣿⣿⡎⢿⣿⣿⣿⣿⣿⣿⣇⠄⠈⠁
⠄⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⢇⣎⢿⣿⣿⣿⣿⣿⣿⣿⣶⣶
⠄⠄⠻⢿⣿⣿⣿⣿⣿⣿⣿⢟⣫⣾⣿⣷⡹⣿⣿⣿⣿⣿⣿⣿⡟
⠄⠄⠄⠄⢮⣭⣍⡭⣭⡵⣾⣿⣿⣿⡎⣿⣿⣌⠻⠿⠿⠿⠟⠋
⠄⠄⠄⠄⠈⠻⣿⣿⣿⣿⣹⣿⣿⣿⡇⣿⣿⡿
⠄⠄⣀⣴⣾⣶⡞⣿⣿⣿⣿⣿⣿⣿⣾⣿⡿ 
-->

```{r}
# --- 1. Create Date Hierarchy ---
# We parse the Date and extract hierarchy levels
processed_data <- all_states_data %>%
  mutate(
    Year = year(Date),
    Quarter = quarter(Date, with_year = TRUE),
    Month = month(Date, label = TRUE)
  )
```


```{r}
# --- 2. Create Bins for a Measure ---
# We bin 'MedianListingPrice_AllHomes' into categories
price_breaks <- c(0, 150000, 200000, 250000, 300000, 500000, Inf)
price_labels <- c(
  "Under $150k", "$150k-$200k", "$200k-$250k",
  "$250k-$300k", "$300k-$500k", "Over $500k"
)
```

```{r}
processed_data <- processed_data %>%
  mutate(
    Price_Bin = cut(MedianListingPrice_AllHomes,
                    breaks = price_breaks,
                    labels = price_labels,
                    right = FALSE) # Bins are [min, max)
  )
```

```{r}
# Show the new hierarchical and binned columns
print("Data with new hierarchy and bins:")
glimpse(processed_data %>%
  select(RegionName, Date, Year, Quarter, Month, MedianListingPrice_AllHomes, Price_Bin))
```


## 4. Explore ZHVI with Date Hierarchy

For sake of simplicity we are going to focus for now on a single State (California) and the Zillow Home Value Index (ZHVI_AllHomes) over time.

```{r}
# Let's analyze the Zillow Home Value Index (ZHVI_AllHomes) for "California".
# all_states_data
cali_zhvi <- processed_data %>%
  filter(RegionName == "California") %>%
  select(Date, ZHVI_AllHomes) %>%
  # Remove any missing values for this metric
  na.omit()

cali_ts <- cali_zhvi %>%
  as_tsibble(index = Date)

```


```{r}
# We set 'Date' as the 'index' (the time component).
cali_ts_fill_gaps <- cali_zhvi %>%
  as_tsibble(index = Date) %>%
  fill_gaps()
# If you were analyzing multiple states, you would add a 'key'.
# Example for multiple states (not run here):
# multi_state_ts <- all_states_data %>%
#   select(Date, RegionName, ZHVI_AllHomes) %>%
#   as_tsibble(index = Date, key = RegionName)
```

Lets see how many N/As after filling the gaps:

```{r}
#| output: false
# Count total NAs per column
colSums(is.na(cali_ts))
colSums(is.na(cali_ts_fill_gaps))
```

```{r}
# Inspect the new, focused data frame
head(cali_zhvi)
head(cali_ts)
head(cali_ts_fill_gaps)
```



```{r}
cali_ts %>%
  autoplot(ZHVI_AllHomes) +
  labs(title = "Zillow Home Value Index (ZHVI) for California",
       y = "Home Value Index",
       x = "Year") +
  theme_minimal()
```

```{r}
# This will scan cali_ts and report any gaps
gap_summary <- count_gaps(cali_ts)

# Print the summary
print(gap_summary)
```

```{r}
cali_ts_fill_gaps %>%
  autoplot(ZHVI_AllHomes) +
  labs(title = "Zillow Home Value Index (ZHVI) for California",
       y = "Home Value Index",
       x = "Year") +
  theme_minimal()
```


```{r}
# Inspect the new, focused data frame
# print("cali_zhvi (data frame):")
# DT::datatable(cali_zhvi, options = list(pageLength = 5))

print("cali_ts (tsibble):")
DT::datatable(cali_ts, options = list(pageLength = 5))

# print("cali_ts_fill_gaps (tsibble with filled gaps):")
# DT::datatable(cali_ts_fill_gaps, options = list(pageLength = 5))
```

1. Create a new 'Month' column using the yearmonth() function
2. Group by this new explicit month
3. Summarise the data (using mean() is safe, but since you have one observation per month, last() or sum() would also work)
4. Convert to a tsibble, now indexed by the new 'Month' object

```{r}
cali_ts_monthly <- cali_zhvi %>%
  mutate(Month = yearmonth(Date)) %>%
  group_by(Month) %>%
  summarise(ZHVI_AllHomes = mean(ZHVI_AllHomes, na.rm = TRUE)) %>%
  as_tsibble(index = Month)

# Check the new object
print("New Monthly Tsibble:")
head(cali_ts_monthly)

# Note the <mth> tag in the output, indicating it's a monthly tsibble
print("Class of the new index:")
class(cali_ts_monthly$Month)
```


```{r}
cali_ts_monthly %>%
  autoplot(ZHVI_AllHomes) +
  labs(title = "Zillow Home Value Index (ZHVI) for California",
       y = "Home Value Index",
       x = "Year") +
  theme_minimal()
```

In R, Date objects are stored internally as the number of days that have passed since an "origin" date, which by default is January 1, 1970.

The number 9587 is the number of days since 1970-01-01.

This corresponds to the date April 30, 1996, which is the starting point of your Zillow dataset.

This happens if you (or R) accidentally convert a Date object to a plain number.

The yearmonth object we created in the last step is different. It prints as "1996 Apr" and internally stores the number of months since the 1970 epoch (which would be a much smaller number, like 316).

```{r}
print("cali_ts_monthly (tsibble with only monthly):")
DT::datatable(cali_ts_monthly, options = list(pageLength = 5))
```

<!-- 
⠄⠄⣿⣿⣿⣿⠘⡿⢛⣿⣿⣿⣿⣿⣧⢻⣿⣿⠃⠸⣿⣿⣿⠄⠄⠄⠄⠄
⠄⠄⣿⣿⣿⣿⢀⠼⣛⣛⣭⢭⣟⣛⣛⣛⠿⠿⢆⡠⢿⣿⣿⠄⠄⠄⠄⠄
⠄⠄⠸⣿⣿⢣⢶⣟⣿⣖⣿⣷⣻⣮⡿⣽⣿⣻⣖⣶⣤⣭⡉⠄⠄⠄⠄⠄
⠄⠄⠄⢹⠣⣛⣣⣭⣭⣭⣁⡛⠻⢽⣿⣿⣿⣿⢻⣿⣿⣿⣽⡧⡄⠄⠄⠄
⠄⠄⠄⠄⣼⣿⣿⣿⣿⣿⣿⣿⣿⣶⣌⡛⢿⣽⢘⣿⣷⣿⡻⠏⣛⣀⠄⠄
⠄⠄⠄⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⠙⡅⣿⠚⣡⣴⣿⣿⣿⡆⠄
⠄⠄⣰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⠄⣱⣾⣿⣿⣿⣿⣿⣿⠄
⠄⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿APPROVED⣿⣿⣿⣿⣿⠄
⠄⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠣⣿⣿⣿⣿⣿⣿⣿⣿⣿⠄
⠄⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠛⠑⣿⣮⣝⣛⠿⠿⣿⣿⣿⣿⠄
⢠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⠄⠄⠄⠄⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠄
-->


<!-- TODO -->

Reflecting on how the ZHVI is calculated and how it compares to other ways to calculate Home value index like: Case-Shiller

The Standard & Poor's CoreLogic Case–Shiller Home Price Indices are repeat-sales house price indices for the United States.

[FipeZAP Index](https://downloads.fipe.org.br/indices/fipezap/metodologia/FipeZap%20Index%20-%20Methodology%20v20110216.pdf) 



**Conceptual Accuracy (What it gets right)**

The R code correctly captures the concept of measuring the middle-third of the market:

Focus on the Middle Tier: The core idea of calculating the ZHVI is to use an aggressively trimmed mean—specifically the mean of values between the 35th and 65th percentile. The code successfully identifies these cut-off points ($\mathbf{q35}$ and $\mathbf{q65}$) and calculates the mean of the values within that range. This properly simulates the "robust" nature of the ZHVI, which aims to exclude extreme high-end and low-end sales that might skew a simple median or average.

**Major Flaws (What it gets wrong)**

1. The Data Source is Incorrect (Sales vs. All Homes)

* **Your Code's Flaw:** The input data, hypothetical_sales, is a list of sales transactions.

* **Actual ZHVI Method:** The ZHVI is calculated using the Zestimate for every single home (over 100 million properties), not just the few that sold in a given month.
  * **Impact:** Indices based on sales (like **Case-Shiller**) are susceptible to the compositional bias of which homes happen to sell that month. The ZHVI avoids this by including the estimated value (Zestimate) for all homes, providing a much more comprehensive view of the entire market's value, whether a house sold or not.

2. It Calculates a Static Value, Not Appreciation

* **Your Code's Flaw:** The output, **simulated_zhvi**, is a single-month dollar value.

* **Actual ZHVI Method:** The ZHVI is an **index** that tracks the change in home values over time (appreciation). It is a chained index where the monthly value is determined by:

  1. Calculating the weighted mean appreciation of individual Zestimates from the prior month to the current month.

  2. Applying that appreciation rate to the ZHVI level from the prior month.

  * **Impact:** To calculate a correct ZHVI, you would need two separate sets of Zestimates (Time $t-1$ and Time $t$), calculate the percentage change for each home, and then average those changes to determine the appreciation factor for the overall index. The code calculates a robust mean, but it doesn't show how that mean changes month-over-month to create the index.

**Hypothetical Fix for Flaw 2 (Conceptual only)**

A conceptually more accurate approach would require comparing two months:
1. Month 1 Zestimates (t-1): Calculate the $\text{Mean}_{35-65}$ of Zestimates for Month 1.

2. Month 2 Zestimates (t): Calculate the $\text{Mean}_{35-65}$ of Zestimates for Month 2.

3. Appreciation:

$$\text{Monthly Appreciation} = \frac{\text{Mean}_{35-65}(t)}{\text{Mean}_{35-65}(t-1)} - 1$$

4. ZHVI Calculation: $\text{ZHVI}(t) = \text{ZHVI}(t-1) \times (1 + \text{Monthly Appreciation})$



```{r}
# this seems to be incorrect

# 1. Create a hypothetical set of 100 home sales.
# We'll use random numbers for this example.
set.seed(42) # Makes our "random" numbers reproducible
hypothetical_sales <- round(rnorm(100, mean = 350000, sd = 75000))

# 2. Find the 35th and 65th percentile values
# These are the "cut-off" points.
q35 <- quantile(hypothetical_sales, 0.35)
q65 <- quantile(hypothetical_sales, 0.65)

# 3. Filter to get only the "middle-tier" homes
# (i.e., homes with a value between the 35th and 65th percentile)
middle_tier_homes <- hypothetical_sales[
  hypothetical_sales >= q35 & hypothetical_sales <= q65
]

# 4. Calculate the "Simulated ZHVI"
# This is the mean of only those middle-tier homes.
simulated_zhvi <- mean(middle_tier_homes)

# --- Print the results ---
print(paste("Total number of hypothetical sales:", length(hypothetical_sales)))
print(paste("35th Percentile Value:", q35))
print(paste("65th Percentile Value:", q65))

print(paste("Number of homes in middle-tier (35th-65th percentile):", length(middle_tier_homes)))
print(paste("Simulated ZHVI (Mean of middle-tier):", round(simulated_zhvi, 2)))
```


ZHVI is probably being calculated from most likely the sales data that generated the MedianListingPrice_AllHomes.

MedianListingPrice_1Bedroom	

MedianListingPrice_2Bedroom

MedianListingPrice_3Bedroom	

MedianListingPrice_4Bedroom	

MedianListingPrice_5BedroomOrMore	

MedianListingPrice_AllHomes	

MedianListingPrice_CondoCoop	

MedianListingPrice_DuplexTriplex	

MedianListingPrice_SingleFamilyResidence



### References

::: {#refs}
:::
