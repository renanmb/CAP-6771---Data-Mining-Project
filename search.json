[
  {
    "objectID": "people/barbosa-renan/index.html#education",
    "href": "people/barbosa-renan/index.html#education",
    "title": "Renan Monteiro Barbosa",
    "section": "Education",
    "text": "Education\nB.S. Mechanical Engineering | University of West Florida"
  },
  {
    "objectID": "people/subedi-binita/index.html#education",
    "href": "people/subedi-binita/index.html#education",
    "title": "Binita Subedi",
    "section": "Education",
    "text": "Education\nMSC Cyber Security | University of West Florida"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "Welcome to our team! We are a group of collaborators dedicated to…\n\n\n\nRenan Monteiro Barbosa\n\nData Scientist\n\n\n\nRenan Monteiro Barbosa\n\n\nJane is a data scientist with over 10 years of experience…\nContact * Email: jane@example.com * GitHub: https://github.com/renanmb\n\n\n\nBinita Subedi\n\nAdd description here\n\n\n\nBinita Subedi\n\n\nBinita Subedi is a software engineer specializing in web development…\nContact * Email: bs174@students.uwf.edu * GitHub: https://github.com/Binita-subedi\n\n\n\nMatthew Ray\n\nAdd description here\n\n\n\nMatthew Ray\n\n\nMatthew Ray works on developing new research methodologies for…\nContact * Email: mr168@students.uwf.edu * Website: alex-johnson.com"
  },
  {
    "objectID": "posts/renan-blog-post-3/index.html",
    "href": "posts/renan-blog-post-3/index.html",
    "title": "Project 2 - Data Cubbing and Binning",
    "section": "",
    "text": "For the Project 2 we further processed our dataset by leveraging binning and a data cube structure built on hierarchies ."
  },
  {
    "objectID": "posts/renan-blog-post-3/index.html#introduction",
    "href": "posts/renan-blog-post-3/index.html#introduction",
    "title": "Project 2 - Data Cubbing and Binning",
    "section": "1. Introduction",
    "text": "1. Introduction\nAnalyzing the State_time_series.csv dataset, which contains granular real estate metrics over time, benefits significantly from data transformation and aggregation. Binning is essential for this process, as it converts continuous variables, such as MedianListingPrice_AllHomes, into discrete, manageable categories (e.g., ‘$150k-$200k’, ‘$200k-$250k’). This discretization simplifies complex data, making it easier to summarize, visualize, and identify trends.\nFurthermore, leveraging a data cube structure built on hierarchies allows for powerful, multidimensional analysis. For instance, the Date field isn’t just a single point in time; it’s part of a hierarchy that can be “rolled up” from a specific day to a Month, Quarter, or Year. Similarly, the RegionName (State) could be aggregated into broader geographical regions (e.g., “Northeast”, “West Coast”). By combining these binned and hierarchical dimensions, we can quickly “slice and dice” the data to answer complex questions, such as “How many homes in the $200k-$250k price bin were available in the Northeast region during Q3 2018?” This turns a massive, raw dataset into a flexible tool for gaining actionable insights.\nThe Zillow Home Value Index or ZHVI is a smoothed, seasonally adjusted measure of the typical home value and market changes across a given region and housing type. It reflects the typical value for homes in the 35th to 65th percentile range.\nThe Dataset Zillow Economics Data[ZHVI?], can be downloaded:\n```{bash}\n#!/bin/bash\ncurl -L -o ~/Downloads/zecon.zip\\\n  https://www.kaggle.com/api/v1/datasets/download/zillow/zecon\n```\n\n\n\n1.1 Load packages\n\n# install.packages(\"fpp3\")\nlibrary(fpp3)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggtime)\nlibrary(patchwork)\nlibrary(DT)\n\nPossible errors makings notes:\n```{bash}\nAttaching package: ‘ggtime’\n\nThe following objects are masked from ‘package:feasts’:\n\n    gg_arma, gg_irf, gg_lag, gg_season, gg_subseries, gg_tsdisplay,\n    gg_tsresiduals\n```"
  },
  {
    "objectID": "posts/renan-blog-post-3/index.html#load-the-dataset",
    "href": "posts/renan-blog-post-3/index.html#load-the-dataset",
    "title": "Project 2 - Data Cubbing and Binning",
    "section": "2. Load the Dataset",
    "text": "2. Load the Dataset\nThe following code will locate the folder datasets and then it will give back the variable datasets_path which you can use to build the path to the desired data to be loaded.\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nzillow_economics_data_path &lt;- file.path(datasets_path, \"zillow-economics-data-01\")\n\nstate_time_series &lt;- file.path(zillow_economics_data_path, \"State_time_series.csv\")\nall_states_data &lt;- read.csv(state_time_series)"
  },
  {
    "objectID": "posts/renan-blog-post-3/index.html#data-exploration-and-processing",
    "href": "posts/renan-blog-post-3/index.html#data-exploration-and-processing",
    "title": "Project 2 - Data Cubbing and Binning",
    "section": "3. Data Exploration and Processing",
    "text": "3. Data Exploration and Processing\nThe ‘Date’ column is a character and should be converted to a Date object.\n\nall_states_data$Date &lt;- as.Date(all_states_data$Date)\n# str(all_states_data)\n\nExploring the N/As and the structure of the dataset:\n\n# Get the total number of rows for calculating percentages\ntotal_rows &lt;- nrow(all_states_data)\n\n# Create a pretty summary table\nna_summary &lt;- all_states_data %&gt;%\n  # 1. Count NAs for every column\n  summarise(across(everything(), ~sum(is.na(.)))) %&gt;%\n  \n  # 2. Pivot the data from wide to long\n  pivot_longer(everything(),\n               names_to = \"Column\",\n               values_to = \"NA_Count\") %&gt;%\n  \n  # 3. (Optional) Filter to only show columns that HAVE NAs\n  filter(NA_Count &gt; 0) %&gt;%\n  \n  # 4. (Optional) Add a percentage column\n  mutate(NA_Percentage = (NA_Count / total_rows) * 100) %&gt;%\n\n  # 5. Sort by the highest NA count\n  arrange(desc(NA_Count))\n\n# Print the clean table\nprint(na_summary)\n\n# A tibble: 80 × 3\n   Column                                  NA_Count NA_Percentage\n   &lt;chr&gt;                                      &lt;int&gt;         &lt;dbl&gt;\n 1 PctOfHomesSellingForGain_AllHomes          12609          95.4\n 2 PctOfHomesSellingForLoss_AllHomes          12609          95.4\n 3 MedianRentalPrice_5BedroomOrMore           11994          90.8\n 4 MedianRentalPricePerSqft_5BedroomOrMore    11752          88.9\n 5 MedianRentalPricePerSqft_Studio            10875          82.3\n 6 MedianRentalPrice_CondoCoop                10437          79.0\n 7 MedianRentalPricePerSqft_DuplexTriplex     10293          77.9\n 8 MedianRentalPrice_Studio                   10211          77.3\n 9 MedianListingPrice_1Bedroom                10205          77.2\n10 MedianRentalPrice_DuplexTriplex            10068          76.2\n# ℹ 70 more rows\n\n\n\n\n# --- 1. Create Date Hierarchy ---\n# We parse the Date and extract hierarchy levels\nprocessed_data &lt;- all_states_data %&gt;%\n  mutate(\n    Year = year(Date),\n    Quarter = quarter(Date, with_year = TRUE),\n    Month = month(Date, label = TRUE)\n  )\n\n\n# --- 2. Create Bins for a Measure ---\n# We bin 'MedianListingPrice_AllHomes' into categories\nprice_breaks &lt;- c(0, 150000, 200000, 250000, 300000, 500000, Inf)\nprice_labels &lt;- c(\n  \"Under $150k\", \"$150k-$200k\", \"$200k-$250k\",\n  \"$250k-$300k\", \"$300k-$500k\", \"Over $500k\"\n)\n\n\nprocessed_data &lt;- processed_data %&gt;%\n  mutate(\n    Price_Bin = cut(MedianListingPrice_AllHomes,\n                    breaks = price_breaks,\n                    labels = price_labels,\n                    right = FALSE) # Bins are [min, max)\n  )\n\n\n# Show the new hierarchical and binned columns\nprint(\"Data with new hierarchy and bins:\")\n\n[1] \"Data with new hierarchy and bins:\"\n\nglimpse(processed_data %&gt;%\n  select(RegionName, Date, Year, Quarter, Month, MedianListingPrice_AllHomes, Price_Bin))\n\nRows: 13,212\nColumns: 7\n$ RegionName                  &lt;chr&gt; \"Alabama\", \"Arizona\", \"Arkansas\", \"Califor…\n$ Date                        &lt;date&gt; 1996-04-30, 1996-04-30, 1996-04-30, 1996-…\n$ Year                        &lt;dbl&gt; 1996, 1996, 1996, 1996, 1996, 1996, 1996, …\n$ Quarter                     &lt;dbl&gt; 1996.2, 1996.2, 1996.2, 1996.2, 1996.2, 19…\n$ Month                       &lt;ord&gt; Apr, Apr, Apr, Apr, Apr, Apr, Apr, Apr, Ap…\n$ MedianListingPrice_AllHomes &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Price_Bin                   &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…"
  },
  {
    "objectID": "posts/renan-blog-post-3/index.html#explore-zhvi-with-date-hierarchy",
    "href": "posts/renan-blog-post-3/index.html#explore-zhvi-with-date-hierarchy",
    "title": "Project 2 - Data Cubbing and Binning",
    "section": "4. Explore ZHVI with Date Hierarchy",
    "text": "4. Explore ZHVI with Date Hierarchy\nFor sake of simplicity we are going to focus for now on a single State (California) and the Zillow Home Value Index (ZHVI_AllHomes) over time.\n\n# Let's analyze the Zillow Home Value Index (ZHVI_AllHomes) for \"California\".\n# all_states_data\ncali_zhvi &lt;- processed_data %&gt;%\n  filter(RegionName == \"California\") %&gt;%\n  select(Date, ZHVI_AllHomes) %&gt;%\n  # Remove any missing values for this metric\n  na.omit()\n\ncali_ts &lt;- cali_zhvi %&gt;%\n  as_tsibble(index = Date)\n\n\n# We set 'Date' as the 'index' (the time component).\ncali_ts_fill_gaps &lt;- cali_zhvi %&gt;%\n  as_tsibble(index = Date) %&gt;%\n  fill_gaps()\n# If you were analyzing multiple states, you would add a 'key'.\n# Example for multiple states (not run here):\n# multi_state_ts &lt;- all_states_data %&gt;%\n#   select(Date, RegionName, ZHVI_AllHomes) %&gt;%\n#   as_tsibble(index = Date, key = RegionName)\n\nLets see how many N/As after filling the gaps:\n\n# Count total NAs per column\ncolSums(is.na(cali_ts))\ncolSums(is.na(cali_ts_fill_gaps))\n\n\n# Inspect the new, focused data frame\nhead(cali_zhvi)\n\n        Date ZHVI_AllHomes\n1 1996-04-30        157900\n2 1996-05-31        157800\n3 1996-06-30        157500\n4 1996-07-31        157300\n5 1996-08-31        157000\n6 1996-09-30        156800\n\nhead(cali_ts)\n\n# A tsibble: 6 x 2 [1D]\n  Date       ZHVI_AllHomes\n  &lt;date&gt;             &lt;int&gt;\n1 1996-04-30        157900\n2 1996-05-31        157800\n3 1996-06-30        157500\n4 1996-07-31        157300\n5 1996-08-31        157000\n6 1996-09-30        156800\n\nhead(cali_ts_fill_gaps)\n\n# A tsibble: 6 x 2 [1D]\n  Date       ZHVI_AllHomes\n  &lt;date&gt;             &lt;int&gt;\n1 1996-04-30        157900\n2 1996-05-01            NA\n3 1996-05-02            NA\n4 1996-05-03            NA\n5 1996-05-04            NA\n6 1996-05-05            NA\n\n\n\ncali_ts %&gt;%\n  autoplot(ZHVI_AllHomes) +\n  labs(title = \"Zillow Home Value Index (ZHVI) for California\",\n       y = \"Home Value Index\",\n       x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# This will scan cali_ts and report any gaps\ngap_summary &lt;- count_gaps(cali_ts)\n\n# Print the summary\nprint(gap_summary)\n\n# A tibble: 260 × 3\n   .from      .to           .n\n   &lt;date&gt;     &lt;date&gt;     &lt;int&gt;\n 1 1996-05-01 1996-05-30    30\n 2 1996-06-01 1996-06-29    29\n 3 1996-07-01 1996-07-30    30\n 4 1996-08-01 1996-08-30    30\n 5 1996-09-01 1996-09-29    29\n 6 1996-10-01 1996-10-30    30\n 7 1996-11-01 1996-11-29    29\n 8 1996-12-01 1996-12-30    30\n 9 1997-01-01 1997-01-30    30\n10 1997-02-01 1997-02-27    27\n# ℹ 250 more rows\n\n\n\ncali_ts_fill_gaps %&gt;%\n  autoplot(ZHVI_AllHomes) +\n  labs(title = \"Zillow Home Value Index (ZHVI) for California\",\n       y = \"Home Value Index\",\n       x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Inspect the new, focused data frame\n# print(\"cali_zhvi (data frame):\")\n# DT::datatable(cali_zhvi, options = list(pageLength = 5))\n\nprint(\"cali_ts (tsibble):\")\n\n[1] \"cali_ts (tsibble):\"\n\nDT::datatable(cali_ts, options = list(pageLength = 5))\n\n\n\n\n# print(\"cali_ts_fill_gaps (tsibble with filled gaps):\")\n# DT::datatable(cali_ts_fill_gaps, options = list(pageLength = 5))\n\n\nCreate a new ‘Month’ column using the yearmonth() function\nGroup by this new explicit month\nSummarise the data (using mean() is safe, but since you have one observation per month, last() or sum() would also work)\nConvert to a tsibble, now indexed by the new ‘Month’ object\n\n\ncali_ts_monthly &lt;- cali_zhvi %&gt;%\n  mutate(Month = yearmonth(Date)) %&gt;%\n  group_by(Month) %&gt;%\n  summarise(ZHVI_AllHomes = mean(ZHVI_AllHomes, na.rm = TRUE)) %&gt;%\n  as_tsibble(index = Month)\n\n# Check the new object\nprint(\"New Monthly Tsibble:\")\n\n[1] \"New Monthly Tsibble:\"\n\nhead(cali_ts_monthly)\n\n# A tsibble: 6 x 2 [1M]\n     Month ZHVI_AllHomes\n     &lt;mth&gt;         &lt;dbl&gt;\n1 1996 Apr        157900\n2 1996 May        157800\n3 1996 Jun        157500\n4 1996 Jul        157300\n5 1996 Aug        157000\n6 1996 Sep        156800\n\n# Note the &lt;mth&gt; tag in the output, indicating it's a monthly tsibble\nprint(\"Class of the new index:\")\n\n[1] \"Class of the new index:\"\n\nclass(cali_ts_monthly$Month)\n\n[1] \"yearmonth\"  \"vctrs_vctr\"\n\n\n\ncali_ts_monthly %&gt;%\n  autoplot(ZHVI_AllHomes) +\n  labs(title = \"Zillow Home Value Index (ZHVI) for California\",\n       y = \"Home Value Index\",\n       x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn R, Date objects are stored internally as the number of days that have passed since an “origin” date, which by default is January 1, 1970.\nThe number 9587 is the number of days since 1970-01-01.\nThis corresponds to the date April 30, 1996, which is the starting point of your Zillow dataset.\nThis happens if you (or R) accidentally convert a Date object to a plain number.\nThe yearmonth object we created in the last step is different. It prints as “1996 Apr” and internally stores the number of months since the 1970 epoch (which would be a much smaller number, like 316).\n\nprint(\"cali_ts_monthly (tsibble with only monthly):\")\n\n[1] \"cali_ts_monthly (tsibble with only monthly):\"\n\nDT::datatable(cali_ts_monthly, options = list(pageLength = 5))\n\n\n\n\n\n\n\nReferences"
  },
  {
    "objectID": "posts/renan-blog-post-2/index.html",
    "href": "posts/renan-blog-post-2/index.html",
    "title": "House Dataset Exploration",
    "section": "",
    "text": "This report has a very basic exploration of the dataset for sake of simplicity and to keep it a small report, further exploration will be continuosly update on the project page: https://github.com/renanmb/CAP-6771—Data-Mining-Project"
  },
  {
    "objectID": "posts/renan-blog-post-2/index.html#introduction",
    "href": "posts/renan-blog-post-2/index.html#introduction",
    "title": "House Dataset Exploration",
    "section": "1. Introduction",
    "text": "1. Introduction\nOn this blog post we will be exploring several data sources for the House Price Forecasting project and evaluating what can be done and what the next steps we must take to properly aanswer the research questions.\nRun the following command to download the Zillow Economics Data[1]:\n```{bash}\n#!/bin/bash\ncurl -L -o ~/Downloads/zecon.zip\\\n  https://www.kaggle.com/api/v1/datasets/download/zillow/zecon\n```\n\n1.1 Load packages\n\n# install.packages(\"fpp3\")\nlibrary(fpp3)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggtime)\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/renan-blog-post-2/index.html#load-the-dataset",
    "href": "posts/renan-blog-post-2/index.html#load-the-dataset",
    "title": "House Dataset Exploration",
    "section": "2. Load the Dataset",
    "text": "2. Load the Dataset\nThe following code will locate the folder datasets and then it will give back the variable datasets_path which you can use to build the path to the desired data to be loaded.\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nzillow_economics_data_path &lt;- file.path(datasets_path, \"zillow-economics-data-01\")\n\nstate_time_series &lt;- file.path(zillow_economics_data_path, \"State_time_series.csv\")\nall_states_data &lt;- read.csv(state_time_series)"
  },
  {
    "objectID": "posts/renan-blog-post-2/index.html#exploring-data",
    "href": "posts/renan-blog-post-2/index.html#exploring-data",
    "title": "House Dataset Exploration",
    "section": "3. Exploring Data",
    "text": "3. Exploring Data\nThe ‘Date’ column is a character, let’s convert it to a Date object.\n\n# Inspect the data structure\nstr(all_states_data)\n\n'data.frame':   13212 obs. of  82 variables:\n $ Date                                                         : chr  \"1996-04-30\" \"1996-04-30\" \"1996-04-30\" \"1996-04-30\" ...\n $ RegionName                                                   : chr  \"Alabama\" \"Arizona\" \"Arkansas\" \"California\" ...\n $ DaysOnZillow_AllHomes                                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ InventorySeasonallyAdjusted_AllHomes                         : int  NA NA NA NA NA NA NA NA NA NA ...\n $ InventoryRaw_AllHomes                                        : int  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_1Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_2Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_3Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_4Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_5BedroomOrMore                     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_AllHomes                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_CondoCoop                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_DuplexTriplex                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_SingleFamilyResidence              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_1Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_2Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_3Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_4Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_5BedroomOrMore                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_AllHomes                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_CondoCoop                                 : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_DuplexTriplex                             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_SingleFamilyResidence                     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPctOfPriceReduction_AllHomes                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPctOfPriceReduction_CondoCoop                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPctOfPriceReduction_SingleFamilyResidence              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPriceCutDollar_AllHomes                                : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPriceCutDollar_CondoCoop                               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPriceCutDollar_SingleFamilyResidence                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_1Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_2Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_3Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_4Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_5BedroomOrMore                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_AllHomes                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_CondoCoop                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_DuplexTriplex                       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_MultiFamilyResidence5PlusUnits      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_SingleFamilyResidence               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_Studio                              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_1Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_2Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_3Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_4Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_5BedroomOrMore                             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_AllHomes                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_CondoCoop                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_DuplexTriplex                              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_MultiFamilyResidence5PlusUnits             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_SingleFamilyResidence                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_Studio                                     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ ZHVIPerSqft_AllHomes                                         : int  50 62 42 102 82 85 71 56 55 185 ...\n $ PctOfHomesDecreasingInValues_AllHomes                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfHomesIncreasingInValues_AllHomes                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfHomesSellingForGain_AllHomes                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfHomesSellingForLoss_AllHomes                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductionsSeasAdj_AllHomes             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductionsSeasAdj_CondoCoop            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductionsSeasAdj_SingleFamilyResidence: num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductions_AllHomes                    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductions_CondoCoop                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductions_SingleFamilyResidence       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PriceToRentRatio_AllHomes                                    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Sale_Counts                                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Sale_Counts_Seas_Adj                                         : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Sale_Prices                                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ ZHVI_1bedroom                                                : int  61500 59200 53000 93700 77800 64700 90100 45400 74900 152300 ...\n $ ZHVI_2bedroom                                                : int  48900 86400 54500 123400 97500 97000 88200 65400 64700 186600 ...\n $ ZHVI_3bedroom                                                : int  78200 96100 76800 150900 129000 130400 103500 89100 88000 231800 ...\n $ ZHVI_4bedroom                                                : int  146500 128400 135100 196100 176100 194800 157800 133600 149700 303400 ...\n $ ZHVI_5BedroomOrMore                                          : int  206300 190500 186000 265300 212900 299800 176100 199900 212800 345500 ...\n $ ZHVI_AllHomes                                                : int  79500 103600 64400 157900 128100 132000 106800 86300 92000 227400 ...\n $ ZHVI_BottomTier                                              : int  45600 67100 38400 95100 82700 83700 77200 52500 57200 144500 ...\n $ ZHVI_CondoCoop                                               : int  99500 78900 70300 136100 99400 85000 NA 70600 89300 177000 ...\n $ ZHVI_MiddleTier                                              : int  79500 103600 64400 157900 128100 132000 106800 86300 92000 227400 ...\n $ ZHVI_SingleFamilyResidence                                   : int  79000 107500 64500 162000 133600 141000 107400 92100 92400 262600 ...\n $ ZHVI_TopTier                                                 : int  140200 168700 115200 270600 209300 231600 161600 155300 163900 374700 ...\n $ ZRI_AllHomes                                                 : int  NA NA NA NA NA NA NA NA NA NA ...\n $ ZRI_AllHomesPlusMultifamily                                  : int  NA NA NA NA NA NA NA NA NA NA ...\n $ ZriPerSqft_AllHomes                                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Zri_MultiFamilyResidenceRental                               : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Zri_SingleFamilyResidenceRental                              : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\nNow we Convert ‘Date’ column to a Date object.\n\nall_states_data$Date &lt;- as.Date(all_states_data$Date)\n\nLets Check how many N/As and think about\n\n# Count total NAs per column\ncolSums(is.na(all_states_data))\n\nMaking it pretty\n\n# Get the total number of rows for calculating percentages\ntotal_rows &lt;- nrow(all_states_data)\n\n# Create a pretty summary table\nna_summary &lt;- all_states_data %&gt;%\n  # 1. Count NAs for every column\n  summarise(across(everything(), ~sum(is.na(.)))) %&gt;%\n  \n  # 2. Pivot the data from wide to long\n  pivot_longer(everything(),\n               names_to = \"Column\",\n               values_to = \"NA_Count\") %&gt;%\n  \n  # 3. (Optional) Filter to only show columns that HAVE NAs\n  filter(NA_Count &gt; 0) %&gt;%\n  \n  # 4. (Optional) Add a percentage column\n  mutate(NA_Percentage = (NA_Count / total_rows) * 100) %&gt;%\n\n  # 5. Sort by the highest NA count\n  arrange(desc(NA_Count))\n\n# Print the clean table\nprint(na_summary)\n\n# A tibble: 80 × 3\n   Column                                  NA_Count NA_Percentage\n   &lt;chr&gt;                                      &lt;int&gt;         &lt;dbl&gt;\n 1 PctOfHomesSellingForGain_AllHomes          12609          95.4\n 2 PctOfHomesSellingForLoss_AllHomes          12609          95.4\n 3 MedianRentalPrice_5BedroomOrMore           11994          90.8\n 4 MedianRentalPricePerSqft_5BedroomOrMore    11752          88.9\n 5 MedianRentalPricePerSqft_Studio            10875          82.3\n 6 MedianRentalPrice_CondoCoop                10437          79.0\n 7 MedianRentalPricePerSqft_DuplexTriplex     10293          77.9\n 8 MedianRentalPrice_Studio                   10211          77.3\n 9 MedianListingPrice_1Bedroom                10205          77.2\n10 MedianRentalPrice_DuplexTriplex            10068          76.2\n# ℹ 70 more rows\n\n\n\n#  Some random tinkering\n# head(all_states_data)\n# summary(all_states_data)"
  },
  {
    "objectID": "posts/renan-blog-post-2/index.html#time-series-initial-analysis",
    "href": "posts/renan-blog-post-2/index.html#time-series-initial-analysis",
    "title": "House Dataset Exploration",
    "section": "4. Time Series Initial Analysis",
    "text": "4. Time Series Initial Analysis\nNow we will further explore the dataset. As we could observe that the dataset is large and has many columns for metrics we must continue to filter for a specific state and metric to start making this more manageable. Then later we will come back to explore the full dataset and the other datasets.\n\n4.1 Zillow Economics Data\nWe create the timeseries object using ‘tsibble’ which is a time-aware data frame. This is the standard object for the ‘fpp3’ workflow. Then we index the time component.\nFor sake of simplicity we are going to focus for now on a single State (California) and the Zillow Home Value Index (ZHVI_AllHomes) over time.\n\n# Let's analyze the Zillow Home Value Index (ZHVI_AllHomes) for \"California\".\ncali_zhvi &lt;- all_states_data %&gt;%\n  filter(RegionName == \"California\") %&gt;%\n  select(Date, ZHVI_AllHomes) %&gt;%\n  # Remove any missing values for this metric\n  na.omit()\n\nJust a quick check:\n\n# Inspect the new, focused data frame\nhead(cali_zhvi)\n\n        Date ZHVI_AllHomes\n1 1996-04-30        157900\n2 1996-05-31        157800\n3 1996-06-30        157500\n4 1996-07-31        157300\n5 1996-08-31        157000\n6 1996-09-30        156800\n\n\nMaking the Time Series object cali_ts:\n\ncali_ts &lt;- cali_zhvi %&gt;%\n  as_tsibble(index = Date)\n\nFurther experiments need to handle the N/As when filling the gaps:\n\n# We set 'Date' as the 'index' (the time component).\ncali_ts_fill_gaps &lt;- cali_zhvi %&gt;%\n  as_tsibble(index = Date) %&gt;%\n  fill_gaps()\n# If you were analyzing multiple states, you would add a 'key'.\n# Example for multiple states (not run here):\n# multi_state_ts &lt;- all_states_data %&gt;%\n#   select(Date, RegionName, ZHVI_AllHomes) %&gt;%\n#   as_tsibble(index = Date, key = RegionName)\n\nLets see how many N/As after filling the gaps:\n\n# Count total NAs per column\ncolSums(is.na(cali_ts))\ncolSums(is.na(cali_ts_fill_gaps))\n\nForward Fill (LOCF): Uses the last known value to fill gaps. Ideal for stable or categorical data but not for volatile metrics.\n\n# 1. APPLY LOCF TO THE CALIFORNIA DATA\n# ------------------------------------\n# We use fill() on the ZHVI_AllHomes column.\n# The default direction is \"down\", which is exactly what LOCF is.\ncali_locf &lt;- cali_ts_fill_gaps %&gt;%\n  fill(ZHVI_AllHomes)\n\n\n# cali_ts\n\n\n4.1.1 Plot the Time Series\nWe can make an initial plot to see that there is a trend and the data is non-stationary.\n\ncali_ts %&gt;%\n  autoplot(ZHVI_AllHomes) +\n  labs(title = \"Zillow Home Value Index (ZHVI) for California\",\n       y = \"Home Value Index\",\n       x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can see that filling the gaps creates some issues, this was fixed with the LOCF:\n\ncali_ts_fill_gaps %&gt;%\n  autoplot(ZHVI_AllHomes) +\n  labs(title = \"Zillow Home Value Index (ZHVI) for California\",\n       y = \"Home Value Index\",\n       x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nExploring for Seasonality which is not that evident, need further study:\n\n# --- B. Look for Seasonality ---\n# A season plot layers the data by year, helping to spot seasonal patterns.\ncali_locf %&gt;%\n  gg_season(ZHVI_AllHomes, labels = \"right\") +\n  labs(title = \"Seasonal Plot: ZHVI for California\",\n       y = \"Home Value Index\")\n\n\n\n\n\n\n\n\nLet’s plot the “before” and “after” side-by-side to see if there are any evident changes.\n\n# Plot 1: Using the original, gappy data\np1 &lt;- ggplot(cali_ts, aes(x = Date, y = ZHVI_AllHomes)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Original (cali_ts)\",\n       subtitle = \"ggplot connects the dots over implicit gaps\") +\n  theme_minimal()\n\n# Plot 2: Using the data fixed with fill_gaps()\np2 &lt;- ggplot(cali_locf, aes(x = Date, y = ZHVI_AllHomes)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"After: Forward Fill (LOCF)\",\n       subtitle = \"Gaps are filled with the last known value\") +\n  theme_minimal()\n\n# Combine them side-by-side\np1 + p2"
  },
  {
    "objectID": "posts/renan-blog-post-2/index.html#misc",
    "href": "posts/renan-blog-post-2/index.html#misc",
    "title": "House Dataset Exploration",
    "section": "MISC",
    "text": "MISC\nThe naniar package is built specifically for exploring missing data. It has a miss_var_summary() function that does help visualizing missing data. Might consider implementing.\n```{r}\n# install.packages(\"naniar\")\nlibrary(naniar)\n\n# This one function does it all, already sorted!\nmiss_var_summary(all_states_data)\n\n# Example output:\n# A tibble: 271 × 3\n#   variable        n_miss  pct_miss\n#   &lt;chr&gt;            &lt;int&gt;     &lt;dbl&gt;\n# 1 ZRI_AllHomes_CondoCoop 149463  79.2\n# 2 ZRI_AllHomes_DuplexTriplex  142137  75.3\n# ...\n```\n\nReferences\n\n\n1. zillow. (n.d.). Zillow Economics Data. https://www.kaggle.com/datasets/zillow/zecon"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data mining Project",
    "section": "",
    "text": "As part of the CARP-6771 Data mining Fall-2025 class at University of West Florida (UWF), our team completed a project on House Price Forecasting.\nThis project was completed under the guidance of Dr. Sikha Bagui (Sikha Bagui | Dr. Sikha Bagui)."
  },
  {
    "objectID": "slides.html#introduction",
    "href": "slides.html#introduction",
    "title": "Present a great story for data science projects",
    "section": "Introduction",
    "text": "Introduction\n\nDevelop a storyline that captures attention and maintains interest.\nYour audience is your peers\nClearly state the problem or question you’re addressing.\nIntroduce why it is relevant needs.\nProvide an overview of your approach.\n\nIn kernel estimator, weight function is known as kernel function[1]. Cite this paper[2]. The GEE[3]. The PCA[4]*"
  },
  {
    "objectID": "slides.html#methods",
    "href": "slides.html#methods",
    "title": "Present a great story for data science projects",
    "section": "Methods",
    "text": "Methods\n\nDetail the models or algorithms used.\nJustify your choices based on the problem and data."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization",
    "href": "slides.html#data-exploration-and-visualization",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\n\nDescribe your data sources and collection process.\nPresent initial findings and insights through visualizations.\nHighlight unexpected patterns or anomalies."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization-1",
    "href": "slides.html#data-exploration-and-visualization-1",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\nA study was conducted to determine how…"
  },
  {
    "objectID": "slides.html#modeling-and-results",
    "href": "slides.html#modeling-and-results",
    "title": "Present a great story for data science projects",
    "section": "Modeling and Results",
    "text": "Modeling and Results\n\nExplain your data preprocessing and cleaning steps.\nPresent your key findings in a clear and concise manner.\nUse visuals to support your claims.\nTell a story about what the data reveals."
  },
  {
    "objectID": "slides.html#conclusion",
    "href": "slides.html#conclusion",
    "title": "Present a great story for data science projects",
    "section": "Conclusion",
    "text": "Conclusion\n\nSummarize your key findings.\nDiscuss the implications of your results."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Present a great story for data science projects",
    "section": "References",
    "text": "References\n\n\n1. Efromovich, S. (2008). Nonparametric curve estimation: Methods, theory, and applications. Springer New York. https://books.google.com/books?id=mdoLBwAAQBAJ\n\n\n2. Bro, R., & Smilde, A. K. (2014). Principal component analysis. Analytical Methods, 6(9), 2812–2831.\n\n\n3. Wang, M. (2014). Generalized estimating equations in longitudinal data analysis: A review and recent developments. Advances in Statistics, 2014.\n\n\n4. Daffertshofer, A., Lamoth, C. J., Meijer, O. G., & Beek, P. J. (2004). PCA in studying coordination and variability: A tutorial. Clinical Biomechanics, 19(4), 415–428."
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html",
    "href": "posts/renan-blog-post-1/index.html",
    "title": "Welcome to the Project",
    "section": "",
    "text": "For this class setup a website to present the project"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html#introduction",
    "href": "posts/renan-blog-post-1/index.html#introduction",
    "title": "Welcome to the Project",
    "section": "Introduction",
    "text": "Introduction\nMake some cheesy introduction"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html#how-to-setup-renv",
    "href": "posts/renan-blog-post-1/index.html#how-to-setup-renv",
    "title": "Welcome to the Project",
    "section": "How to setup RENV",
    "text": "How to setup RENV\nWhat is renv ?? well, it is an R package that provides powerful dependency management for your projects, ensuring they are isolated, portable, and reproducible.\n```{bash}\nrenv::init()\n```"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html#how-to-setup-uv",
    "href": "posts/renan-blog-post-1/index.html#how-to-setup-uv",
    "title": "Welcome to the Project",
    "section": "How to setup UV",
    "text": "How to setup UV\nUV is an extremely fast, next-generation Python package installer and resolver built in Rust by Astral, by consolidating environment creation, package installation, and dependency locking into a single, blazing-fast command-line tool, UV drastically simplifies and accelerates Python development workflows, saving developers significant time and frustration.\n```{bash}\nuv add requests\nuv remove requests\nuv lock --upgrade-package requests\nuv venv\nuv sync\n```\nActivating a VENV\n```{bash}\nsource .venv/bin/activate\n```\nYou can add dependencies to your pyproject.toml with the uv add command. This will also update the lockfile and project environment:\n```{bash}\nuv add requests\n```\nTo remove a package, you can use uv remove:\n```{bash}\nuv remove requests\n```\nTo upgrade a package, run uv lock with the –upgrade-package flag:\n```{bash}\nuv lock --upgrade-package requests\n```\nCreating a virtual environment\n```{bash}\nuv venv\n```\nSyncing the environment\n```{bash}\nuv sync\n```"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html#getting-started-with-quarto",
    "href": "posts/renan-blog-post-1/index.html#getting-started-with-quarto",
    "title": "Welcome to the Project",
    "section": "Getting Started with Quarto",
    "text": "Getting Started with Quarto\nInstall the dependencies"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html#how-to-render-and-preview-your-project",
    "href": "posts/renan-blog-post-1/index.html#how-to-render-and-preview-your-project",
    "title": "Welcome to the Project",
    "section": "How to Render and Preview your project",
    "text": "How to Render and Preview your project\nEither use the IDE of your preference, VSCode has extensions to support\nor run on the cmd:\nTo render the quarto document run:\n```{bash}\nquarto render\n```\nTo preview the rendered quarto document run:\n```{bash}\nquarto preview\n```"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html#how-to-make-citations",
    "href": "posts/renan-blog-post-1/index.html#how-to-make-citations",
    "title": "Welcome to the Project",
    "section": "How to make Citations",
    "text": "How to make Citations\nBibTex is a great why to generate consistent and properly formatted Citations, to generate bibtex citations u can use GoogleScholar:\nExample: Add here some content explaining how to use GoogleScholar for citations\nAdd another example using citation generator\n\nCreate your BibTeX file (e.g., references.bib) and add your entries.\n\nExample:\n```{text}\n@inproceedings{Liu_2021,\n   title={Priority prediction of Asian Hornet sighting report using machine learning methods},\n   url={http://dx.doi.org/10.1109/SEAI52285.2021.9477549},\n   DOI={10.1109/seai52285.2021.9477549},\n   booktitle={2021 IEEE International Conference on Software Engineering and Artificial Intelligence (SEAI)},\n   publisher={IEEE},\n   author={Liu, Yixin and Guo, Jiaxin and Dong, Jieyang and Jiang, Luoqian and Ouyang, Haoyuan},\n   year={2021},\n   month=jun, pages={7–11} }\n```\n\nLink the file in your Quarto YAML header (the part at the very top):\n\n```{yml}\n---\ntitle: \"My Document\"\nbibliography: references.bib\n---\n```\n\nCite in your text using the BibTeX key: (e.g.,[1]).\n\nExample:\n```{text}\n[Priority prediction of Asian Hornet sighting report using machine learning methods](https://huggingface.co/papers/2107.05465). @Liu_2021\n```\nPriority prediction of Asian Hornet sighting report using machine learning methods.[1]\n\nAdd a “References” section at the end of your document where the bibliography will be printed.\n\nExample:\n```{text}\n### References\n\n::: {#refs}\n:::\n```\n\nReferences\n\n\n1. Liu, Y., Guo, J., Dong, J., Jiang, L., & Ouyang, H. (2021). Priority prediction of asian hornet sighting report using machine learning methods. 2021 IEEE International Conference on Software Engineering and Artificial Intelligence (SEAI), 7–11. https://doi.org/10.1109/seai52285.2021.9477549"
  },
  {
    "objectID": "posts/binita-blog-post-1/index.html",
    "href": "posts/binita-blog-post-1/index.html",
    "title": "Adding Binning",
    "section": "",
    "text": "On this exploratory phase I will be adding binning to handle with the granularity and hierarchy of the dataset."
  },
  {
    "objectID": "posts/binita-blog-post-1/index.html#introduction",
    "href": "posts/binita-blog-post-1/index.html#introduction",
    "title": "Adding Binning",
    "section": "Introduction",
    "text": "Introduction\nThis Project is explained on Quarto/R Markdown code.\nThe loaded dataset finds the root on Git repository and constructs the path to the datasets folder and the CSV file.\nIt then loads the CSV into a data frame called all_states_data. A data frame all_states_data\nThe project contains a data frame all_states_data with 13,212 rows and 82 columns.\nTime series analysis packages (including tsibble, feasts, fable): fpp3\nData manipulation and cleaning: dplyr, tidyr\nPlotting: ggplot2\nExtends ggplot2 for time series plots: ggtime\nCombine multiple ggplots: patchwork"
  },
  {
    "objectID": "posts/binita-blog-post-1/index.html#load-packages",
    "href": "posts/binita-blog-post-1/index.html#load-packages",
    "title": "Adding Binning",
    "section": "1. Load packages",
    "text": "1. Load packages\n\n# install.packages(\"fpp3\")\nlibrary(fpp3)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggtime)\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/binita-blog-post-1/index.html#load-the-dataset",
    "href": "posts/binita-blog-post-1/index.html#load-the-dataset",
    "title": "Adding Binning",
    "section": "2. Load the Dataset",
    "text": "2. Load the Dataset\nThe following code will locate the folder datasets and then it will give back the variable datasets_path which you can use to build the path to the desired data to be loaded.\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nzillow_economics_data_path &lt;- file.path(datasets_path, \"zillow-economics-data-01\")\n\nstate_time_series &lt;- file.path(zillow_economics_data_path, \"State_time_series.csv\")\nall_states_data &lt;- read.csv(state_time_series)"
  },
  {
    "objectID": "posts/binita-blog-post-1/index.html#exploring-data",
    "href": "posts/binita-blog-post-1/index.html#exploring-data",
    "title": "Adding Binning",
    "section": "3. Exploring Data",
    "text": "3. Exploring Data\nLet’s convert Data column to Data type\n\n# convert data column to Data type\nall_states_data &lt;- all_states_data %&gt;%\nmutate(Date = as.Date(Date, format = \"%Y-%m-%d\"))\n\n\n# Check the structure\nstr(all_states_data)\n\n'data.frame':   13212 obs. of  82 variables:\n $ Date                                                         : Date, format: \"1996-04-30\" \"1996-04-30\" ...\n $ RegionName                                                   : chr  \"Alabama\" \"Arizona\" \"Arkansas\" \"California\" ...\n $ DaysOnZillow_AllHomes                                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ InventorySeasonallyAdjusted_AllHomes                         : int  NA NA NA NA NA NA NA NA NA NA ...\n $ InventoryRaw_AllHomes                                        : int  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_1Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_2Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_3Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_4Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_5BedroomOrMore                     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_AllHomes                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_CondoCoop                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_DuplexTriplex                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_SingleFamilyResidence              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_1Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_2Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_3Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_4Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_5BedroomOrMore                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_AllHomes                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_CondoCoop                                 : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_DuplexTriplex                             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_SingleFamilyResidence                     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPctOfPriceReduction_AllHomes                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPctOfPriceReduction_CondoCoop                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPctOfPriceReduction_SingleFamilyResidence              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPriceCutDollar_AllHomes                                : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPriceCutDollar_CondoCoop                               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPriceCutDollar_SingleFamilyResidence                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_1Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_2Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_3Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_4Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_5BedroomOrMore                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_AllHomes                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_CondoCoop                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_DuplexTriplex                       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_MultiFamilyResidence5PlusUnits      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_SingleFamilyResidence               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_Studio                              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_1Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_2Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_3Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_4Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_5BedroomOrMore                             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_AllHomes                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_CondoCoop                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_DuplexTriplex                              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_MultiFamilyResidence5PlusUnits             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_SingleFamilyResidence                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_Studio                                     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ ZHVIPerSqft_AllHomes                                         : int  50 62 42 102 82 85 71 56 55 185 ...\n $ PctOfHomesDecreasingInValues_AllHomes                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfHomesIncreasingInValues_AllHomes                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfHomesSellingForGain_AllHomes                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfHomesSellingForLoss_AllHomes                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductionsSeasAdj_AllHomes             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductionsSeasAdj_CondoCoop            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductionsSeasAdj_SingleFamilyResidence: num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductions_AllHomes                    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductions_CondoCoop                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductions_SingleFamilyResidence       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PriceToRentRatio_AllHomes                                    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Sale_Counts                                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Sale_Counts_Seas_Adj                                         : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Sale_Prices                                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ ZHVI_1bedroom                                                : int  61500 59200 53000 93700 77800 64700 90100 45400 74900 152300 ...\n $ ZHVI_2bedroom                                                : int  48900 86400 54500 123400 97500 97000 88200 65400 64700 186600 ...\n $ ZHVI_3bedroom                                                : int  78200 96100 76800 150900 129000 130400 103500 89100 88000 231800 ...\n $ ZHVI_4bedroom                                                : int  146500 128400 135100 196100 176100 194800 157800 133600 149700 303400 ...\n $ ZHVI_5BedroomOrMore                                          : int  206300 190500 186000 265300 212900 299800 176100 199900 212800 345500 ...\n $ ZHVI_AllHomes                                                : int  79500 103600 64400 157900 128100 132000 106800 86300 92000 227400 ...\n $ ZHVI_BottomTier                                              : int  45600 67100 38400 95100 82700 83700 77200 52500 57200 144500 ...\n $ ZHVI_CondoCoop                                               : int  99500 78900 70300 136100 99400 85000 NA 70600 89300 177000 ...\n $ ZHVI_MiddleTier                                              : int  79500 103600 64400 157900 128100 132000 106800 86300 92000 227400 ...\n $ ZHVI_SingleFamilyResidence                                   : int  79000 107500 64500 162000 133600 141000 107400 92100 92400 262600 ...\n $ ZHVI_TopTier                                                 : int  140200 168700 115200 270600 209300 231600 161600 155300 163900 374700 ...\n $ ZRI_AllHomes                                                 : int  NA NA NA NA NA NA NA NA NA NA ...\n $ ZRI_AllHomesPlusMultifamily                                  : int  NA NA NA NA NA NA NA NA NA NA ...\n $ ZriPerSqft_AllHomes                                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Zri_MultiFamilyResidenceRental                               : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Zri_SingleFamilyResidenceRental                              : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\nWe convert the Date column to proper Date type so it can be used in time series analysis.\nstr() shows structure: column names, types, and example data.\nOutput:\n\nData column is Data type.\nOther column include RegionName, ZHVI_AllHomes, and many Zillow metrics.\n\n\n# Convert to tsibble for time series analysis\n# Assuming 'State' is a column and 'Value' is the metric\n\nlibrary(tsibble)\n\nstate_tsibble &lt;- all_states_data %&gt;%\nas_tsibble(key = RegionName, index = Date)\n\nWe convert the data frame into a tsibble (time series tibble)\n\nkey = RegionName: each state is treated as a seperate time series\nindex = Date:the date column is the time index.\n\nOutput:\nstate_tsibble is now a tsibble with 13,212 rows and 82 columns, ready for time series operations.\n\n# Inspect first rows\nstate_tsibble %&gt;% head()\n\n# A tsibble: 6 x 82 [1D]\n# Key:       RegionName [1]\n  Date       RegionName DaysOnZillow_AllHomes InventorySeasonallyAdjusted_AllH…¹\n  &lt;date&gt;     &lt;chr&gt;                      &lt;dbl&gt;                              &lt;int&gt;\n1 1996-04-30 Alabama                       NA                                 NA\n2 1996-05-31 Alabama                       NA                                 NA\n3 1996-06-30 Alabama                       NA                                 NA\n4 1996-07-31 Alabama                       NA                                 NA\n5 1996-08-31 Alabama                       NA                                 NA\n6 1996-09-30 Alabama                       NA                                 NA\n# ℹ abbreviated name: ¹​InventorySeasonallyAdjusted_AllHomes\n# ℹ 78 more variables: InventoryRaw_AllHomes &lt;int&gt;,\n#   MedianListingPricePerSqft_1Bedroom &lt;dbl&gt;,\n#   MedianListingPricePerSqft_2Bedroom &lt;dbl&gt;,\n#   MedianListingPricePerSqft_3Bedroom &lt;dbl&gt;,\n#   MedianListingPricePerSqft_4Bedroom &lt;dbl&gt;,\n#   MedianListingPricePerSqft_5BedroomOrMore &lt;dbl&gt;, …"
  },
  {
    "objectID": "posts/binita-blog-post-1/index.html#binning-time-series-data",
    "href": "posts/binita-blog-post-1/index.html#binning-time-series-data",
    "title": "Adding Binning",
    "section": "4. Binning Time series Data",
    "text": "4. Binning Time series Data\n\n# Monthly aggregation (mean per month)\nlibrary(dplyr)\nlibrary(ggtime)\nlibrary(tsibble)\n\nmonthly_binned &lt;- state_tsibble %&gt;%\nindex_by(Month = yearmonth(Date)) %&gt;%\nsummarise(Avg_ZHVI_AllHomes = mean(ZHVI_AllHomes, na.rm = TRUE))\n\nmonthly_binned %&gt;% head()\n\n# A tsibble: 6 x 2 [1M]\n     Month Avg_ZHVI_AllHomes\n     &lt;mth&gt;             &lt;dbl&gt;\n1 1996 Apr           101744.\n2 1996 May           101440 \n3 1996 Jun           101555 \n4 1996 Jul           101678.\n5 1996 Aug           101815 \n6 1996 Sep           101998.\n\n\nindex_by(Month = yearmonth(Date)): groups the data by month.\nsummarise(Avg_ZHVI_AllHomes = mean(…)): calculates the average zillow home Value Index (ZHVI) per month.\nna.rm = TRUE: ignores missing values.\n\n# Quarterly aggregation example\nquarterly_binned &lt;- state_tsibble %&gt;%\n  index_by(Quarter = yearquarter(Date)) %&gt;%\n  summarise(Avg_ZHVI_AllHomes = mean(ZHVI_AllHomes, na.rm = TRUE))\n\nquarterly_binned %&gt;% head()\n\n# A tsibble: 6 x 2 [1Q]\n  Quarter Avg_ZHVI_AllHomes\n    &lt;qtr&gt;             &lt;dbl&gt;\n1 1996 Q2           101578.\n2 1996 Q3           101830 \n3 1996 Q4           102449.\n4 1997 Q1           103248.\n5 1997 Q2           104115.\n6 1997 Q3           105008 \n\n\nThe output is same as monthly aggregation, but grouped by quarter instead of month.\n\n# Plot binned data\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(tsibble)\nlibrary(ggtime)\n\n# Convert Month to a regular Date (first day of the month)\nmonthly_binned_df &lt;- monthly_binned %&gt;%\n  as_tibble() %&gt;%                    # convert to tibble\n  mutate(Month = as.Date(Month))     # convert yearmonth to Date\n\nWe convert the monthly_binned tsibble into a regular tibble.\nAlso, convert Month from yearmonth to a standard Date for ggplot.\nThe output monthly_binned_df has two columns: Month (Date) and Avg_ZHVI_AllHomes\n\n# Plot with ggplot\nggplot(monthly_binned_df, aes(x = Month, y = Avg_ZHVI_AllHomes)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Monthly Binned Average of ZHVI_AllHomes\",\n       x = \"Month\", y = \"Average ZHVI_AllHomes\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nWe plot a line chart of monthly average home values\n\nx = Month,\nY = Avg_ZHVI_AllHomes\n\ngeom_line(color = “blue”): draws the line in blue.\nalso, rotates x-axis labels for readability.\nA line graph shows trends in average home prices per month over time.\nPeaks and troughs indicate periods of rising/falling home prices.\n\nReferences"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Literature Review",
    "section": "",
    "text": "These are the literature review done by all the students during this semester.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding Binning\n\n\n\nbinita\n\n\n\nAdding Binning to dataset to decrease Granularity\n\n\n\n\n\nBinita Subedi\n\n\n\n\n\n\n\n\n\n\n\n\nHouse Dataset Exploration\n\n\n\nrenan\n\n\n\nExploring House price datasets for CAP-6771 (Fall 2025)\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nProject 2 - Data Cubbing and Binning\n\n\n\nrenan\n\n\n\nIn this post apply binning to pre-process the dataset\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the Project\n\n\n\nrenan\n\n\n\nFirst post for course CAP-6771 (Fall 2025)\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "House Price Forecasting",
    "section": "",
    "text": "The report goes here need some work\nSlides: slides.html"
  },
  {
    "objectID": "report.html#introduction",
    "href": "report.html#introduction",
    "title": "House Price Forecasting",
    "section": "Introduction",
    "text": "Introduction\nNeed to add report stuff\n\nReferences"
  },
  {
    "objectID": "people/index.html",
    "href": "people/index.html",
    "title": "Meet the Group",
    "section": "",
    "text": "Graduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | University of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Cyber Security\n\n\n\n\n\neducation\n\n\nMSC Cyber Security | University of West Florida\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html#graduate-students",
    "href": "people/index.html#graduate-students",
    "title": "Meet the Group",
    "section": "",
    "text": "Graduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | University of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Cyber Security\n\n\n\n\n\neducation\n\n\nMSC Cyber Security | University of West Florida\n\n\n\n\n\n\n\n\nNo matching items"
  }
]