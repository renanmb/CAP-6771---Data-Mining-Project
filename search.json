[
  {
    "objectID": "people/barbosa-renan/index.html#education",
    "href": "people/barbosa-renan/index.html#education",
    "title": "Renan Monteiro Barbosa",
    "section": "Education",
    "text": "Education\nB.S. Mechanical Engineering | University of West Florida"
  },
  {
    "objectID": "people/subedi-binita/index.html#education",
    "href": "people/subedi-binita/index.html#education",
    "title": "Binita Subedi",
    "section": "Education",
    "text": "Education\nMSC Cyber Security | University of West Florida"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "Welcome to our team! We are a group of collaborators dedicated to…\n\n\n\nRenan Monteiro Barbosa\n\nData Scientist\n\n\n\nRenan Monteiro Barbosa\n\n\nJane is a data scientist with over 10 years of experience…\nContact * Email: jane@example.com * GitHub: https://github.com/renanmb\n\n\n\nBinita Subedi\n\nAdd description here\n\n\n\nBinita Subedi\n\n\nBinita Subedi is a software engineer specializing in web development…\nContact * Email: bs174@students.uwf.edu * GitHub: https://github.com/Binita-subedi\n\n\n\nMatthew Ray\n\nAdd description here\n\n\n\nMatthew Ray\n\n\nMatthew Ray works on developing new research methodologies for…\nContact * Email: mr168@students.uwf.edu * Website: alex-johnson.com"
  },
  {
    "objectID": "posts/renan-blog-post-4/index.html",
    "href": "posts/renan-blog-post-4/index.html",
    "title": "Health Risk Dataset",
    "section": "",
    "text": "The Dataset Healthcare Risk Factors Dataset can be downloaded:"
  },
  {
    "objectID": "posts/renan-blog-post-4/index.html#introduction",
    "href": "posts/renan-blog-post-4/index.html#introduction",
    "title": "Health Risk Dataset",
    "section": "1. Introduction",
    "text": "1. Introduction\n\n\n1.1 Load packages\n\n# install.packages(\"fpp3\")\nlibrary(fpp3)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggtime)\nlibrary(patchwork)\nlibrary(DT)\nlibrary(skimr)"
  },
  {
    "objectID": "posts/renan-blog-post-4/index.html#load-the-dataset",
    "href": "posts/renan-blog-post-4/index.html#load-the-dataset",
    "title": "Health Risk Dataset",
    "section": "2. Load the Dataset",
    "text": "2. Load the Dataset\nThe following code will locate the folder datasets and then it will give back the variable datasets_path which you can use to build the path to the desired data to be loaded.\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nhealth_data_path &lt;- file.path(datasets_path, \"health-risk-data\")\n\ndirty_v3_path &lt;- file.path(health_data_path, \"dirty_v3_path.csv\")\nhealth_data &lt;- read.csv(dirty_v3_path)"
  },
  {
    "objectID": "posts/renan-blog-post-4/index.html#data-exploration",
    "href": "posts/renan-blog-post-4/index.html#data-exploration",
    "title": "Health Risk Dataset",
    "section": "3. Data Exploration",
    "text": "3. Data Exploration\nListing all the attributes in the Dataset:\n\nknitr::kable(data.frame(Feature = names(health_data)))\n\n\n\n\nFeature\n\n\n\n\nAge\n\n\nGender\n\n\nMedical.Condition\n\n\nGlucose\n\n\nBlood.Pressure\n\n\nBMI\n\n\nOxygen.Saturation\n\n\nLengthOfStay\n\n\nCholesterol\n\n\nTriglycerides\n\n\nHbA1c\n\n\nSmoking\n\n\nAlcohol\n\n\nPhysical.Activity\n\n\nDiet.Score\n\n\nFamily.History\n\n\nStress.Level\n\n\nSleep.Hours\n\n\nrandom_notes\n\n\nnoise_col\n\n\n\n\n\nBelow we use the Skimr package to check things like NAs, categorical vs numeric variables\n\nskim(health_data)\n\n\nData summary\n\n\nName\nhealth_data\n\n\nNumber of rows\n30000\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n0\n6\n4500\n3\n0\n\n\nMedical.Condition\n0\n1\n0\n12\n4500\n8\n0\n\n\nrandom_notes\n0\n1\n2\n5\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAge\n4500\n0.85\n54.62\n15.97\n10.00\n45.00\n55.00\n66.00\n89.00\n▁▃▇▇▃\n\n\nGlucose\n4500\n0.85\n123.62\n41.58\n20.32\n96.28\n110.50\n136.61\n318.51\n▁▇▂▁▁\n\n\nBlood.Pressure\n4500\n0.85\n140.46\n21.08\n74.24\n125.14\n138.32\n153.79\n226.38\n▁▇▇▂▁\n\n\nBMI\n0\n1.00\n28.48\n5.73\n7.67\n24.59\n28.05\n31.81\n56.85\n▁▇▇▁▁\n\n\nOxygen.Saturation\n0\n1.00\n94.95\n3.74\n67.51\n93.00\n95.30\n97.38\n110.07\n▁▁▂▇▁\n\n\nLengthOfStay\n0\n1.00\n4.41\n2.76\n1.00\n3.00\n4.00\n5.00\n19.00\n▇▅▁▁▁\n\n\nCholesterol\n0\n1.00\n213.03\n33.52\n95.73\n189.50\n211.84\n235.31\n358.37\n▁▆▇▂▁\n\n\nTriglycerides\n0\n1.00\n176.84\n48.81\n-22.48\n141.28\n173.36\n208.63\n421.51\n▁▅▇▁▁\n\n\nHbA1c\n0\n1.00\n6.29\n1.32\n3.28\n5.33\n5.97\n6.92\n12.36\n▂▇▂▁▁\n\n\nSmoking\n0\n1.00\n0.28\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nAlcohol\n0\n1.00\n0.24\n0.43\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nPhysical.Activity\n0\n1.00\n3.80\n2.01\n-3.68\n2.35\n3.59\n5.06\n12.41\n▁▅▇▂▁\n\n\nDiet.Score\n0\n1.00\n4.03\n1.82\n-1.75\n2.77\n3.79\n5.02\n12.06\n▁▇▇▂▁\n\n\nFamily.History\n0\n1.00\n0.44\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nStress.Level\n0\n1.00\n5.92\n2.23\n-2.44\n4.37\n5.90\n7.44\n15.45\n▁▅▇▂▁\n\n\nSleep.Hours\n0\n1.00\n6.23\n1.19\n1.59\n5.41\n6.23\n7.05\n10.35\n▁▂▇▅▁\n\n\nnoise_col\n0\n1.00\n-0.52\n100.08\n-412.17\n-68.27\n-0.51\n66.81\n467.89\n▁▃▇▂▁\n\n\n\n\n\nBelow we there is a summary showing a significant ammount of NAs in 3 numerical variables\n\n\nCode\n# Get the total number of rows for calculating percentages\ntotal_rows &lt;- nrow(health_data)\n\n# Create a pretty summary table\nna_summary &lt;- health_data %&gt;%\n  # 1. Count NAs for every column\n  summarise(across(everything(), ~sum(is.na(.)))) %&gt;%\n  \n  # 2. Pivot the data from wide to long\n  pivot_longer(everything(),\n               names_to = \"Column\",\n               values_to = \"NA_Count\") %&gt;%\n  \n  # 3. (Optional) Filter to only show columns that HAVE NAs\n  filter(NA_Count &gt; 0) %&gt;%\n  \n  # 4. (Optional) Add a percentage column\n  mutate(NA_Percentage = (NA_Count / total_rows) * 100) %&gt;%\n\n  # 5. Sort by the highest NA count\n  arrange(desc(NA_Count))\n\n# Print the clean table\nprint(na_summary)\n\n\n# A tibble: 3 × 3\n  Column         NA_Count NA_Percentage\n  &lt;chr&gt;             &lt;int&gt;         &lt;dbl&gt;\n1 Age                4500            15\n2 Glucose            4500            15\n3 Blood.Pressure     4500            15\n\n\nSample of the Dataset in the form of a data table to make visually clear how the dataset looks like and what features are available.\n\nprint(\"all_states_data (tsibble):\")\n\n[1] \"all_states_data (tsibble):\"\n\nDT::datatable(health_data, options = list(pageLength = 5))\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html"
  },
  {
    "objectID": "posts/renan-blog-post-4/index.html#data-preprocessing-and-cleaning",
    "href": "posts/renan-blog-post-4/index.html#data-preprocessing-and-cleaning",
    "title": "Health Risk Dataset",
    "section": "4. Data Preprocessing and Cleaning",
    "text": "4. Data Preprocessing and Cleaning"
  },
  {
    "objectID": "posts/renan-blog-post-4/index.html#determination-of-relevant-variables",
    "href": "posts/renan-blog-post-4/index.html#determination-of-relevant-variables",
    "title": "Health Risk Dataset",
    "section": "5. Determination of Relevant Variables",
    "text": "5. Determination of Relevant Variables\nThe outcome of risk must be defined as a target variable in order to determine the most relevant variables related to healthcare risks.\n\n5.1 Define a Target Variable\n\n\n5.2 Feature Selection Methodology\n\n\n\nReferences"
  },
  {
    "objectID": "posts/renan-blog-post-3/index.html",
    "href": "posts/renan-blog-post-3/index.html",
    "title": "Project 2 - Data Cubbing and Binning",
    "section": "",
    "text": "For the Project 2 we further processed our dataset by leveraging binning and a data cube structure built on hierarchies ."
  },
  {
    "objectID": "posts/renan-blog-post-3/index.html#introduction",
    "href": "posts/renan-blog-post-3/index.html#introduction",
    "title": "Project 2 - Data Cubbing and Binning",
    "section": "1. Introduction",
    "text": "1. Introduction\nAnalyzing the State_time_series.csv dataset, which contains granular real estate metrics over time, benefits significantly from data transformation and aggregation. Binning is essential for this process, as it converts continuous variables, such as MedianListingPrice_AllHomes, into discrete, manageable categories (e.g., ‘$150k-$200k’, ‘$200k-$250k’). This discretization simplifies complex data, making it easier to summarize, visualize, and identify trends.\nFurthermore, leveraging a data cube structure built on hierarchies allows for powerful, multidimensional analysis. For instance, the Date field isn’t just a single point in time; it’s part of a hierarchy that can be “rolled up” from a specific day to a Month, Quarter, or Year. Similarly, the RegionName (State) could be aggregated into broader geographical regions (e.g., “Northeast”, “West Coast”). By combining these binned and hierarchical dimensions, we can quickly “slice and dice” the data to answer complex questions, such as “How many homes in the $200k-$250k price bin were available in the Northeast region during Q3 2018?” This turns a massive, raw dataset into a flexible tool for gaining actionable insights.\nThe Zillow Home Value Index or ZHVI is a smoothed, seasonally adjusted measure of the typical home value and market changes across a given region and housing type. It reflects the typical value for homes in the 35th to 65th percentile range.\nThe Dataset Zillow Economics Data[ZHVI?], can be downloaded:\n```{bash}\n#!/bin/bash\ncurl -L -o ~/Downloads/zecon.zip\\\n  https://www.kaggle.com/api/v1/datasets/download/zillow/zecon\n```\n\n\n\n1.1 Load packages\n\n# install.packages(\"fpp3\")\nlibrary(fpp3)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggtime)\nlibrary(patchwork)\nlibrary(DT)\n\nPossible errors makings notes:\n```{bash}\nAttaching package: ‘ggtime’\n\nThe following objects are masked from ‘package:feasts’:\n\n    gg_arma, gg_irf, gg_lag, gg_season, gg_subseries, gg_tsdisplay,\n    gg_tsresiduals\n```"
  },
  {
    "objectID": "posts/renan-blog-post-3/index.html#load-the-dataset",
    "href": "posts/renan-blog-post-3/index.html#load-the-dataset",
    "title": "Project 2 - Data Cubbing and Binning",
    "section": "2. Load the Dataset",
    "text": "2. Load the Dataset\nThe following code will locate the folder datasets and then it will give back the variable datasets_path which you can use to build the path to the desired data to be loaded.\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nzillow_economics_data_path &lt;- file.path(datasets_path, \"zillow-economics-data-01\")\n\nstate_time_series &lt;- file.path(zillow_economics_data_path, \"State_time_series.csv\")\nall_states_data &lt;- read.csv(state_time_series)"
  },
  {
    "objectID": "posts/renan-blog-post-3/index.html#data-exploration-and-processing",
    "href": "posts/renan-blog-post-3/index.html#data-exploration-and-processing",
    "title": "Project 2 - Data Cubbing and Binning",
    "section": "3. Data Exploration and Processing",
    "text": "3. Data Exploration and Processing\nThe ‘Date’ column is a character and should be converted to a Date object.\n\nall_states_data$Date &lt;- as.Date(all_states_data$Date)\n# str(all_states_data)\n\nExploring the N/As and the structure of the dataset:\n\n# Get the total number of rows for calculating percentages\ntotal_rows &lt;- nrow(all_states_data)\n\n# Create a pretty summary table\nna_summary &lt;- all_states_data %&gt;%\n  # 1. Count NAs for every column\n  summarise(across(everything(), ~sum(is.na(.)))) %&gt;%\n  \n  # 2. Pivot the data from wide to long\n  pivot_longer(everything(),\n               names_to = \"Column\",\n               values_to = \"NA_Count\") %&gt;%\n  \n  # 3. (Optional) Filter to only show columns that HAVE NAs\n  filter(NA_Count &gt; 0) %&gt;%\n  \n  # 4. (Optional) Add a percentage column\n  mutate(NA_Percentage = (NA_Count / total_rows) * 100) %&gt;%\n\n  # 5. Sort by the highest NA count\n  arrange(desc(NA_Count))\n\n# Print the clean table\nprint(na_summary)\n\n# A tibble: 80 × 3\n   Column                                  NA_Count NA_Percentage\n   &lt;chr&gt;                                      &lt;int&gt;         &lt;dbl&gt;\n 1 PctOfHomesSellingForGain_AllHomes          12609          95.4\n 2 PctOfHomesSellingForLoss_AllHomes          12609          95.4\n 3 MedianRentalPrice_5BedroomOrMore           11994          90.8\n 4 MedianRentalPricePerSqft_5BedroomOrMore    11752          88.9\n 5 MedianRentalPricePerSqft_Studio            10875          82.3\n 6 MedianRentalPrice_CondoCoop                10437          79.0\n 7 MedianRentalPricePerSqft_DuplexTriplex     10293          77.9\n 8 MedianRentalPrice_Studio                   10211          77.3\n 9 MedianListingPrice_1Bedroom                10205          77.2\n10 MedianRentalPrice_DuplexTriplex            10068          76.2\n# ℹ 70 more rows\n\n\nSample of the Dataset in the form of a data table to make visually clear how the dataset looks like and what features are available.\n\nprint(\"all_states_data (tsibble):\")\n\n[1] \"all_states_data (tsibble):\"\n\nDT::datatable(all_states_data, options = list(pageLength = 5))\n\nWarning in instance$preRenderHook(instance): It seems your data is too big for\nclient-side DataTables. You may consider server-side processing:\nhttps://rstudio.github.io/DT/server.html\n\n\n\n\n\n\n\nlibrary(skimr)\nskim(all_states_data)\n\n\nData summary\n\n\nName\nall_states_data\n\n\nNumber of rows\n13212\n\n\nNumber of columns\n82\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nDate\n1\n\n\nnumeric\n80\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nRegionName\n0\n1\n4\n18\n0\n52\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nDate\n0\n1\n1996-04-30\n2017-12-31\n2007-05-31\n261\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDaysOnZillow_AllHomes\n8367\n0.37\n110.12\n27.47\n49.25\n90.25\n108.50\n126.75\n251.62\n▃▇▃▁▁\n\n\nInventorySeasonallyAdjusted_AllHomes\n8316\n0.37\n33292.66\n34926.70\n972.00\n9828.50\n21712.50\n47453.00\n260687.00\n▇▂▁▁▁\n\n\nInventoryRaw_AllHomes\n8316\n0.37\n33299.01\n35014.16\n911.00\n9756.25\n21289.00\n46891.00\n268055.00\n▇▂▁▁▁\n\n\nMedianListingPricePerSqft_1Bedroom\n9626\n0.27\n182.47\n99.81\n57.14\n125.69\n162.75\n202.63\n627.55\n▇▅▁▁▁\n\n\nMedianListingPricePerSqft_2Bedroom\n8678\n0.34\n135.49\n74.25\n60.00\n92.16\n121.30\n152.24\n550.64\n▇▂▁▁▁\n\n\nMedianListingPricePerSqft_3Bedroom\n8605\n0.35\n129.45\n58.84\n56.48\n93.28\n116.63\n143.19\n460.47\n▇▃▁▁▁\n\n\nMedianListingPricePerSqft_4Bedroom\n8535\n0.35\n133.46\n58.34\n61.80\n99.60\n119.82\n144.75\n480.77\n▇▂▁▁▁\n\n\nMedianListingPricePerSqft_5BedroomOrMore\n8643\n0.35\n135.67\n64.92\n63.78\n99.08\n119.75\n149.35\n617.96\n▇▁▁▁▁\n\n\nMedianListingPricePerSqft_AllHomes\n8538\n0.35\n136.66\n68.76\n62.14\n96.01\n120.58\n153.44\n520.72\n▇▂▁▁▁\n\n\nMedianListingPricePerSqft_CondoCoop\n9063\n0.31\n163.55\n99.42\n61.92\n113.08\n141.00\n177.07\n1000.00\n▇▁▁▁▁\n\n\nMedianListingPricePerSqft_DuplexTriplex\n9248\n0.30\n97.09\n57.02\n32.14\n60.89\n81.21\n113.52\n446.43\n▇▂▁▁▁\n\n\nMedianListingPricePerSqft_SingleFamilyResidence\n8573\n0.35\n133.37\n60.79\n63.27\n95.55\n120.09\n149.84\n475.36\n▇▂▁▁▁\n\n\nMedianListingPrice_1Bedroom\n10205\n0.23\n147083.00\n67877.11\n49900.00\n99000.00\n130000.00\n169900.00\n399000.00\n▇▇▂▁▁\n\n\nMedianListingPrice_2Bedroom\n8839\n0.33\n158872.73\n80629.40\n57000.00\n109500.00\n147000.00\n179900.00\n599000.00\n▇▃▁▁▁\n\n\nMedianListingPrice_3Bedroom\n8842\n0.33\n209225.76\n89233.54\n109900.00\n149000.00\n189900.00\n240000.00\n687000.00\n▇▃▁▁▁\n\n\nMedianListingPrice_4Bedroom\n8876\n0.33\n310849.76\n115574.82\n169000.00\n238744.50\n283020.00\n339900.00\n950000.00\n▇▃▁▁▁\n\n\nMedianListingPrice_5BedroomOrMore\n8989\n0.32\n416375.41\n191106.68\n159900.00\n310000.00\n369700.00\n452172.50\n1847500.00\n▇▂▁▁▁\n\n\nMedianListingPrice_AllHomes\n8966\n0.32\n223378.49\n84461.60\n112944.00\n159900.00\n209000.00\n259900.00\n610000.00\n▇▆▁▁▁\n\n\nMedianListingPrice_CondoCoop\n9402\n0.29\n202333.42\n79067.15\n82500.00\n152362.50\n184900.00\n228000.00\n754500.00\n▇▃▁▁▁\n\n\nMedianListingPrice_DuplexTriplex\n9323\n0.29\n207474.74\n119710.56\n64900.00\n129900.00\n178900.00\n245000.00\n939000.00\n▇▂▁▁▁\n\n\nMedianListingPrice_SingleFamilyResidence\n9082\n0.31\n228169.61\n96836.13\n112900.00\n159900.00\n209900.00\n265000.00\n725000.00\n▇▅▁▁▁\n\n\nMedianPctOfPriceReduction_AllHomes\n8724\n0.34\n3.85\n0.82\n1.74\n3.24\n3.72\n4.35\n8.34\n▂▇▃▁▁\n\n\nMedianPctOfPriceReduction_CondoCoop\n9340\n0.29\n4.02\n1.06\n1.68\n3.26\n3.81\n4.56\n10.00\n▅▇▂▁▁\n\n\nMedianPctOfPriceReduction_SingleFamilyResidence\n8724\n0.34\n3.85\n0.82\n1.72\n3.23\n3.74\n4.35\n8.35\n▂▇▃▁▁\n\n\nMedianPriceCutDollar_AllHomes\n8724\n0.34\n8034.11\n3071.98\n5000.00\n5100.00\n7500.00\n10000.00\n24000.00\n▇▆▁▁▁\n\n\nMedianPriceCutDollar_CondoCoop\n9340\n0.29\n7453.13\n2890.12\n2050.00\n5000.00\n6800.00\n10000.00\n27753.50\n▇▆▁▁▁\n\n\nMedianPriceCutDollar_SingleFamilyResidence\n8724\n0.34\n8244.97\n3547.92\n5000.00\n5300.00\n7900.00\n10000.00\n26000.00\n▇▅▁▁▁\n\n\nMedianRentalPricePerSqft_1Bedroom\n9588\n0.27\n1.40\n0.57\n0.72\n0.99\n1.21\n1.61\n3.37\n▇▅▂▁▁\n\n\nMedianRentalPricePerSqft_2Bedroom\n9065\n0.31\n1.11\n0.45\n0.52\n0.81\n0.95\n1.26\n3.11\n▇▅▁▁▁\n\n\nMedianRentalPricePerSqft_3Bedroom\n8985\n0.32\n0.97\n0.32\n0.53\n0.75\n0.86\n1.07\n2.45\n▇▃▁▁▁\n\n\nMedianRentalPricePerSqft_4Bedroom\n9808\n0.26\n0.85\n0.25\n0.49\n0.68\n0.78\n0.94\n2.13\n▇▅▁▁▁\n\n\nMedianRentalPricePerSqft_5BedroomOrMore\n11752\n0.11\n0.76\n0.16\n0.36\n0.63\n0.74\n0.89\n1.27\n▁▇▆▃▁\n\n\nMedianRentalPricePerSqft_AllHomes\n8864\n0.33\n1.01\n0.42\n0.58\n0.76\n0.88\n1.12\n3.15\n▇▂▁▁▁\n\n\nMedianRentalPricePerSqft_CondoCoop\n10004\n0.24\n1.30\n0.68\n0.64\n0.90\n1.07\n1.41\n4.82\n▇▁▁▁▁\n\n\nMedianRentalPricePerSqft_DuplexTriplex\n10293\n0.22\n1.07\n0.56\n0.50\n0.74\n0.87\n1.12\n3.86\n▇▁▁▁▁\n\n\nMedianRentalPricePerSqft_MultiFamilyResidence5PlusUnits\n9189\n0.30\n1.17\n0.50\n0.59\n0.84\n1.00\n1.30\n3.38\n▇▃▁▁▁\n\n\nMedianRentalPricePerSqft_SingleFamilyResidence\n8923\n0.32\n0.94\n0.28\n0.58\n0.75\n0.86\n1.03\n2.44\n▇▃▁▁▁\n\n\nMedianRentalPricePerSqft_Studio\n10875\n0.18\n1.18\n0.74\n0.57\n0.74\n0.85\n1.26\n3.98\n▇▂▁▁▁\n\n\nMedianRentalPrice_1Bedroom\n9686\n0.27\n978.56\n431.03\n495.00\n650.00\n860.00\n1195.00\n2690.00\n▇▃▁▁▁\n\n\nMedianRentalPrice_2Bedroom\n9168\n0.31\n1097.62\n473.15\n575.00\n775.00\n925.00\n1279.00\n3215.00\n▇▃▁▁▁\n\n\nMedianRentalPrice_3Bedroom\n9075\n0.31\n1356.52\n465.58\n750.00\n1050.00\n1200.00\n1515.00\n3550.00\n▇▃▁▁▁\n\n\nMedianRentalPrice_4Bedroom\n9856\n0.25\n1722.48\n486.16\n950.00\n1395.00\n1595.00\n1950.00\n3850.00\n▇▆▂▁▁\n\n\nMedianRentalPrice_5BedroomOrMore\n11994\n0.09\n2139.04\n698.33\n795.00\n1750.00\n1995.00\n2495.00\n4500.00\n▂▇▃▁▁\n\n\nMedianRentalPrice_AllHomes\n9060\n0.31\n1362.29\n455.57\n750.00\n1050.00\n1200.00\n1590.00\n3600.00\n▇▃▁▁▁\n\n\nMedianRentalPrice_CondoCoop\n10437\n0.21\n1410.25\n480.29\n697.50\n1050.00\n1295.00\n1595.00\n3200.00\n▇▇▂▂▁\n\n\nMedianRentalPrice_DuplexTriplex\n10068\n0.24\n963.56\n434.44\n500.00\n685.00\n800.00\n1100.00\n2895.00\n▇▂▁▁▁\n\n\nMedianRentalPrice_MultiFamilyResidence5PlusUnits\n9029\n0.32\n1091.13\n449.92\n550.00\n750.00\n950.00\n1296.00\n2895.00\n▇▃▁▁▁\n\n\nMedianRentalPrice_SingleFamilyResidence\n9120\n0.31\n1362.84\n442.51\n750.00\n1050.00\n1205.00\n1582.25\n3400.00\n▇▅▁▁▁\n\n\nMedianRentalPrice_Studio\n10211\n0.23\n1220.91\n353.50\n490.00\n975.00\n1150.00\n1400.00\n2500.00\n▂▇▅▁▁\n\n\nZHVIPerSqft_AllHomes\n620\n0.95\n116.42\n62.91\n35.00\n77.00\n98.00\n141.00\n499.00\n▇▃▁▁▁\n\n\nPctOfHomesDecreasingInValues_AllHomes\n4292\n0.68\n33.29\n22.47\n0.18\n16.98\n27.45\n45.08\n99.38\n▇▇▃▂▁\n\n\nPctOfHomesIncreasingInValues_AllHomes\n4292\n0.68\n59.03\n23.77\n0.47\n44.22\n63.54\n76.30\n99.76\n▂▃▅▇▅\n\n\nPctOfHomesSellingForGain_AllHomes\n12609\n0.05\n85.88\n11.99\n50.99\n79.09\n89.97\n95.63\n100.00\n▁▁▂▃▇\n\n\nPctOfHomesSellingForLoss_AllHomes\n12609\n0.05\n14.12\n11.99\n0.00\n4.36\n10.03\n20.91\n49.01\n▇▃▂▁▁\n\n\nPctOfListingsWithPriceReductionsSeasAdj_AllHomes\n8724\n0.34\n12.38\n2.22\n4.51\n10.91\n12.31\n13.92\n20.64\n▁▃▇▃▁\n\n\nPctOfListingsWithPriceReductionsSeasAdj_CondoCoop\n9164\n0.31\n10.39\n2.60\n2.23\n8.53\n10.38\n12.19\n19.61\n▁▅▇▃▁\n\n\nPctOfListingsWithPriceReductionsSeasAdj_SingleFamilyResidence\n8724\n0.34\n12.60\n2.27\n4.44\n11.09\n12.54\n14.11\n20.76\n▁▃▇▃▁\n\n\nPctOfListingsWithPriceReductions_AllHomes\n8724\n0.34\n12.36\n3.00\n3.56\n10.23\n12.30\n14.37\n20.92\n▁▅▇▅▁\n\n\nPctOfListingsWithPriceReductions_CondoCoop\n9164\n0.31\n10.38\n3.00\n1.79\n8.25\n10.25\n12.38\n22.90\n▁▇▇▂▁\n\n\nPctOfListingsWithPriceReductions_SingleFamilyResidence\n8724\n0.34\n12.58\n3.07\n3.74\n10.42\n12.50\n14.62\n21.69\n▁▅▇▅▁\n\n\nPriceToRentRatio_AllHomes\n8912\n0.33\n11.44\n2.22\n7.05\n9.73\n11.14\n12.74\n21.55\n▆▇▃▁▁\n\n\nSale_Counts\n7837\n0.41\n7065.63\n7978.75\n130.00\n1672.00\n4546.00\n9247.00\n50275.00\n▇▂▁▁▁\n\n\nSale_Counts_Seas_Adj\n7837\n0.41\n7049.04\n7808.91\n242.00\n1713.00\n4764.00\n9393.00\n41779.00\n▇▂▁▁▁\n\n\nSale_Prices\n9218\n0.30\n194551.78\n75755.80\n83800.00\n137000.00\n180900.00\n235775.00\n543100.00\n▇▆▂▁▁\n\n\nZHVI_1bedroom\n2607\n0.80\n117060.42\n61056.04\n30900.00\n74600.00\n100400.00\n142300.00\n390200.00\n▇▆▁▁▁\n\n\nZHVI_2bedroom\n1467\n0.89\n135168.80\n72267.65\n32800.00\n86700.00\n115400.00\n166800.00\n542400.00\n▇▅▁▁▁\n\n\nZHVI_3bedroom\n425\n0.97\n167062.80\n80212.51\n49600.00\n116400.00\n141200.00\n204400.00\n639700.00\n▇▃▁▁▁\n\n\nZHVI_4bedroom\n853\n0.94\n243829.67\n106216.76\n64700.00\n174900.00\n218000.00\n281000.00\n850400.00\n▇▆▁▁▁\n\n\nZHVI_5BedroomOrMore\n1398\n0.89\n323733.49\n170291.68\n68600.00\n217900.00\n288000.00\n365300.00\n1497000.00\n▇▂▁▁▁\n\n\nZHVI_AllHomes\n774\n0.94\n169753.41\n83882.41\n38200.00\n114500.00\n144750.00\n207600.00\n620400.00\n▇▅▁▁▁\n\n\nZHVI_BottomTier\n896\n0.93\n102669.97\n49705.35\n32600.00\n66600.00\n87400.00\n128200.00\n335600.00\n▇▅▂▁▁\n\n\nZHVI_CondoCoop\n1530\n0.88\n156769.89\n79695.00\n42200.00\n111300.00\n134700.00\n175800.00\n782900.00\n▇▂▁▁▁\n\n\nZHVI_MiddleTier\n774\n0.94\n169753.41\n83882.41\n38200.00\n114500.00\n144750.00\n207600.00\n620400.00\n▇▅▁▁▁\n\n\nZHVI_SingleFamilyResidence\n774\n0.94\n174154.38\n92243.37\n37900.00\n115000.00\n147300.00\n211775.00\n737500.00\n▇▃▁▁▁\n\n\nZHVI_TopTier\n688\n0.95\n293973.81\n147829.06\n70900.00\n194700.00\n251100.00\n349400.00\n988100.00\n▇▆▂▁▁\n\n\nZRI_AllHomes\n8958\n0.32\n1321.33\n371.01\n799.00\n1047.00\n1210.00\n1474.00\n2690.00\n▇▇▂▁▁\n\n\nZRI_AllHomesPlusMultifamily\n8876\n0.33\n1318.06\n368.78\n799.00\n1036.00\n1210.00\n1477.00\n2653.00\n▇▇▂▁▁\n\n\nZriPerSqft_AllHomes\n8876\n0.33\n0.93\n0.28\n0.56\n0.73\n0.86\n1.07\n2.29\n▇▅▁▁▁\n\n\nZri_MultiFamilyResidenceRental\n8876\n0.33\n1233.05\n369.43\n713.00\n959.75\n1126.00\n1399.50\n2606.00\n▇▅▃▁▁\n\n\nZri_SingleFamilyResidenceRental\n8958\n0.32\n1327.52\n383.31\n799.00\n1039.25\n1220.00\n1467.75\n2754.00\n▇▆▂▁▁\n\n\n\n\n\n\n\n# --- 1. Create Date Hierarchy ---\n# We parse the Date and extract hierarchy levels\nprocessed_data &lt;- all_states_data %&gt;%\n  mutate(\n    Year = year(Date),\n    Quarter = quarter(Date, with_year = TRUE),\n    Month = month(Date, label = TRUE)\n  )\n\n\n# --- 2. Create Bins for a Measure ---\n# We bin 'MedianListingPrice_AllHomes' into categories\nprice_breaks &lt;- c(0, 150000, 200000, 250000, 300000, 500000, Inf)\nprice_labels &lt;- c(\n  \"Under $150k\", \"$150k-$200k\", \"$200k-$250k\",\n  \"$250k-$300k\", \"$300k-$500k\", \"Over $500k\"\n)\n\n\nprocessed_data &lt;- processed_data %&gt;%\n  mutate(\n    Price_Bin = cut(MedianListingPrice_AllHomes,\n                    breaks = price_breaks,\n                    labels = price_labels,\n                    right = FALSE) # Bins are [min, max)\n  )\n\n\n# Show the new hierarchical and binned columns\nprint(\"Data with new hierarchy and bins:\")\n\n[1] \"Data with new hierarchy and bins:\"\n\nglimpse(processed_data %&gt;%\n  select(RegionName, Date, Year, Quarter, Month, MedianListingPrice_AllHomes, Price_Bin))\n\nRows: 13,212\nColumns: 7\n$ RegionName                  &lt;chr&gt; \"Alabama\", \"Arizona\", \"Arkansas\", \"Califor…\n$ Date                        &lt;date&gt; 1996-04-30, 1996-04-30, 1996-04-30, 1996-…\n$ Year                        &lt;dbl&gt; 1996, 1996, 1996, 1996, 1996, 1996, 1996, …\n$ Quarter                     &lt;dbl&gt; 1996.2, 1996.2, 1996.2, 1996.2, 1996.2, 19…\n$ Month                       &lt;ord&gt; Apr, Apr, Apr, Apr, Apr, Apr, Apr, Apr, Ap…\n$ MedianListingPrice_AllHomes &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Price_Bin                   &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…"
  },
  {
    "objectID": "posts/renan-blog-post-3/index.html#explore-zhvi-with-date-hierarchy",
    "href": "posts/renan-blog-post-3/index.html#explore-zhvi-with-date-hierarchy",
    "title": "Project 2 - Data Cubbing and Binning",
    "section": "4. Explore ZHVI with Date Hierarchy",
    "text": "4. Explore ZHVI with Date Hierarchy\nFor sake of simplicity we are going to focus for now on a single State (California) and the Zillow Home Value Index (ZHVI_AllHomes) over time.\n\n# Let's analyze the Zillow Home Value Index (ZHVI_AllHomes) for \"California\".\n# all_states_data\ncali_zhvi &lt;- processed_data %&gt;%\n  filter(RegionName == \"California\") %&gt;%\n  select(Date, ZHVI_AllHomes) %&gt;%\n  # Remove any missing values for this metric\n  na.omit()\n\ncali_ts &lt;- cali_zhvi %&gt;%\n  as_tsibble(index = Date)\n\n\n# We set 'Date' as the 'index' (the time component).\ncali_ts_fill_gaps &lt;- cali_zhvi %&gt;%\n  as_tsibble(index = Date) %&gt;%\n  fill_gaps()\n# If you were analyzing multiple states, you would add a 'key'.\n# Example for multiple states (not run here):\n# multi_state_ts &lt;- all_states_data %&gt;%\n#   select(Date, RegionName, ZHVI_AllHomes) %&gt;%\n#   as_tsibble(index = Date, key = RegionName)\n\nLets see how many N/As after filling the gaps:\n\n# Count total NAs per column\ncolSums(is.na(cali_ts))\ncolSums(is.na(cali_ts_fill_gaps))\n\n\n# Inspect the new, focused data frame\nhead(cali_zhvi)\n\n        Date ZHVI_AllHomes\n1 1996-04-30        157900\n2 1996-05-31        157800\n3 1996-06-30        157500\n4 1996-07-31        157300\n5 1996-08-31        157000\n6 1996-09-30        156800\n\nhead(cali_ts)\n\n# A tsibble: 6 x 2 [1D]\n  Date       ZHVI_AllHomes\n  &lt;date&gt;             &lt;int&gt;\n1 1996-04-30        157900\n2 1996-05-31        157800\n3 1996-06-30        157500\n4 1996-07-31        157300\n5 1996-08-31        157000\n6 1996-09-30        156800\n\nhead(cali_ts_fill_gaps)\n\n# A tsibble: 6 x 2 [1D]\n  Date       ZHVI_AllHomes\n  &lt;date&gt;             &lt;int&gt;\n1 1996-04-30        157900\n2 1996-05-01            NA\n3 1996-05-02            NA\n4 1996-05-03            NA\n5 1996-05-04            NA\n6 1996-05-05            NA\n\n\n\ncali_ts %&gt;%\n  autoplot(ZHVI_AllHomes) +\n  labs(title = \"Zillow Home Value Index (ZHVI) for California\",\n       y = \"Home Value Index\",\n       x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# This will scan cali_ts and report any gaps\ngap_summary &lt;- count_gaps(cali_ts)\n\n# Print the summary\nprint(gap_summary)\n\n# A tibble: 260 × 3\n   .from      .to           .n\n   &lt;date&gt;     &lt;date&gt;     &lt;int&gt;\n 1 1996-05-01 1996-05-30    30\n 2 1996-06-01 1996-06-29    29\n 3 1996-07-01 1996-07-30    30\n 4 1996-08-01 1996-08-30    30\n 5 1996-09-01 1996-09-29    29\n 6 1996-10-01 1996-10-30    30\n 7 1996-11-01 1996-11-29    29\n 8 1996-12-01 1996-12-30    30\n 9 1997-01-01 1997-01-30    30\n10 1997-02-01 1997-02-27    27\n# ℹ 250 more rows\n\n\n\ncali_ts_fill_gaps %&gt;%\n  autoplot(ZHVI_AllHomes) +\n  labs(title = \"Zillow Home Value Index (ZHVI) for California\",\n       y = \"Home Value Index\",\n       x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Inspect the new, focused data frame\n# print(\"cali_zhvi (data frame):\")\n# DT::datatable(cali_zhvi, options = list(pageLength = 5))\n\nprint(\"cali_ts (tsibble):\")\n\n[1] \"cali_ts (tsibble):\"\n\nDT::datatable(cali_ts, options = list(pageLength = 5))\n\n\n\n\n# print(\"cali_ts_fill_gaps (tsibble with filled gaps):\")\n# DT::datatable(cali_ts_fill_gaps, options = list(pageLength = 5))\n\n\nCreate a new ‘Month’ column using the yearmonth() function\nGroup by this new explicit month\nSummarise the data (using mean() is safe, but since you have one observation per month, last() or sum() would also work)\nConvert to a tsibble, now indexed by the new ‘Month’ object\n\n\ncali_ts_monthly &lt;- cali_zhvi %&gt;%\n  mutate(Month = yearmonth(Date)) %&gt;%\n  group_by(Month) %&gt;%\n  summarise(ZHVI_AllHomes = mean(ZHVI_AllHomes, na.rm = TRUE)) %&gt;%\n  as_tsibble(index = Month)\n\n# Check the new object\nprint(\"New Monthly Tsibble:\")\n\n[1] \"New Monthly Tsibble:\"\n\nhead(cali_ts_monthly)\n\n# A tsibble: 6 x 2 [1M]\n     Month ZHVI_AllHomes\n     &lt;mth&gt;         &lt;dbl&gt;\n1 1996 Apr        157900\n2 1996 May        157800\n3 1996 Jun        157500\n4 1996 Jul        157300\n5 1996 Aug        157000\n6 1996 Sep        156800\n\n# Note the &lt;mth&gt; tag in the output, indicating it's a monthly tsibble\nprint(\"Class of the new index:\")\n\n[1] \"Class of the new index:\"\n\nclass(cali_ts_monthly$Month)\n\n[1] \"yearmonth\"  \"vctrs_vctr\"\n\n\n\ncali_ts_monthly %&gt;%\n  autoplot(ZHVI_AllHomes) +\n  labs(title = \"Zillow Home Value Index (ZHVI) for California\",\n       y = \"Home Value Index\",\n       x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn R, Date objects are stored internally as the number of days that have passed since an “origin” date, which by default is January 1, 1970.\nThe number 9587 is the number of days since 1970-01-01.\nThis corresponds to the date April 30, 1996, which is the starting point of your Zillow dataset.\nThis happens if you (or R) accidentally convert a Date object to a plain number.\nThe yearmonth object we created in the last step is different. It prints as “1996 Apr” and internally stores the number of months since the 1970 epoch (which would be a much smaller number, like 316).\n\nprint(\"cali_ts_monthly (tsibble with only monthly):\")\n\n[1] \"cali_ts_monthly (tsibble with only monthly):\"\n\nDT::datatable(cali_ts_monthly, options = list(pageLength = 5))\n\n\n\n\n\n\n\nReflecting on how the ZHVI is calculated and how it compares to other ways to calculate Home value index like: Case-Shiller\nThe Standard & Poor’s CoreLogic Case–Shiller Home Price Indices are repeat-sales house price indices for the United States.\nFipeZAP Index\nConceptual Accuracy (What it gets right)\nThe R code correctly captures the concept of measuring the middle-third of the market:\nFocus on the Middle Tier: The core idea of calculating the ZHVI is to use an aggressively trimmed mean—specifically the mean of values between the 35th and 65th percentile. The code successfully identifies these cut-off points (\\(\\mathbf{q35}\\) and \\(\\mathbf{q65}\\)) and calculates the mean of the values within that range. This properly simulates the “robust” nature of the ZHVI, which aims to exclude extreme high-end and low-end sales that might skew a simple median or average.\nMajor Flaws (What it gets wrong)\n\nThe Data Source is Incorrect (Sales vs. All Homes)\n\n\nYour Code’s Flaw: The input data, hypothetical_sales, is a list of sales transactions.\nActual ZHVI Method: The ZHVI is calculated using the Zestimate for every single home (over 100 million properties), not just the few that sold in a given month.\n\nImpact: Indices based on sales (like Case-Shiller) are susceptible to the compositional bias of which homes happen to sell that month. The ZHVI avoids this by including the estimated value (Zestimate) for all homes, providing a much more comprehensive view of the entire market’s value, whether a house sold or not.\n\n\n\nIt Calculates a Static Value, Not Appreciation\n\n\nYour Code’s Flaw: The output, simulated_zhvi, is a single-month dollar value.\nActual ZHVI Method: The ZHVI is an index that tracks the change in home values over time (appreciation). It is a chained index where the monthly value is determined by:\n\nCalculating the weighted mean appreciation of individual Zestimates from the prior month to the current month.\nApplying that appreciation rate to the ZHVI level from the prior month.\n\n\nImpact: To calculate a correct ZHVI, you would need two separate sets of Zestimates (Time \\(t-1\\) and Time \\(t\\)), calculate the percentage change for each home, and then average those changes to determine the appreciation factor for the overall index. The code calculates a robust mean, but it doesn’t show how that mean changes month-over-month to create the index.\n\n\nHypothetical Fix for Flaw 2 (Conceptual only)\nA conceptually more accurate approach would require comparing two months: 1. Month 1 Zestimates (t-1): Calculate the \\(\\text{Mean}_{35-65}\\) of Zestimates for Month 1.\n\nMonth 2 Zestimates (t): Calculate the \\(\\text{Mean}_{35-65}\\) of Zestimates for Month 2.\nAppreciation:\n\n\\[\\text{Monthly Appreciation} = \\frac{\\text{Mean}_{35-65}(t)}{\\text{Mean}_{35-65}(t-1)} - 1\\]\n\nZHVI Calculation: \\(\\text{ZHVI}(t) = \\text{ZHVI}(t-1) \\times (1 + \\text{Monthly Appreciation})\\)\n\n\n# this seems to be incorrect\n\n# 1. Create a hypothetical set of 100 home sales.\n# We'll use random numbers for this example.\nset.seed(42) # Makes our \"random\" numbers reproducible\nhypothetical_sales &lt;- round(rnorm(100, mean = 350000, sd = 75000))\n\n# 2. Find the 35th and 65th percentile values\n# These are the \"cut-off\" points.\nq35 &lt;- quantile(hypothetical_sales, 0.35)\nq65 &lt;- quantile(hypothetical_sales, 0.65)\n\n# 3. Filter to get only the \"middle-tier\" homes\n# (i.e., homes with a value between the 35th and 65th percentile)\nmiddle_tier_homes &lt;- hypothetical_sales[\n  hypothetical_sales &gt;= q35 & hypothetical_sales &lt;= q65\n]\n\n# 4. Calculate the \"Simulated ZHVI\"\n# This is the mean of only those middle-tier homes.\nsimulated_zhvi &lt;- mean(middle_tier_homes)\n\n# --- Print the results ---\nprint(paste(\"Total number of hypothetical sales:\", length(hypothetical_sales)))\n\n[1] \"Total number of hypothetical sales: 100\"\n\nprint(paste(\"35th Percentile Value:\", q35))\n\n[1] \"35th Percentile Value: 328947.5\"\n\nprint(paste(\"65th Percentile Value:\", q65))\n\n[1] \"65th Percentile Value: 389868.05\"\n\nprint(paste(\"Number of homes in middle-tier (35th-65th percentile):\", length(middle_tier_homes)))\n\n[1] \"Number of homes in middle-tier (35th-65th percentile): 30\"\n\nprint(paste(\"Simulated ZHVI (Mean of middle-tier):\", round(simulated_zhvi, 2)))\n\n[1] \"Simulated ZHVI (Mean of middle-tier): 359053.03\"\n\n\nZHVI is probably being calculated from most likely the sales data that generated the MedianListingPrice_AllHomes.\nMedianListingPrice_1Bedroom\nMedianListingPrice_2Bedroom\nMedianListingPrice_3Bedroom\nMedianListingPrice_4Bedroom\nMedianListingPrice_5BedroomOrMore\nMedianListingPrice_AllHomes\nMedianListingPrice_CondoCoop\nMedianListingPrice_DuplexTriplex\nMedianListingPrice_SingleFamilyResidence\n\nReferences"
  },
  {
    "objectID": "posts/binita-blog-post-1/index.html",
    "href": "posts/binita-blog-post-1/index.html",
    "title": "Adding Binning",
    "section": "",
    "text": "On this exploratory phase I will be adding binning to handle with the granularity and hierarchy of the dataset."
  },
  {
    "objectID": "posts/binita-blog-post-1/index.html#introduction",
    "href": "posts/binita-blog-post-1/index.html#introduction",
    "title": "Adding Binning",
    "section": "Introduction",
    "text": "Introduction\nThis Project is explained on Quarto/R Markdown code.\nThe loaded dataset finds the root on Git repository and constructs the path to the datasets folder and the CSV file.\nIt then loads the CSV into a data frame called all_states_data. A data frame all_states_data\nThe project contains a data frame all_states_data with 13,212 rows and 82 columns.\nTime series analysis packages (including tsibble, feasts, fable): fpp3\nData manipulation and cleaning: dplyr, tidyr\nPlotting: ggplot2\nExtends ggplot2 for time series plots: ggtime\nCombine multiple ggplots: patchwork"
  },
  {
    "objectID": "posts/binita-blog-post-1/index.html#load-packages",
    "href": "posts/binita-blog-post-1/index.html#load-packages",
    "title": "Adding Binning",
    "section": "1. Load packages",
    "text": "1. Load packages\n\n# install.packages(\"fpp3\")\nlibrary(fpp3)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggtime)\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/binita-blog-post-1/index.html#load-the-dataset",
    "href": "posts/binita-blog-post-1/index.html#load-the-dataset",
    "title": "Adding Binning",
    "section": "2. Load the Dataset",
    "text": "2. Load the Dataset\nThe following code will locate the folder datasets and then it will give back the variable datasets_path which you can use to build the path to the desired data to be loaded.\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nzillow_economics_data_path &lt;- file.path(datasets_path, \"zillow-economics-data-01\")\n\nstate_time_series &lt;- file.path(zillow_economics_data_path, \"State_time_series.csv\")\nall_states_data &lt;- read.csv(state_time_series)"
  },
  {
    "objectID": "posts/binita-blog-post-1/index.html#exploring-data",
    "href": "posts/binita-blog-post-1/index.html#exploring-data",
    "title": "Adding Binning",
    "section": "3. Exploring Data",
    "text": "3. Exploring Data\nLet’s convert Data column to Data type\n\n# convert data column to Data type\nall_states_data &lt;- all_states_data %&gt;%\nmutate(Date = as.Date(Date, format = \"%Y-%m-%d\"))\n\n\n# Check the structure\nstr(all_states_data)\n\n'data.frame':   13212 obs. of  82 variables:\n $ Date                                                         : Date, format: \"1996-04-30\" \"1996-04-30\" ...\n $ RegionName                                                   : chr  \"Alabama\" \"Arizona\" \"Arkansas\" \"California\" ...\n $ DaysOnZillow_AllHomes                                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ InventorySeasonallyAdjusted_AllHomes                         : int  NA NA NA NA NA NA NA NA NA NA ...\n $ InventoryRaw_AllHomes                                        : int  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_1Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_2Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_3Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_4Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_5BedroomOrMore                     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_AllHomes                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_CondoCoop                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_DuplexTriplex                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_SingleFamilyResidence              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_1Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_2Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_3Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_4Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_5BedroomOrMore                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_AllHomes                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_CondoCoop                                 : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_DuplexTriplex                             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_SingleFamilyResidence                     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPctOfPriceReduction_AllHomes                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPctOfPriceReduction_CondoCoop                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPctOfPriceReduction_SingleFamilyResidence              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPriceCutDollar_AllHomes                                : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPriceCutDollar_CondoCoop                               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPriceCutDollar_SingleFamilyResidence                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_1Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_2Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_3Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_4Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_5BedroomOrMore                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_AllHomes                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_CondoCoop                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_DuplexTriplex                       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_MultiFamilyResidence5PlusUnits      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_SingleFamilyResidence               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_Studio                              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_1Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_2Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_3Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_4Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_5BedroomOrMore                             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_AllHomes                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_CondoCoop                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_DuplexTriplex                              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_MultiFamilyResidence5PlusUnits             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_SingleFamilyResidence                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_Studio                                     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ ZHVIPerSqft_AllHomes                                         : int  50 62 42 102 82 85 71 56 55 185 ...\n $ PctOfHomesDecreasingInValues_AllHomes                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfHomesIncreasingInValues_AllHomes                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfHomesSellingForGain_AllHomes                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfHomesSellingForLoss_AllHomes                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductionsSeasAdj_AllHomes             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductionsSeasAdj_CondoCoop            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductionsSeasAdj_SingleFamilyResidence: num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductions_AllHomes                    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductions_CondoCoop                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductions_SingleFamilyResidence       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PriceToRentRatio_AllHomes                                    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Sale_Counts                                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Sale_Counts_Seas_Adj                                         : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Sale_Prices                                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ ZHVI_1bedroom                                                : int  61500 59200 53000 93700 77800 64700 90100 45400 74900 152300 ...\n $ ZHVI_2bedroom                                                : int  48900 86400 54500 123400 97500 97000 88200 65400 64700 186600 ...\n $ ZHVI_3bedroom                                                : int  78200 96100 76800 150900 129000 130400 103500 89100 88000 231800 ...\n $ ZHVI_4bedroom                                                : int  146500 128400 135100 196100 176100 194800 157800 133600 149700 303400 ...\n $ ZHVI_5BedroomOrMore                                          : int  206300 190500 186000 265300 212900 299800 176100 199900 212800 345500 ...\n $ ZHVI_AllHomes                                                : int  79500 103600 64400 157900 128100 132000 106800 86300 92000 227400 ...\n $ ZHVI_BottomTier                                              : int  45600 67100 38400 95100 82700 83700 77200 52500 57200 144500 ...\n $ ZHVI_CondoCoop                                               : int  99500 78900 70300 136100 99400 85000 NA 70600 89300 177000 ...\n $ ZHVI_MiddleTier                                              : int  79500 103600 64400 157900 128100 132000 106800 86300 92000 227400 ...\n $ ZHVI_SingleFamilyResidence                                   : int  79000 107500 64500 162000 133600 141000 107400 92100 92400 262600 ...\n $ ZHVI_TopTier                                                 : int  140200 168700 115200 270600 209300 231600 161600 155300 163900 374700 ...\n $ ZRI_AllHomes                                                 : int  NA NA NA NA NA NA NA NA NA NA ...\n $ ZRI_AllHomesPlusMultifamily                                  : int  NA NA NA NA NA NA NA NA NA NA ...\n $ ZriPerSqft_AllHomes                                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Zri_MultiFamilyResidenceRental                               : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Zri_SingleFamilyResidenceRental                              : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\nWe convert the Date column to proper Date type so it can be used in time series analysis.\nstr() shows structure: column names, types, and example data.\nOutput:\n\nData column is Data type.\nOther column include RegionName, ZHVI_AllHomes, and many Zillow metrics.\n\n\n# Convert to tsibble for time series analysis\n# Assuming 'State' is a column and 'Value' is the metric\n\nlibrary(tsibble)\n\nstate_tsibble &lt;- all_states_data %&gt;%\nas_tsibble(key = RegionName, index = Date)\n\nWe convert the data frame into a tsibble (time series tibble)\n\nkey = RegionName: each state is treated as a seperate time series\nindex = Date:the date column is the time index.\n\nOutput:\nstate_tsibble is now a tsibble with 13,212 rows and 82 columns, ready for time series operations.\n\n# Inspect first rows\nstate_tsibble %&gt;% head()\n\n# A tsibble: 6 x 82 [1D]\n# Key:       RegionName [1]\n  Date       RegionName DaysOnZillow_AllHomes InventorySeasonallyAdjusted_AllH…¹\n  &lt;date&gt;     &lt;chr&gt;                      &lt;dbl&gt;                              &lt;int&gt;\n1 1996-04-30 Alabama                       NA                                 NA\n2 1996-05-31 Alabama                       NA                                 NA\n3 1996-06-30 Alabama                       NA                                 NA\n4 1996-07-31 Alabama                       NA                                 NA\n5 1996-08-31 Alabama                       NA                                 NA\n6 1996-09-30 Alabama                       NA                                 NA\n# ℹ abbreviated name: ¹​InventorySeasonallyAdjusted_AllHomes\n# ℹ 78 more variables: InventoryRaw_AllHomes &lt;int&gt;,\n#   MedianListingPricePerSqft_1Bedroom &lt;dbl&gt;,\n#   MedianListingPricePerSqft_2Bedroom &lt;dbl&gt;,\n#   MedianListingPricePerSqft_3Bedroom &lt;dbl&gt;,\n#   MedianListingPricePerSqft_4Bedroom &lt;dbl&gt;,\n#   MedianListingPricePerSqft_5BedroomOrMore &lt;dbl&gt;, …"
  },
  {
    "objectID": "posts/binita-blog-post-1/index.html#binning-time-series-data",
    "href": "posts/binita-blog-post-1/index.html#binning-time-series-data",
    "title": "Adding Binning",
    "section": "4. Binning Time series Data",
    "text": "4. Binning Time series Data\n\n# Monthly aggregation (mean per month)\nlibrary(dplyr)\nlibrary(ggtime)\nlibrary(tsibble)\n\nmonthly_binned &lt;- state_tsibble %&gt;%\nindex_by(Month = yearmonth(Date)) %&gt;%\nsummarise(Avg_ZHVI_AllHomes = mean(ZHVI_AllHomes, na.rm = TRUE))\n\nmonthly_binned %&gt;% head()\n\n# A tsibble: 6 x 2 [1M]\n     Month Avg_ZHVI_AllHomes\n     &lt;mth&gt;             &lt;dbl&gt;\n1 1996 Apr           101744.\n2 1996 May           101440 \n3 1996 Jun           101555 \n4 1996 Jul           101678.\n5 1996 Aug           101815 \n6 1996 Sep           101998.\n\n\nindex_by(Month = yearmonth(Date)): groups the data by month.\nsummarise(Avg_ZHVI_AllHomes = mean(…)): calculates the average zillow home Value Index (ZHVI) per month.\nna.rm = TRUE: ignores missing values.\n\n# Quarterly aggregation example\nquarterly_binned &lt;- state_tsibble %&gt;%\n  index_by(Quarter = yearquarter(Date)) %&gt;%\n  summarise(Avg_ZHVI_AllHomes = mean(ZHVI_AllHomes, na.rm = TRUE))\n\nquarterly_binned %&gt;% head()\n\n# A tsibble: 6 x 2 [1Q]\n  Quarter Avg_ZHVI_AllHomes\n    &lt;qtr&gt;             &lt;dbl&gt;\n1 1996 Q2           101578.\n2 1996 Q3           101830 \n3 1996 Q4           102449.\n4 1997 Q1           103248.\n5 1997 Q2           104115.\n6 1997 Q3           105008 \n\n\nThe output is same as monthly aggregation, but grouped by quarter instead of month.\n\n# Plot binned data\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(tsibble)\nlibrary(ggtime)\n\n# Convert Month to a regular Date (first day of the month)\nmonthly_binned_df &lt;- monthly_binned %&gt;%\n  as_tibble() %&gt;%                    # convert to tibble\n  mutate(Month = as.Date(Month))     # convert yearmonth to Date\n\nWe convert the monthly_binned tsibble into a regular tibble.\nAlso, convert Month from yearmonth to a standard Date for ggplot.\nThe output monthly_binned_df has two columns: Month (Date) and Avg_ZHVI_AllHomes\n\n# Plot with ggplot\nggplot(monthly_binned_df, aes(x = Month, y = Avg_ZHVI_AllHomes)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Monthly Binned Average of ZHVI_AllHomes\",\n       x = \"Month\", y = \"Average ZHVI_AllHomes\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nWe plot a line chart of monthly average home values\n\nx = Month,\nY = Avg_ZHVI_AllHomes\n\ngeom_line(color = “blue”): draws the line in blue.\nalso, rotates x-axis labels for readability.\nA line graph shows trends in average home prices per month over time.\nPeaks and troughs indicate periods of rising/falling home prices.\n\nReferences"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html",
    "href": "posts/renan-blog-post-1/index.html",
    "title": "Welcome to the Project",
    "section": "",
    "text": "For this class setup a website to present the project"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html#introduction",
    "href": "posts/renan-blog-post-1/index.html#introduction",
    "title": "Welcome to the Project",
    "section": "Introduction",
    "text": "Introduction\nMake some cheesy introduction"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html#how-to-setup-renv",
    "href": "posts/renan-blog-post-1/index.html#how-to-setup-renv",
    "title": "Welcome to the Project",
    "section": "How to setup RENV",
    "text": "How to setup RENV\nWhat is renv ?? well, it is an R package that provides powerful dependency management for your projects, ensuring they are isolated, portable, and reproducible.\n```{bash}\nrenv::init()\n```"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html#how-to-setup-uv",
    "href": "posts/renan-blog-post-1/index.html#how-to-setup-uv",
    "title": "Welcome to the Project",
    "section": "How to setup UV",
    "text": "How to setup UV\nUV is an extremely fast, next-generation Python package installer and resolver built in Rust by Astral, by consolidating environment creation, package installation, and dependency locking into a single, blazing-fast command-line tool, UV drastically simplifies and accelerates Python development workflows, saving developers significant time and frustration.\n```{bash}\nuv add requests\nuv remove requests\nuv lock --upgrade-package requests\nuv venv\nuv sync\n```\nActivating a VENV\n```{bash}\nsource .venv/bin/activate\n```\nYou can add dependencies to your pyproject.toml with the uv add command. This will also update the lockfile and project environment:\n```{bash}\nuv add requests\n```\nTo remove a package, you can use uv remove:\n```{bash}\nuv remove requests\n```\nTo upgrade a package, run uv lock with the –upgrade-package flag:\n```{bash}\nuv lock --upgrade-package requests\n```\nCreating a virtual environment\n```{bash}\nuv venv\n```\nSyncing the environment\n```{bash}\nuv sync\n```"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html#getting-started-with-quarto",
    "href": "posts/renan-blog-post-1/index.html#getting-started-with-quarto",
    "title": "Welcome to the Project",
    "section": "Getting Started with Quarto",
    "text": "Getting Started with Quarto\nInstall the dependencies"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html#how-to-render-and-preview-your-project",
    "href": "posts/renan-blog-post-1/index.html#how-to-render-and-preview-your-project",
    "title": "Welcome to the Project",
    "section": "How to Render and Preview your project",
    "text": "How to Render and Preview your project\nEither use the IDE of your preference, VSCode has extensions to support\nor run on the cmd:\nTo render the quarto document run:\n```{bash}\nquarto render\n```\nTo preview the rendered quarto document run:\n```{bash}\nquarto preview\n```"
  },
  {
    "objectID": "posts/renan-blog-post-1/index.html#how-to-make-citations",
    "href": "posts/renan-blog-post-1/index.html#how-to-make-citations",
    "title": "Welcome to the Project",
    "section": "How to make Citations",
    "text": "How to make Citations\nBibTex is a great why to generate consistent and properly formatted Citations, to generate bibtex citations u can use GoogleScholar:\nExample: Add here some content explaining how to use GoogleScholar for citations\nAdd another example using citation generator\n\nCreate your BibTeX file (e.g., references.bib) and add your entries.\n\nExample:\n```{text}\n@inproceedings{Liu_2021,\n   title={Priority prediction of Asian Hornet sighting report using machine learning methods},\n   url={http://dx.doi.org/10.1109/SEAI52285.2021.9477549},\n   DOI={10.1109/seai52285.2021.9477549},\n   booktitle={2021 IEEE International Conference on Software Engineering and Artificial Intelligence (SEAI)},\n   publisher={IEEE},\n   author={Liu, Yixin and Guo, Jiaxin and Dong, Jieyang and Jiang, Luoqian and Ouyang, Haoyuan},\n   year={2021},\n   month=jun, pages={7–11} }\n```\n\nLink the file in your Quarto YAML header (the part at the very top):\n\n```{yml}\n---\ntitle: \"My Document\"\nbibliography: references.bib\n---\n```\n\nCite in your text using the BibTeX key: (e.g.,[1]).\n\nExample:\n```{text}\n[Priority prediction of Asian Hornet sighting report using machine learning methods](https://huggingface.co/papers/2107.05465). @Liu_2021\n```\nPriority prediction of Asian Hornet sighting report using machine learning methods.[1]\n\nAdd a “References” section at the end of your document where the bibliography will be printed.\n\nExample:\n```{text}\n### References\n\n::: {#refs}\n:::\n```\n\nReferences\n\n\n1. Liu, Y., Guo, J., Dong, J., Jiang, L., & Ouyang, H. (2021). Priority prediction of asian hornet sighting report using machine learning methods. 2021 IEEE International Conference on Software Engineering and Artificial Intelligence (SEAI), 7–11. https://doi.org/10.1109/seai52285.2021.9477549"
  },
  {
    "objectID": "slides.html#introduction",
    "href": "slides.html#introduction",
    "title": "Present a great story for data science projects",
    "section": "Introduction",
    "text": "Introduction\n\nDevelop a storyline that captures attention and maintains interest.\nYour audience is your peers\nClearly state the problem or question you’re addressing.\nIntroduce why it is relevant needs.\nProvide an overview of your approach.\n\nIn kernel estimator, weight function is known as kernel function[1]. Cite this paper[2]. The GEE[3]. The PCA[4]*"
  },
  {
    "objectID": "slides.html#methods",
    "href": "slides.html#methods",
    "title": "Present a great story for data science projects",
    "section": "Methods",
    "text": "Methods\n\nDetail the models or algorithms used.\nJustify your choices based on the problem and data."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization",
    "href": "slides.html#data-exploration-and-visualization",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\n\nDescribe your data sources and collection process.\nPresent initial findings and insights through visualizations.\nHighlight unexpected patterns or anomalies."
  },
  {
    "objectID": "slides.html#data-exploration-and-visualization-1",
    "href": "slides.html#data-exploration-and-visualization-1",
    "title": "Present a great story for data science projects",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\nA study was conducted to determine how…"
  },
  {
    "objectID": "slides.html#modeling-and-results",
    "href": "slides.html#modeling-and-results",
    "title": "Present a great story for data science projects",
    "section": "Modeling and Results",
    "text": "Modeling and Results\n\nExplain your data preprocessing and cleaning steps.\nPresent your key findings in a clear and concise manner.\nUse visuals to support your claims.\nTell a story about what the data reveals."
  },
  {
    "objectID": "slides.html#conclusion",
    "href": "slides.html#conclusion",
    "title": "Present a great story for data science projects",
    "section": "Conclusion",
    "text": "Conclusion\n\nSummarize your key findings.\nDiscuss the implications of your results."
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Present a great story for data science projects",
    "section": "References",
    "text": "References\n\n\n1. Efromovich, S. (2008). Nonparametric curve estimation: Methods, theory, and applications. Springer New York. https://books.google.com/books?id=mdoLBwAAQBAJ\n\n\n2. Bro, R., & Smilde, A. K. (2014). Principal component analysis. Analytical Methods, 6(9), 2812–2831.\n\n\n3. Wang, M. (2014). Generalized estimating equations in longitudinal data analysis: A review and recent developments. Advances in Statistics, 2014.\n\n\n4. Daffertshofer, A., Lamoth, C. J., Meijer, O. G., & Beek, P. J. (2004). PCA in studying coordination and variability: A tutorial. Clinical Biomechanics, 19(4), 415–428."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data mining Project",
    "section": "",
    "text": "As part of the CARP-6771 Data mining Fall-2025 class at University of West Florida (UWF), our team completed a project on House Price Forecasting.\nThis project was completed under the guidance of Dr. Sikha Bagui (Sikha Bagui | Dr. Sikha Bagui)."
  },
  {
    "objectID": "posts/renan-blog-post-2/index.html",
    "href": "posts/renan-blog-post-2/index.html",
    "title": "House Dataset Exploration",
    "section": "",
    "text": "This report has a very basic exploration of the dataset for sake of simplicity and to keep it a small report, further exploration will be continuosly update on the project page: https://github.com/renanmb/CAP-6771—Data-Mining-Project"
  },
  {
    "objectID": "posts/renan-blog-post-2/index.html#introduction",
    "href": "posts/renan-blog-post-2/index.html#introduction",
    "title": "House Dataset Exploration",
    "section": "1. Introduction",
    "text": "1. Introduction\nOn this blog post we will be exploring several data sources for the House Price Forecasting project and evaluating what can be done and what the next steps we must take to properly aanswer the research questions.\nRun the following command to download the Zillow Economics Data[1]:\n```{bash}\n#!/bin/bash\ncurl -L -o ~/Downloads/zecon.zip\\\n  https://www.kaggle.com/api/v1/datasets/download/zillow/zecon\n```\n\n1.1 Load packages\n\n# install.packages(\"fpp3\")\nlibrary(fpp3)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(ggtime)\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/renan-blog-post-2/index.html#load-the-dataset",
    "href": "posts/renan-blog-post-2/index.html#load-the-dataset",
    "title": "House Dataset Exploration",
    "section": "2. Load the Dataset",
    "text": "2. Load the Dataset\nThe following code will locate the folder datasets and then it will give back the variable datasets_path which you can use to build the path to the desired data to be loaded.\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nzillow_economics_data_path &lt;- file.path(datasets_path, \"zillow-economics-data-01\")\n\nstate_time_series &lt;- file.path(zillow_economics_data_path, \"State_time_series.csv\")\nall_states_data &lt;- read.csv(state_time_series)"
  },
  {
    "objectID": "posts/renan-blog-post-2/index.html#exploring-data",
    "href": "posts/renan-blog-post-2/index.html#exploring-data",
    "title": "House Dataset Exploration",
    "section": "3. Exploring Data",
    "text": "3. Exploring Data\nThe ‘Date’ column is a character, let’s convert it to a Date object.\n\n# Inspect the data structure\nstr(all_states_data)\n\n'data.frame':   13212 obs. of  82 variables:\n $ Date                                                         : chr  \"1996-04-30\" \"1996-04-30\" \"1996-04-30\" \"1996-04-30\" ...\n $ RegionName                                                   : chr  \"Alabama\" \"Arizona\" \"Arkansas\" \"California\" ...\n $ DaysOnZillow_AllHomes                                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ InventorySeasonallyAdjusted_AllHomes                         : int  NA NA NA NA NA NA NA NA NA NA ...\n $ InventoryRaw_AllHomes                                        : int  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_1Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_2Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_3Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_4Bedroom                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_5BedroomOrMore                     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_AllHomes                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_CondoCoop                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_DuplexTriplex                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPricePerSqft_SingleFamilyResidence              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_1Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_2Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_3Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_4Bedroom                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_5BedroomOrMore                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_AllHomes                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_CondoCoop                                 : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_DuplexTriplex                             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianListingPrice_SingleFamilyResidence                     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPctOfPriceReduction_AllHomes                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPctOfPriceReduction_CondoCoop                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPctOfPriceReduction_SingleFamilyResidence              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPriceCutDollar_AllHomes                                : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPriceCutDollar_CondoCoop                               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianPriceCutDollar_SingleFamilyResidence                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_1Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_2Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_3Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_4Bedroom                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_5BedroomOrMore                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_AllHomes                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_CondoCoop                           : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_DuplexTriplex                       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_MultiFamilyResidence5PlusUnits      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_SingleFamilyResidence               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPricePerSqft_Studio                              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_1Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_2Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_3Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_4Bedroom                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_5BedroomOrMore                             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_AllHomes                                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_CondoCoop                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_DuplexTriplex                              : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_MultiFamilyResidence5PlusUnits             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_SingleFamilyResidence                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ MedianRentalPrice_Studio                                     : num  NA NA NA NA NA NA NA NA NA NA ...\n $ ZHVIPerSqft_AllHomes                                         : int  50 62 42 102 82 85 71 56 55 185 ...\n $ PctOfHomesDecreasingInValues_AllHomes                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfHomesIncreasingInValues_AllHomes                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfHomesSellingForGain_AllHomes                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfHomesSellingForLoss_AllHomes                            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductionsSeasAdj_AllHomes             : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductionsSeasAdj_CondoCoop            : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductionsSeasAdj_SingleFamilyResidence: num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductions_AllHomes                    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductions_CondoCoop                   : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PctOfListingsWithPriceReductions_SingleFamilyResidence       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ PriceToRentRatio_AllHomes                                    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Sale_Counts                                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Sale_Counts_Seas_Adj                                         : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Sale_Prices                                                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ ZHVI_1bedroom                                                : int  61500 59200 53000 93700 77800 64700 90100 45400 74900 152300 ...\n $ ZHVI_2bedroom                                                : int  48900 86400 54500 123400 97500 97000 88200 65400 64700 186600 ...\n $ ZHVI_3bedroom                                                : int  78200 96100 76800 150900 129000 130400 103500 89100 88000 231800 ...\n $ ZHVI_4bedroom                                                : int  146500 128400 135100 196100 176100 194800 157800 133600 149700 303400 ...\n $ ZHVI_5BedroomOrMore                                          : int  206300 190500 186000 265300 212900 299800 176100 199900 212800 345500 ...\n $ ZHVI_AllHomes                                                : int  79500 103600 64400 157900 128100 132000 106800 86300 92000 227400 ...\n $ ZHVI_BottomTier                                              : int  45600 67100 38400 95100 82700 83700 77200 52500 57200 144500 ...\n $ ZHVI_CondoCoop                                               : int  99500 78900 70300 136100 99400 85000 NA 70600 89300 177000 ...\n $ ZHVI_MiddleTier                                              : int  79500 103600 64400 157900 128100 132000 106800 86300 92000 227400 ...\n $ ZHVI_SingleFamilyResidence                                   : int  79000 107500 64500 162000 133600 141000 107400 92100 92400 262600 ...\n $ ZHVI_TopTier                                                 : int  140200 168700 115200 270600 209300 231600 161600 155300 163900 374700 ...\n $ ZRI_AllHomes                                                 : int  NA NA NA NA NA NA NA NA NA NA ...\n $ ZRI_AllHomesPlusMultifamily                                  : int  NA NA NA NA NA NA NA NA NA NA ...\n $ ZriPerSqft_AllHomes                                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ Zri_MultiFamilyResidenceRental                               : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Zri_SingleFamilyResidenceRental                              : int  NA NA NA NA NA NA NA NA NA NA ...\n\n\nNow we Convert ‘Date’ column to a Date object.\n\nall_states_data$Date &lt;- as.Date(all_states_data$Date)\n\nLets Check how many N/As and think about\n\n# Count total NAs per column\ncolSums(is.na(all_states_data))\n\nMaking it pretty\n\n# Get the total number of rows for calculating percentages\ntotal_rows &lt;- nrow(all_states_data)\n\n# Create a pretty summary table\nna_summary &lt;- all_states_data %&gt;%\n  # 1. Count NAs for every column\n  summarise(across(everything(), ~sum(is.na(.)))) %&gt;%\n  \n  # 2. Pivot the data from wide to long\n  pivot_longer(everything(),\n               names_to = \"Column\",\n               values_to = \"NA_Count\") %&gt;%\n  \n  # 3. (Optional) Filter to only show columns that HAVE NAs\n  filter(NA_Count &gt; 0) %&gt;%\n  \n  # 4. (Optional) Add a percentage column\n  mutate(NA_Percentage = (NA_Count / total_rows) * 100) %&gt;%\n\n  # 5. Sort by the highest NA count\n  arrange(desc(NA_Count))\n\n# Print the clean table\nprint(na_summary)\n\n# A tibble: 80 × 3\n   Column                                  NA_Count NA_Percentage\n   &lt;chr&gt;                                      &lt;int&gt;         &lt;dbl&gt;\n 1 PctOfHomesSellingForGain_AllHomes          12609          95.4\n 2 PctOfHomesSellingForLoss_AllHomes          12609          95.4\n 3 MedianRentalPrice_5BedroomOrMore           11994          90.8\n 4 MedianRentalPricePerSqft_5BedroomOrMore    11752          88.9\n 5 MedianRentalPricePerSqft_Studio            10875          82.3\n 6 MedianRentalPrice_CondoCoop                10437          79.0\n 7 MedianRentalPricePerSqft_DuplexTriplex     10293          77.9\n 8 MedianRentalPrice_Studio                   10211          77.3\n 9 MedianListingPrice_1Bedroom                10205          77.2\n10 MedianRentalPrice_DuplexTriplex            10068          76.2\n# ℹ 70 more rows\n\n\n\n#  Some random tinkering\n# head(all_states_data)\n# summary(all_states_data)"
  },
  {
    "objectID": "posts/renan-blog-post-2/index.html#time-series-initial-analysis",
    "href": "posts/renan-blog-post-2/index.html#time-series-initial-analysis",
    "title": "House Dataset Exploration",
    "section": "4. Time Series Initial Analysis",
    "text": "4. Time Series Initial Analysis\nNow we will further explore the dataset. As we could observe that the dataset is large and has many columns for metrics we must continue to filter for a specific state and metric to start making this more manageable. Then later we will come back to explore the full dataset and the other datasets.\n\n4.1 Zillow Economics Data\nWe create the timeseries object using ‘tsibble’ which is a time-aware data frame. This is the standard object for the ‘fpp3’ workflow. Then we index the time component.\nFor sake of simplicity we are going to focus for now on a single State (California) and the Zillow Home Value Index (ZHVI_AllHomes) over time.\n\n# Let's analyze the Zillow Home Value Index (ZHVI_AllHomes) for \"California\".\ncali_zhvi &lt;- all_states_data %&gt;%\n  filter(RegionName == \"California\") %&gt;%\n  select(Date, ZHVI_AllHomes) %&gt;%\n  # Remove any missing values for this metric\n  na.omit()\n\nJust a quick check:\n\n# Inspect the new, focused data frame\nhead(cali_zhvi)\n\n        Date ZHVI_AllHomes\n1 1996-04-30        157900\n2 1996-05-31        157800\n3 1996-06-30        157500\n4 1996-07-31        157300\n5 1996-08-31        157000\n6 1996-09-30        156800\n\n\nMaking the Time Series object cali_ts:\n\ncali_ts &lt;- cali_zhvi %&gt;%\n  as_tsibble(index = Date)\n\nFurther experiments need to handle the N/As when filling the gaps:\n\n# We set 'Date' as the 'index' (the time component).\ncali_ts_fill_gaps &lt;- cali_zhvi %&gt;%\n  as_tsibble(index = Date) %&gt;%\n  fill_gaps()\n# If you were analyzing multiple states, you would add a 'key'.\n# Example for multiple states (not run here):\n# multi_state_ts &lt;- all_states_data %&gt;%\n#   select(Date, RegionName, ZHVI_AllHomes) %&gt;%\n#   as_tsibble(index = Date, key = RegionName)\n\nLets see how many N/As after filling the gaps:\n\n# Count total NAs per column\ncolSums(is.na(cali_ts))\ncolSums(is.na(cali_ts_fill_gaps))\n\nForward Fill (LOCF): Uses the last known value to fill gaps. Ideal for stable or categorical data but not for volatile metrics.\n\n# 1. APPLY LOCF TO THE CALIFORNIA DATA\n# ------------------------------------\n# We use fill() on the ZHVI_AllHomes column.\n# The default direction is \"down\", which is exactly what LOCF is.\ncali_locf &lt;- cali_ts_fill_gaps %&gt;%\n  fill(ZHVI_AllHomes)\n\n\n# cali_ts\n\n\n4.1.1 Plot the Time Series\nWe can make an initial plot to see that there is a trend and the data is non-stationary.\n\ncali_ts %&gt;%\n  autoplot(ZHVI_AllHomes) +\n  labs(title = \"Zillow Home Value Index (ZHVI) for California\",\n       y = \"Home Value Index\",\n       x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can see that filling the gaps creates some issues, this was fixed with the LOCF:\n\ncali_ts_fill_gaps %&gt;%\n  autoplot(ZHVI_AllHomes) +\n  labs(title = \"Zillow Home Value Index (ZHVI) for California\",\n       y = \"Home Value Index\",\n       x = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nExploring for Seasonality which is not that evident, need further study:\n\n# --- B. Look for Seasonality ---\n# A season plot layers the data by year, helping to spot seasonal patterns.\ncali_locf %&gt;%\n  gg_season(ZHVI_AllHomes, labels = \"right\") +\n  labs(title = \"Seasonal Plot: ZHVI for California\",\n       y = \"Home Value Index\")\n\n\n\n\n\n\n\n\nLet’s plot the “before” and “after” side-by-side to see if there are any evident changes.\n\n# Plot 1: Using the original, gappy data\np1 &lt;- ggplot(cali_ts, aes(x = Date, y = ZHVI_AllHomes)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Original (cali_ts)\",\n       subtitle = \"ggplot connects the dots over implicit gaps\") +\n  theme_minimal()\n\n# Plot 2: Using the data fixed with fill_gaps()\np2 &lt;- ggplot(cali_locf, aes(x = Date, y = ZHVI_AllHomes)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"After: Forward Fill (LOCF)\",\n       subtitle = \"Gaps are filled with the last known value\") +\n  theme_minimal()\n\n# Combine them side-by-side\np1 + p2"
  },
  {
    "objectID": "posts/renan-blog-post-2/index.html#misc",
    "href": "posts/renan-blog-post-2/index.html#misc",
    "title": "House Dataset Exploration",
    "section": "MISC",
    "text": "MISC\nThe naniar package is built specifically for exploring missing data. It has a miss_var_summary() function that does help visualizing missing data. Might consider implementing.\n```{r}\n# install.packages(\"naniar\")\nlibrary(naniar)\n\n# This one function does it all, already sorted!\nmiss_var_summary(all_states_data)\n\n# Example output:\n# A tibble: 271 × 3\n#   variable        n_miss  pct_miss\n#   &lt;chr&gt;            &lt;int&gt;     &lt;dbl&gt;\n# 1 ZRI_AllHomes_CondoCoop 149463  79.2\n# 2 ZRI_AllHomes_DuplexTriplex  142137  75.3\n# ...\n```\n\nReferences\n\n\n1. zillow. (n.d.). Zillow Economics Data. https://www.kaggle.com/datasets/zillow/zecon"
  },
  {
    "objectID": "posts/renan-blog-post-6/index.html",
    "href": "posts/renan-blog-post-6/index.html",
    "title": "The Jackson Heart Study (JHS) Dataset",
    "section": "",
    "text": "The Jackson Heart Study (JHS) dataset can be obtained by sending an access. I would like to thank Dr. Samantha Seals for being kind enough to provide me with the datasets for the JHS."
  },
  {
    "objectID": "posts/renan-blog-post-6/index.html#introduction",
    "href": "posts/renan-blog-post-6/index.html#introduction",
    "title": "The Jackson Heart Study (JHS) Dataset",
    "section": "1. Introduction",
    "text": "1. Introduction\nIn this project we will be evaluating variables that are statistically likely to increase the risk of causing some health diseases related to Heart.\n\n1.1 Load packages\n\npackages = c(\"brant\", \"car\", \"fastDummies\", \"fitdistrplus\", \"jtools\", \"plyr\", \"sas7bdat\", \"sjPlot\", \"skimr\", \"tidyverse\", \"rstatix\", \"ggplot2\", \"MASS\")\n\n# install.packages(packages)\nlapply(packages, library, character.only = TRUE)\n\n\n\n1.1. Load the Dataset\nThe code below automates loading the datasets from the folder datasets located at the root of the github repository. We will Import the data for the JHS study for visit 1 and for the JHS study for visit 2 from analysis1.sas7bdat and analysis2.sas7bdat.\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nhealth_data_path &lt;- file.path(datasets_path, \"jhs_data\")\n\njhs_analysis1_path &lt;- file.path(health_data_path, \"analysis1.sas7bdat\")\njhs_analysis2_path &lt;- file.path(health_data_path, \"analysis2.sas7bdat\")\njhs_analysis1 &lt;- read.sas7bdat(jhs_analysis1_path)\njhs_analysis2 &lt;- read.sas7bdat(jhs_analysis2_path)"
  },
  {
    "objectID": "posts/renan-blog-post-6/index.html#data-exploration-and-processing",
    "href": "posts/renan-blog-post-6/index.html#data-exploration-and-processing",
    "title": "The Jackson Heart Study (JHS) Dataset",
    "section": "2. Data Exploration and Processing",
    "text": "2. Data Exploration and Processing\nIn this section which is intended for the Project 1 assignment we will be exploring the dataset and processing it to extract the most relevant Attributes.\n\n2.1. Dataset Features\nThe Jackson Heart Study (JHS) has a JHS Variable Explorer which helps to explore all the available variables, its characteristics, data types and the structure as some variables have extensive contextual information about they were obtained and their associated study.\nThe objects jhs_analysis1_variables and jhs_analysis2_variables have the individual features and I found useful when I need to quickly check if my variable is available.\n\n# Listing all the attributes in the Dataset\n# making a datatable is too slow and inneficient\n# DT::datatable(jhs_analysis1, options = list(pageLength = 5))\n# DT::datatable(jhs_analysis2, options = list(pageLength = 5))\njhs_analysis1_variables = knitr::kable(data.frame(Feature = names(jhs_analysis1)))\njhs_analysis2_variables = knitr::kable(data.frame(Feature = names(jhs_analysis2)))\n\nComparisson side by side of all the available variables in both Datasets.\n\n# Extract variable names as vectors\nvars1 &lt;- names(jhs_analysis1)\nvars2 &lt;- names(jhs_analysis2)\n\n# Compare variable names\ncommon_vars &lt;- intersect(vars1, vars2)\nonly_in_analysis1 &lt;- setdiff(vars1, vars2)\nonly_in_analysis2 &lt;- setdiff(vars2, vars1)\n\n# Create comparison table\ncomparison &lt;- data.frame(\n  Variable = union(vars1, vars2),\n  In_Analysis1 = union(vars1, vars2) %in% vars1,\n  In_Analysis2 = union(vars1, vars2) %in% vars2\n)\n\nknitr::kable(comparison, caption = \"Variable Comparison Between Datasets\")\n\n\nVariable Comparison Between Datasets\n\n\nVariable\nIn_Analysis1\nIn_Analysis2\n\n\n\n\nsubjid\nTRUE\nTRUE\n\n\nvisit\nTRUE\nTRUE\n\n\nVisitDate\nTRUE\nTRUE\n\n\nDaysFromV1\nTRUE\nTRUE\n\n\nYearsFromV1\nTRUE\nTRUE\n\n\nARIC\nTRUE\nTRUE\n\n\nrecruit\nTRUE\nTRUE\n\n\nageIneligible\nTRUE\nTRUE\n\n\nFastHours\nTRUE\nTRUE\n\n\nage\nTRUE\nTRUE\n\n\nbrthyr\nTRUE\nTRUE\n\n\nbrthmo\nTRUE\nTRUE\n\n\nsex\nTRUE\nTRUE\n\n\nmale\nTRUE\nTRUE\n\n\nmenopause\nTRUE\nTRUE\n\n\nalc\nTRUE\nTRUE\n\n\nalcw\nTRUE\nTRUE\n\n\ncurrentSmoker\nTRUE\nTRUE\n\n\neverSmoker\nTRUE\nTRUE\n\n\nweight\nTRUE\nTRUE\n\n\nheight\nTRUE\nTRUE\n\n\nwaist\nTRUE\nTRUE\n\n\nneck\nTRUE\nTRUE\n\n\nBMI\nTRUE\nTRUE\n\n\nhip\nTRUE\nTRUE\n\n\nbsa\nTRUE\nTRUE\n\n\nBPmeds\nTRUE\nTRUE\n\n\nDMmedsOral\nTRUE\nTRUE\n\n\nDMmedsIns\nTRUE\nTRUE\n\n\nDMmeds\nTRUE\nTRUE\n\n\nstatinMeds\nTRUE\nTRUE\n\n\nhrtMeds\nTRUE\nTRUE\n\n\nbetaBlkMeds\nTRUE\nTRUE\n\n\ncalBlkMeds\nTRUE\nTRUE\n\n\ndiureticMeds\nTRUE\nTRUE\n\n\nantiArythMeds\nTRUE\nTRUE\n\n\nmedAcct\nTRUE\nTRUE\n\n\nBPmedsSelf\nTRUE\nTRUE\n\n\nDMMedType\nTRUE\nTRUE\n\n\ndmMedsSelf\nTRUE\nTRUE\n\n\nstatinMedsSelf\nTRUE\nTRUE\n\n\nhrtMedsSelfEver\nTRUE\nTRUE\n\n\nhrtMedsSelf\nTRUE\nTRUE\n\n\nantiArythMedsSelf\nTRUE\nTRUE\n\n\nsbp\nTRUE\nTRUE\n\n\ndbp\nTRUE\nTRUE\n\n\nBPjnc7\nTRUE\nTRUE\n\n\nHTN\nTRUE\nTRUE\n\n\nabi\nTRUE\nTRUE\n\n\nHbA1c\nTRUE\nTRUE\n\n\nFPG\nTRUE\nTRUE\n\n\nFPG3cat\nTRUE\nTRUE\n\n\nHbA1c3cat\nTRUE\nTRUE\n\n\nHbA1cIFCC\nTRUE\nTRUE\n\n\nHbA1cIFCC3cat\nTRUE\nTRUE\n\n\nfastingInsulin\nTRUE\nTRUE\n\n\nDiabetes\nTRUE\nTRUE\n\n\ndiab3cat\nTRUE\nTRUE\n\n\nHOMA_B\nTRUE\nTRUE\n\n\nHOMA_IR\nTRUE\nTRUE\n\n\nldl\nTRUE\nTRUE\n\n\nhdl\nTRUE\nTRUE\n\n\ntrigs\nTRUE\nTRUE\n\n\ntotchol\nTRUE\nTRUE\n\n\nldl5cat\nTRUE\nTRUE\n\n\nhdl3cat\nTRUE\nTRUE\n\n\ntrigs4cat\nTRUE\nTRUE\n\n\nLEPTIN\nTRUE\nFALSE\n\n\nHSCRP\nTRUE\nFALSE\n\n\nENDOTHELIN\nTRUE\nFALSE\n\n\nALDOSTERONE\nTRUE\nFALSE\n\n\neSelectin\nTRUE\nTRUE\n\n\npSelectin\nTRUE\nTRUE\n\n\nsCort\nTRUE\nTRUE\n\n\nreninRIA\nTRUE\nTRUE\n\n\nreninIRMA\nTRUE\nTRUE\n\n\nadiponectin\nTRUE\nTRUE\n\n\nCreatinineU24hr\nTRUE\nTRUE\n\n\nAlbuminU24hr\nTRUE\nTRUE\n\n\nCreatinineUSpot\nTRUE\nTRUE\n\n\nAlbuminUSpot\nTRUE\nTRUE\n\n\nSCrCC\nTRUE\nTRUE\n\n\nSCrIDMS\nTRUE\nTRUE\n\n\neGFRmdrd\nTRUE\nTRUE\n\n\neGFRckdepi\nTRUE\nTRUE\n\n\nDialysisEver\nTRUE\nFALSE\n\n\nDialysisDuration\nTRUE\nTRUE\n\n\nCKDHx\nTRUE\nTRUE\n\n\nmaneuvers\nTRUE\nFALSE\n\n\nasthma\nTRUE\nFALSE\n\n\nFVC\nTRUE\nFALSE\n\n\nFEV1\nTRUE\nFALSE\n\n\nFEV6\nTRUE\nFALSE\n\n\nFEV1PP\nTRUE\nFALSE\n\n\nFVCPP\nTRUE\nFALSE\n\n\nEF\nTRUE\nFALSE\n\n\nDiastLVdia\nTRUE\nFALSE\n\n\nSystLVdia\nTRUE\nFALSE\n\n\nLVMecho\nTRUE\nFALSE\n\n\nLVMindex\nTRUE\nFALSE\n\n\nLVH\nTRUE\nFALSE\n\n\nEF3cat\nTRUE\nFALSE\n\n\nFS\nTRUE\nFALSE\n\n\nRWT\nTRUE\nFALSE\n\n\necgHR\nTRUE\nFALSE\n\n\nQRS\nTRUE\nFALSE\n\n\nQT\nTRUE\nFALSE\n\n\nConductionDefect\nTRUE\nFALSE\n\n\nMajorScarAnt\nTRUE\nFALSE\n\n\nMinorScarAnt\nTRUE\nFALSE\n\n\nRepolarAnt\nTRUE\nFALSE\n\n\nMIant\nTRUE\nFALSE\n\n\nMajorScarPost\nTRUE\nFALSE\n\n\nMinorScarPost\nTRUE\nFALSE\n\n\nRepolarPost\nTRUE\nFALSE\n\n\nMIpost\nTRUE\nFALSE\n\n\nMajorScarAntLat\nTRUE\nFALSE\n\n\nMinorScarAntLat\nTRUE\nFALSE\n\n\nRepolarAntLat\nTRUE\nFALSE\n\n\nMIAntLat\nTRUE\nFALSE\n\n\nMIecg\nTRUE\nFALSE\n\n\nAfib\nTRUE\nFALSE\n\n\nAflutter\nTRUE\nFALSE\n\n\nQTcFram\nTRUE\nFALSE\n\n\nQTcBaz\nTRUE\nFALSE\n\n\nQTcHod\nTRUE\nFALSE\n\n\nQTcFrid\nTRUE\nFALSE\n\n\nCV\nTRUE\nFALSE\n\n\nLVHcv\nTRUE\nFALSE\n\n\nspeechLossEver\nTRUE\nTRUE\n\n\nvisionLossEver\nTRUE\nTRUE\n\n\ndoubleVisionEver\nTRUE\nTRUE\n\n\nnumbnessEver\nTRUE\nTRUE\n\n\nparalysisEver\nTRUE\nTRUE\n\n\ndizzynessEver\nTRUE\nTRUE\n\n\nstrokeHx\nTRUE\nTRUE\n\n\nMIHx\nTRUE\nTRUE\n\n\nCardiacProcHx\nTRUE\nTRUE\n\n\nCHDHx\nTRUE\nTRUE\n\n\nCarotidAngioHx\nTRUE\nTRUE\n\n\nCVDHx\nTRUE\nTRUE\n\n\nPrivateIns\nTRUE\nFALSE\n\n\nMedicaidIns\nTRUE\nFALSE\n\n\nMedicareIns\nTRUE\nFALSE\n\n\nVAIns\nTRUE\nFALSE\n\n\nInsuranceType\nTRUE\nFALSE\n\n\nInsured\nTRUE\nFALSE\n\n\nPublicIns\nTRUE\nFALSE\n\n\nPublicInsType\nTRUE\nFALSE\n\n\nPrivatePublicIns\nTRUE\nFALSE\n\n\ndailyDiscr\nTRUE\nFALSE\n\n\nlifetimeDiscrm\nTRUE\nFALSE\n\n\ndiscrmBurden\nTRUE\nFALSE\n\n\ndepression\nTRUE\nFALSE\n\n\nperceivedStress\nTRUE\nFALSE\n\n\nweeklyStress\nTRUE\nFALSE\n\n\nfmlyinc\nTRUE\nFALSE\n\n\nIncome\nTRUE\nFALSE\n\n\noccupation\nTRUE\nFALSE\n\n\nedu3cat\nTRUE\nFALSE\n\n\nHSgrad\nTRUE\nFALSE\n\n\nPA3cat\nTRUE\nTRUE\n\n\nnutrition3cat\nTRUE\nTRUE\n\n\nSMK3cat\nTRUE\nTRUE\n\n\nidealHealthSMK\nTRUE\nTRUE\n\n\nBMI3cat\nTRUE\nTRUE\n\n\nidealHealthBMI\nTRUE\nTRUE\n\n\nidealHealthPA\nTRUE\nTRUE\n\n\nidealHealthNutrition\nTRUE\nTRUE\n\n\ntotChol3cat\nTRUE\nTRUE\n\n\nidealHealthChol\nTRUE\nTRUE\n\n\nBP3cat\nTRUE\nTRUE\n\n\nidealHealthBP\nTRUE\nTRUE\n\n\nglucose3cat\nTRUE\nTRUE\n\n\nidealHealthDM\nTRUE\nTRUE\n\n\nvitaminD2\nTRUE\nFALSE\n\n\nvitaminD3\nTRUE\nFALSE\n\n\nvitaminD3epimer\nTRUE\nFALSE\n\n\ndarkgrnVeg\nTRUE\nFALSE\n\n\neggs\nTRUE\nFALSE\n\n\nfish\nTRUE\nFALSE\n\n\nFakeCensusTractID\nTRUE\nTRUE\n\n\nnbmedHHincome\nTRUE\nTRUE\n\n\nnbpctpoverty\nTRUE\nTRUE\n\n\nnbpctBlackNH\nTRUE\nTRUE\n\n\nnbpctWhiteNH\nTRUE\nTRUE\n\n\nnbSESpc2score\nTRUE\nTRUE\n\n\nnbSESanascore\nTRUE\nTRUE\n\n\nnbProblems\nTRUE\nTRUE\n\n\nnbCohesion\nTRUE\nTRUE\n\n\nnbViolence\nTRUE\nTRUE\n\n\nnbK3FavorFoodstore\nTRUE\nTRUE\n\n\nnbK3paFacilities\nTRUE\nTRUE\n\n\nnbpctResiden1mi\nTRUE\nTRUE\n\n\nnbPopDensity1mi\nTRUE\nTRUE\n\n\nsportIndex\nTRUE\nFALSE\n\n\nhyIndex\nTRUE\nFALSE\n\n\nactiveIndex\nTRUE\nFALSE\n\n\nhsCRP\nFALSE\nTRUE\n\n\nendothelin\nFALSE\nTRUE\n\n\naldosterone\nFALSE\nTRUE\n\n\nleptin\nFALSE\nTRUE\n\n\ndialysisEver\nFALSE\nTRUE\n\n\nCAC\nFALSE\nTRUE\n\n\nAAC\nFALSE\nTRUE\n\n\nVAT50mm\nFALSE\nTRUE\n\n\nSAT50mm\nFALSE\nTRUE\n\n\nanyCAC\nFALSE\nTRUE\n\n\nanyAAC\nFALSE\nTRUE\n\n\n\n\n\n\n2.1.1. Dataset 1 jhs_analysis1 Data Summary\nBelow we use the Skimr package to create a more extension Data Summary of jhs_analysis1\n\nskim(jhs_analysis1)\n\n\nData summary\n\n\nName\njhs_analysis1\n\n\nNumber of rows\n2653\n\n\nNumber of columns\n198\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n195\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsubjid\n0\n1\n1\n4\n0\n2653\n0\n\n\nsex\n0\n1\n4\n6\n0\n2\n0\n\n\nfmlyinc\n0\n1\n0\n1\n374\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nvisit\n0\n1.00\n1.00\n0.00\n1.00\n1.00\n1.00\n1.00\n1.00\n▁▁▇▁▁\n\n\nVisitDate\n0\n1.00\n15602.94\n345.53\n14877.00\n15335.00\n15609.00\n15873.00\n16168.00\n▅▅▇▇▇\n\n\nDaysFromV1\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nYearsFromV1\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nARIC\n0\n1.00\n0.29\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nrecruit\n0\n1.00\n3.12\n1.58\n1.00\n1.00\n3.00\n5.00\n5.00\n▇▂▅▆▇\n\n\nageIneligible\n3\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nFastHours\n6\n1.00\n12.82\n3.29\n0.75\n12.00\n13.00\n14.22\n25.25\n▁▁▇▁▁\n\n\nage\n0\n1.00\n54.28\n11.74\n22.91\n45.33\n54.20\n63.16\n83.21\n▁▆▇▇▂\n\n\nbrthyr\n0\n1.00\n1947.95\n12.01\n1919.00\n1939.00\n1948.00\n1957.00\n1980.00\n▂▇▇▆▁\n\n\nbrthmo\n0\n1.00\n6.40\n3.49\n1.00\n3.00\n7.00\n9.00\n12.00\n▇▅▅▅▇\n\n\nmale\n0\n1.00\n0.37\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nmenopause\n1354\n0.49\n0.85\n0.36\n0.00\n1.00\n1.00\n1.00\n1.00\n▂▁▁▁▇\n\n\nalc\n10\n1.00\n0.48\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nalcw\n71\n0.97\n1.61\n5.07\n0.00\n0.00\n0.00\n0.50\n42.00\n▇▁▁▁▁\n\n\ncurrentSmoker\n18\n0.99\n0.12\n0.32\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\neverSmoker\n4\n1.00\n0.31\n0.46\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nweight\n4\n1.00\n91.29\n20.86\n49.10\n77.00\n88.00\n102.80\n174.60\n▃▇▃▁▁\n\n\nheight\n3\n1.00\n169.33\n9.37\n149.00\n162.00\n169.00\n176.00\n192.00\n▃▇▇▆▂\n\n\nwaist\n5\n1.00\n100.65\n15.88\n67.00\n90.00\n99.00\n110.00\n152.00\n▂▇▆▂▁\n\n\nneck\n4\n1.00\n38.59\n3.72\n31.00\n36.00\n38.00\n41.00\n49.00\n▃▇▆▃▁\n\n\nBMI\n4\n1.00\n31.86\n6.97\n18.53\n27.02\n30.77\n35.56\n59.57\n▃▇▃▁▁\n\n\nhip\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nbsa\n4\n1.00\n2.01\n0.23\n1.49\n1.84\n2.00\n2.15\n2.74\n▂▇▇▃▁\n\n\nBPmeds\n20\n0.99\n0.51\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nDMmedsOral\n183\n0.93\n0.10\n0.30\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDMmedsIns\n190\n0.93\n0.05\n0.21\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDMmeds\n23\n0.99\n0.13\n0.34\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nstatinMeds\n25\n0.99\n0.13\n0.33\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nhrtMeds\n144\n0.95\n0.19\n0.40\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nbetaBlkMeds\n179\n0.93\n0.11\n0.31\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ncalBlkMeds\n176\n0.93\n0.20\n0.40\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\ndiureticMeds\n156\n0.94\n0.33\n0.47\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nantiArythMeds\n24\n0.99\n0.04\n0.19\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nmedAcct\n20\n0.99\n1.71\n0.58\n0.00\n2.00\n2.00\n2.00\n2.00\n▁▁▂▁▇\n\n\nBPmedsSelf\n45\n0.98\n0.48\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nDMMedType\n193\n0.93\n0.19\n0.57\n0.00\n0.00\n0.00\n0.00\n3.00\n▇▁▁▁▁\n\n\ndmMedsSelf\n47\n0.98\n0.13\n0.34\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nstatinMedsSelf\n51\n0.98\n0.12\n0.33\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nhrtMedsSelfEver\n995\n0.62\n0.43\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nhrtMedsSelf\n1944\n0.27\n0.58\n0.49\n0.00\n0.00\n1.00\n1.00\n1.00\n▆▁▁▁▇\n\n\nantiArythMedsSelf\n47\n0.98\n0.04\n0.19\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nsbp\n4\n1.00\n126.11\n15.58\n93.57\n114.66\n124.75\n134.83\n188.93\n▃▇▅▁▁\n\n\ndbp\n4\n1.00\n75.82\n8.56\n52.63\n70.07\n75.88\n81.69\n99.95\n▁▅▇▃▁\n\n\nBPjnc7\n4\n1.00\n0.87\n0.77\n0.00\n0.00\n1.00\n1.00\n3.00\n▆▇▁▂▁\n\n\nHTN\n0\n1.00\n0.53\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nabi\n244\n0.91\n1.22\n0.15\n0.63\n1.13\n1.21\n1.31\n1.83\n▁▃▇▂▁\n\n\nHbA1c\n88\n0.97\n5.88\n1.11\n4.20\n5.30\n5.60\n6.10\n12.00\n▇▅▁▁▁\n\n\nFPG\n185\n0.93\n97.58\n26.48\n70.00\n85.00\n91.00\n100.00\n302.00\n▇▁▁▁▁\n\n\nFPG3cat\n185\n0.93\n0.33\n0.61\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▂▁▁\n\n\nHbA1c3cat\n88\n0.97\n0.65\n0.74\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▂\n\n\nHbA1cIFCC\n88\n0.97\n40.72\n12.11\n22.41\n34.43\n37.71\n43.17\n107.66\n▇▅▁▁▁\n\n\nHbA1cIFCC3cat\n88\n0.97\n0.65\n0.74\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▂\n\n\nfastingInsulin\n195\n0.93\n17.62\n11.63\n3.00\n11.00\n15.00\n21.00\n92.00\n▇▂▁▁▁\n\n\nDiabetes\n21\n0.99\n0.19\n0.40\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\ndiab3cat\n21\n0.99\n0.73\n0.76\n0.00\n0.00\n1.00\n1.00\n2.00\n▇▁▆▁▃\n\n\nHOMA_B\n613\n0.77\n225.03\n123.63\n35.06\n143.77\n193.64\n271.92\n855.76\n▇▆▁▁▁\n\n\nHOMA_IR\n613\n0.77\n3.54\n2.04\n0.53\n2.16\n3.01\n4.42\n12.99\n▇▆▂▁▁\n\n\nldl\n210\n0.92\n126.49\n35.72\n42.00\n101.50\n124.00\n148.00\n240.00\n▂▇▇▃▁\n\n\nhdl\n197\n0.93\n51.68\n13.99\n25.00\n41.00\n50.00\n60.00\n102.00\n▅▇▅▁▁\n\n\ntrigs\n197\n0.93\n104.20\n61.08\n27.00\n64.00\n90.00\n126.00\n485.00\n▇▃▁▁▁\n\n\ntotchol\n197\n0.93\n199.04\n38.55\n111.00\n173.00\n196.00\n222.00\n327.00\n▂▇▇▂▁\n\n\nldl5cat\n210\n0.92\n1.43\n1.12\n0.00\n1.00\n1.00\n2.00\n4.00\n▆▇▆▃▁\n\n\nhdl3cat\n197\n0.93\n0.88\n0.78\n0.00\n0.00\n1.00\n2.00\n2.00\n▇▁▇▁▆\n\n\ntrigs4cat\n197\n0.93\n0.22\n0.55\n0.00\n0.00\n0.00\n0.00\n3.00\n▇▁▁▁▁\n\n\nLEPTIN\n58\n0.98\n27.92\n22.09\n1.30\n10.20\n23.20\n39.45\n125.60\n▇▅▂▁▁\n\n\nHSCRP\n43\n0.98\n0.49\n0.67\n0.01\n0.11\n0.26\n0.55\n4.78\n▇▁▁▁▁\n\n\nENDOTHELIN\n44\n0.98\n1.31\n0.54\n0.40\n0.90\n1.20\n1.60\n4.10\n▇▇▂▁▁\n\n\nALDOSTERONE\n44\n0.98\n5.61\n4.35\n1.90\n2.60\n4.40\n7.20\n32.00\n▇▂▁▁▁\n\n\neSelectin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\npSelectin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nsCort\n44\n0.98\n9.57\n3.84\n2.20\n6.80\n9.00\n11.70\n24.80\n▅▇▃▁▁\n\n\nreninRIA\n1468\n0.45\n1.47\n4.04\n0.19\n0.20\n0.40\n1.00\n34.40\n▇▁▁▁▁\n\n\nreninIRMA\n1236\n0.53\n11.39\n23.17\n4.90\n5.10\n6.70\n9.60\n321.30\n▇▁▁▁▁\n\n\nadiponectin\n80\n0.97\n5079.41\n3626.80\n694.65\n2652.82\n4045.90\n6356.52\n24487.00\n▇▃▁▁▁\n\n\nCreatinineU24hr\n2032\n0.23\n1.47\n0.51\n0.40\n1.10\n1.40\n1.70\n3.20\n▂▇▃▂▁\n\n\nAlbuminU24hr\n2034\n0.23\n31.32\n148.48\n1.60\n5.10\n7.20\n14.10\n2318.00\n▇▁▁▁▁\n\n\nCreatinineUSpot\n1361\n0.49\n151.57\n74.64\n21.00\n95.00\n145.50\n198.00\n416.00\n▆▇▅▁▁\n\n\nAlbuminUSpot\n1361\n0.49\n3.45\n12.51\n0.20\n0.51\n0.87\n1.66\n156.00\n▇▁▁▁▁\n\n\nSCrCC\n33\n0.99\n1.00\n0.23\n0.66\n0.85\n0.94\n1.13\n3.28\n▇▂▁▁▁\n\n\nSCrIDMS\n33\n0.99\n0.91\n0.24\n0.56\n0.75\n0.85\n1.04\n3.27\n▇▂▁▁▁\n\n\neGFRmdrd\n33\n0.99\n87.27\n16.85\n22.25\n76.79\n86.98\n97.72\n136.57\n▁▂▇▆▁\n\n\neGFRckdepi\n33\n0.99\n96.14\n20.05\n18.67\n83.10\n97.37\n110.41\n142.14\n▁▁▆▇▂\n\n\nDialysisEver\n30\n0.99\n0.00\n0.06\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDialysisDuration\n2648\n0.00\n10.60\n11.50\n1.00\n1.00\n6.00\n18.00\n27.00\n▇▁▁▂▂\n\n\nCKDHx\n9\n1.00\n0.04\n0.19\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nmaneuvers\n103\n0.96\n4.52\n1.43\n2.00\n3.00\n4.00\n5.00\n9.00\n▇▇▇▁▂\n\n\nasthma\n15\n0.99\n0.14\n0.48\n0.00\n0.00\n0.00\n0.00\n2.00\n▇▁▁▁▁\n\n\nFVC\n117\n0.96\n3.03\n0.82\n1.22\n2.43\n2.91\n3.54\n5.63\n▂▇▆▂▁\n\n\nFEV1\n117\n0.96\n2.44\n0.68\n0.86\n1.98\n2.37\n2.86\n4.53\n▂▇▇▃▁\n\n\nFEV6\n117\n0.96\n2.97\n0.80\n1.18\n2.39\n2.86\n3.47\n5.43\n▂▇▆▃▁\n\n\nFEV1PP\n117\n0.96\n92.74\n16.58\n39.90\n82.83\n93.32\n103.39\n140.31\n▁▃▇▅▁\n\n\nFVCPP\n117\n0.96\n91.33\n15.93\n44.77\n81.84\n91.08\n100.67\n158.76\n▁▇▇▁▁\n\n\nEF\n85\n0.97\n62.03\n6.93\n30.00\n55.00\n65.00\n65.00\n75.00\n▁▁▃▇▂\n\n\nDiastLVdia\n879\n0.67\n49.51\n4.29\n39.60\n46.60\n49.50\n52.10\n66.90\n▂▇▆▁▁\n\n\nSystLVdia\n880\n0.67\n30.10\n4.68\n21.70\n26.70\n30.00\n32.80\n50.70\n▆▇▃▁▁\n\n\nLVMecho\n881\n0.67\n147.05\n38.35\n79.17\n119.44\n142.05\n167.37\n308.46\n▆▇▃▁▁\n\n\nLVMindex\n883\n0.67\n35.51\n8.94\n19.20\n29.32\n33.97\n40.17\n74.53\n▅▇▂▁▁\n\n\nLVH\n883\n0.67\n0.06\n0.24\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nEF3cat\n85\n0.97\n0.29\n0.46\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▃▁▁\n\n\nFS\n880\n0.67\n0.04\n0.21\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nRWT\n881\n0.67\n0.35\n0.05\n0.23\n0.31\n0.34\n0.38\n0.56\n▃▇▃▁▁\n\n\necgHR\n6\n1.00\n63.57\n10.10\n43.00\n57.00\n63.00\n70.00\n98.00\n▃▇▅▂▁\n\n\nQRS\n49\n0.98\n93.64\n13.46\n72.00\n84.00\n92.00\n100.00\n162.00\n▇▇▁▁▁\n\n\nQT\n6\n1.00\n414.94\n30.60\n340.00\n394.00\n412.00\n434.00\n516.00\n▂▇▇▂▁\n\n\nConductionDefect\n0\n1.00\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMajorScarAnt\n0\n1.00\n0.01\n0.08\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMinorScarAnt\n0\n1.00\n0.01\n0.11\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nRepolarAnt\n0\n1.00\n0.04\n0.20\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMIant\n0\n1.00\n0.01\n0.08\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMajorScarPost\n0\n1.00\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMinorScarPost\n0\n1.00\n0.02\n0.15\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nRepolarPost\n0\n1.00\n0.02\n0.12\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMIpost\n0\n1.00\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMajorScarAntLat\n0\n1.00\n0.00\n0.04\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMinorScarAntLat\n0\n1.00\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nRepolarAntLat\n0\n1.00\n0.04\n0.20\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMIAntLat\n0\n1.00\n0.00\n0.05\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMIecg\n0\n1.00\n0.02\n0.15\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nAfib\n6\n1.00\n0.00\n0.04\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nAflutter\n7\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nQTcFram\n6\n1.00\n419.93\n23.12\n360.97\n404.89\n419.00\n433.85\n502.00\n▂▇▇▂▁\n\n\nQTcBaz\n6\n1.00\n424.12\n26.18\n359.67\n406.02\n423.17\n441.21\n508.91\n▂▇▇▃▁\n\n\nQTcHod\n6\n1.00\n421.20\n23.15\n367.25\n406.00\n419.50\n435.00\n507.25\n▂▇▆▂▁\n\n\nQTcFrid\n6\n1.00\n420.75\n23.01\n365.79\n405.41\n419.50\n434.51\n502.05\n▂▇▇▂▁\n\n\nCV\n6\n1.00\n1472.56\n632.70\n12.00\n1064.50\n1408.00\n1834.00\n3890.00\n▂▇▅▁▁\n\n\nLVHcv\n6\n1.00\n0.09\n0.28\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nspeechLossEver\n13\n1.00\n0.02\n0.13\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nvisionLossEver\n11\n1.00\n0.03\n0.18\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ndoubleVisionEver\n15\n0.99\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nnumbnessEver\n14\n0.99\n0.06\n0.23\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nparalysisEver\n14\n0.99\n0.03\n0.16\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ndizzynessEver\n15\n0.99\n0.04\n0.19\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nstrokeHx\n0\n1.00\n0.03\n0.16\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMIHx\n0\n1.00\n0.04\n0.20\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nCardiacProcHx\n35\n0.99\n0.06\n0.24\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nCHDHx\n0\n1.00\n0.06\n0.24\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nCarotidAngioHx\n1\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nCVDHx\n0\n1.00\n0.08\n0.27\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nPrivateIns\n9\n1.00\n0.72\n0.45\n0.00\n0.00\n1.00\n1.00\n1.00\n▃▁▁▁▇\n\n\nMedicaidIns\n10\n1.00\n0.08\n0.27\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMedicareIns\n5\n1.00\n0.22\n0.41\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nVAIns\n19\n0.99\n0.07\n0.26\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nInsuranceType\n10\n1.00\n2.33\n2.77\n0.00\n1.00\n1.00\n3.00\n15.00\n▇▂▁▁▁\n\n\nInsured\n6\n1.00\n0.87\n0.33\n0.00\n1.00\n1.00\n1.00\n1.00\n▁▁▁▁▇\n\n\nPublicIns\n5\n1.00\n0.13\n0.33\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nPublicInsType\n2317\n0.13\n2.14\n0.68\n1.00\n2.00\n2.00\n3.00\n3.00\n▃▁▇▁▅\n\n\nPrivatePublicIns\n6\n1.00\n1.25\n0.82\n0.00\n1.00\n1.00\n1.00\n3.00\n▂▇▁▂▂\n\n\ndailyDiscr\n40\n0.98\n2.12\n1.01\n1.00\n1.33\n1.89\n2.67\n5.67\n▇▅▂▁▁\n\n\nlifetimeDiscrm\n80\n0.97\n3.15\n2.10\n0.00\n1.00\n3.00\n5.00\n8.00\n▆▇▃▅▂\n\n\ndiscrmBurden\n404\n0.85\n2.34\n0.77\n1.00\n1.83\n2.17\n2.83\n4.00\n▆▇▅▇▂\n\n\ndepression\n817\n0.69\n10.49\n7.69\n0.00\n5.00\n9.00\n14.00\n41.00\n▇▆▂▁▁\n\n\nperceivedStress\n26\n0.99\n5.32\n4.37\n0.00\n2.00\n5.00\n8.00\n19.00\n▇▆▃▂▁\n\n\nweeklyStress\n1041\n0.61\n81.79\n78.63\n0.00\n27.00\n55.00\n109.00\n405.00\n▇▃▁▁▁\n\n\nIncome\n386\n0.85\n2.92\n1.01\n1.00\n2.00\n3.00\n4.00\n4.00\n▂▅▁▇▇\n\n\noccupation\n19\n0.99\n2.46\n1.70\n1.00\n1.00\n2.00\n3.00\n6.00\n▇▂▁▁▂\n\n\nedu3cat\n334\n0.87\n1.78\n0.41\n1.00\n2.00\n2.00\n2.00\n2.00\n▂▁▁▁▇\n\n\nHSgrad\n3\n1.00\n0.86\n0.35\n0.00\n1.00\n1.00\n1.00\n1.00\n▁▁▁▁▇\n\n\nPA3cat\n2\n1.00\n0.75\n0.78\n0.00\n0.00\n1.00\n1.00\n2.00\n▇▁▆▁▃\n\n\nnutrition3cat\n241\n0.91\n0.44\n0.52\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▆▁▁\n\n\nSMK3cat\n41\n0.98\n1.76\n0.65\n0.00\n2.00\n2.00\n2.00\n2.00\n▁▁▁▁▇\n\n\nidealHealthSMK\n41\n0.98\n0.87\n0.33\n0.00\n1.00\n1.00\n1.00\n1.00\n▁▁▁▁▇\n\n\nBMI3cat\n4\n1.00\n0.59\n0.71\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▂\n\n\nidealHealthBMI\n4\n1.00\n0.13\n0.34\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nidealHealthPA\n2\n1.00\n0.21\n0.41\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nidealHealthNutrition\n241\n0.91\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ntotChol3cat\n210\n0.92\n1.32\n0.70\n0.00\n1.00\n1.00\n2.00\n2.00\n▂▁▇▁▇\n\n\nidealHealthChol\n210\n0.92\n0.46\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nBP3cat\n10\n1.00\n1.04\n0.62\n0.00\n1.00\n1.00\n1.00\n2.00\n▂▁▇▁▃\n\n\nidealHealthBP\n10\n1.00\n0.21\n0.41\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nglucose3cat\n119\n0.96\n1.28\n0.73\n0.00\n1.00\n1.00\n2.00\n2.00\n▃▁▇▁▇\n\n\nidealHealthDM\n119\n0.96\n0.45\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nvitaminD2\n1344\n0.49\n2.57\n3.84\n0.00\n0.20\n0.50\n3.50\n17.80\n▇▁▁▁▁\n\n\nvitaminD3\n60\n0.98\n13.18\n6.07\n3.30\n8.60\n11.90\n16.70\n33.30\n▆▇▅▂▁\n\n\nvitaminD3epimer\n806\n0.70\n0.52\n0.38\n0.00\n0.30\n0.40\n0.70\n2.20\n▇▅▂▁▁\n\n\ndarkgrnVeg\n107\n0.96\n0.75\n0.73\n0.01\n0.25\n0.50\n1.03\n4.27\n▇▂▁▁▁\n\n\neggs\n107\n0.96\n0.54\n0.62\n0.00\n0.09\n0.32\n0.64\n3.36\n▇▂▁▁▁\n\n\nfish\n107\n0.96\n0.18\n0.38\n0.00\n0.00\n0.09\n0.17\n2.99\n▇▁▁▁▁\n\n\nFakeCensusTractID\n65\n0.98\n134.02\n73.83\n2.00\n66.00\n135.00\n200.00\n268.00\n▅▇▇▇▅\n\n\nnbmedHHincome\n6\n1.00\n33963.00\n15406.32\n13110.00\n25229.00\n28889.00\n43510.00\n104707.00\n▇▃▂▁▁\n\n\nnbpctpoverty\n6\n1.00\n0.23\n0.12\n0.01\n0.11\n0.22\n0.32\n0.53\n▇▇▇▆▂\n\n\nnbpctBlackNH\n6\n1.00\n0.79\n0.27\n0.06\n0.64\n0.93\n0.98\n0.99\n▁▁▁▁▇\n\n\nnbpctWhiteNH\n6\n1.00\n0.20\n0.27\n0.00\n0.01\n0.06\n0.34\n0.91\n▇▁▁▁▁\n\n\nnbSESpc2score\n6\n1.00\n0.60\n0.65\n-1.74\n0.22\n0.75\n1.06\n1.62\n▁▁▅▇▇\n\n\nnbSESanascore\n6\n1.00\n-2.49\n3.72\n-8.91\n-5.48\n-3.14\n-0.40\n8.42\n▆▇▅▃▁\n\n\nnbProblems\n9\n1.00\n1.56\n0.18\n1.19\n1.37\n1.56\n1.71\n2.07\n▂▇▇▅▁\n\n\nnbCohesion\n9\n1.00\n3.02\n0.12\n2.63\n2.93\n3.03\n3.12\n3.21\n▁▂▇▇▇\n\n\nnbViolence\n10\n1.00\n1.26\n0.12\n1.03\n1.15\n1.26\n1.33\n1.76\n▆▇▃▁▁\n\n\nnbK3FavorFoodstore\n6\n1.00\n0.26\n0.21\n0.00\n0.06\n0.23\n0.43\n0.72\n▇▅▃▃▂\n\n\nnbK3paFacilities\n6\n1.00\n0.44\n0.36\n0.00\n0.18\n0.37\n0.62\n1.74\n▇▆▂▁▁\n\n\nnbpctResiden1mi\n360\n0.86\n0.31\n0.11\n0.05\n0.24\n0.32\n0.39\n0.62\n▂▆▇▃▁\n\n\nnbPopDensity1mi\n6\n1.00\n2066.75\n1262.43\n16.32\n1070.28\n2098.44\n3191.84\n4336.95\n▇▇▇▇▅\n\n\nsportIndex\n117\n0.96\n2.21\n1.24\n1.00\n1.00\n2.25\n3.25\n4.75\n▇▂▃▃▁\n\n\nhyIndex\n26\n0.99\n2.31\n0.61\n1.00\n1.86\n2.29\n2.71\n4.14\n▃▇▇▂▁\n\n\nactiveIndex\n14\n0.99\n2.12\n0.79\n1.00\n1.50\n2.00\n2.75\n4.50\n▇▇▆▂▁\n\n\n\n\n\n\n\n2.1.2. Dataset 2 jhs_analysis2 Data Summary\nBelow we use the Skimr package to create a more extension Data Summary of jhs_analysis2\n\nskim(jhs_analysis2)\n\n\nData summary\n\n\nName\njhs_analysis2\n\n\nNumber of rows\n2653\n\n\nNumber of columns\n134\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n132\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsubjid\n0\n1\n1\n4\n0\n2653\n0\n\n\nsex\n0\n1\n4\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nvisit\n0\n1.00\n2.00\n0.00\n2.00\n2.00\n2.00\n2.00\n2.00\n▁▁▇▁▁\n\n\nVisitDate\n0\n1.00\n17318.60\n324.40\n16711.00\n17023.00\n17336.00\n17590.00\n17896.00\n▆▇▆▇▆\n\n\nDaysFromV1\n0\n1.00\n1715.64\n209.55\n1351.00\n1584.00\n1664.00\n1808.00\n2707.00\n▇▇▂▁▁\n\n\nYearsFromV1\n0\n1.00\n4.70\n0.57\n3.70\n4.34\n4.56\n4.95\n7.41\n▇▇▂▁▁\n\n\nARIC\n0\n1.00\n0.29\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nrecruit\n0\n1.00\n3.12\n1.58\n1.00\n1.00\n3.00\n5.00\n5.00\n▇▂▅▆▇\n\n\nageIneligible\n3\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nFastHours\n15\n0.99\n12.27\n4.38\n1.13\n11.70\n13.32\n14.71\n21.57\n▂▁▆▇▁\n\n\nage\n0\n1.00\n58.98\n11.72\n28.21\n50.18\n58.87\n67.89\n87.76\n▁▆▇▇▂\n\n\nbrthyr\n0\n1.00\n1947.95\n12.00\n1920.00\n1939.00\n1948.00\n1957.00\n1980.00\n▂▇▇▆▁\n\n\nbrthmo\n0\n1.00\n6.40\n3.49\n1.00\n3.00\n7.00\n9.00\n12.00\n▇▅▅▅▇\n\n\nmale\n0\n1.00\n0.37\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nmenopause\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nalc\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nalcw\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ncurrentSmoker\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\neverSmoker\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nweight\n43\n0.98\n91.85\n20.88\n48.62\n77.09\n89.07\n103.24\n167.53\n▃▇▅▁▁\n\n\nheight\n30\n0.99\n169.06\n9.46\n147.32\n162.56\n167.64\n175.26\n193.04\n▂▇▆▆▂\n\n\nwaist\n3\n1.00\n102.45\n15.88\n68.58\n91.44\n101.60\n111.76\n157.48\n▃▇▆▂▁\n\n\nhip\n3\n1.00\n114.57\n14.33\n86.36\n104.14\n111.76\n121.92\n166.37\n▃▇▃▁▁\n\n\nBMI\n43\n0.98\n32.15\n6.95\n18.40\n27.27\n31.10\n35.76\n58.80\n▃▇▃▁▁\n\n\nneck\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nbsa\n43\n0.98\n2.01\n0.23\n1.47\n1.85\n2.00\n2.16\n2.75\n▂▇▇▃▁\n\n\nBPmeds\n212\n0.92\n0.69\n0.46\n0.00\n0.00\n1.00\n1.00\n1.00\n▃▁▁▁▇\n\n\nDMmedsOral\n521\n0.80\n0.18\n0.39\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nDMmedsIns\n556\n0.79\n0.08\n0.27\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDMmeds\n212\n0.92\n0.22\n0.41\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nstatinMeds\n211\n0.92\n0.32\n0.47\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nhrtMeds\n549\n0.79\n0.08\n0.28\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nbetaBlkMeds\n527\n0.80\n0.18\n0.39\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\ncalBlkMeds\n512\n0.81\n0.28\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\ndiureticMeds\n436\n0.84\n0.52\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nantiArythMeds\n214\n0.92\n0.06\n0.24\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nmedAcct\n566\n0.79\n2.00\n0.00\n2.00\n2.00\n2.00\n2.00\n2.00\n▁▁▇▁▁\n\n\nBPmedsSelf\n218\n0.92\n0.66\n0.47\n0.00\n0.00\n1.00\n1.00\n1.00\n▅▁▁▁▇\n\n\nDMMedType\n562\n0.79\n0.31\n0.70\n0.00\n0.00\n0.00\n0.00\n3.00\n▇▂▁▁▁\n\n\ndmMedsSelf\n219\n0.92\n0.21\n0.41\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nstatinMedsSelf\n223\n0.92\n0.31\n0.46\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nhrtMedsSelfEver\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nhrtMedsSelf\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nantiArythMedsSelf\n228\n0.91\n0.06\n0.23\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nsbp\n2\n1.00\n126.92\n18.03\n90.82\n114.66\n124.75\n136.67\n200.00\n▃▇▃▁▁\n\n\ndbp\n2\n1.00\n74.18\n9.83\n49.00\n67.58\n74.00\n80.03\n106.00\n▂▇▇▃▁\n\n\nBPjnc7\n2\n1.00\n0.92\n0.85\n0.00\n0.00\n1.00\n1.00\n3.00\n▆▇▁▃▁\n\n\nHTN\n0\n1.00\n0.67\n0.47\n0.00\n0.00\n1.00\n1.00\n1.00\n▃▁▁▁▇\n\n\nabi\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nHbA1c\n683\n0.74\n6.04\n1.05\n4.50\n5.50\n5.80\n6.30\n12.00\n▇▃▁▁▁\n\n\nFPG\n886\n0.67\n104.75\n30.95\n66.00\n90.00\n97.00\n107.00\n305.00\n▇▁▁▁▁\n\n\nFPG3cat\n886\n0.67\n0.52\n0.68\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▂\n\n\nHbA1c3cat\n683\n0.74\n0.82\n0.73\n0.00\n0.00\n1.00\n1.00\n2.00\n▇▁▇▁▃\n\n\nHbA1cIFCC\n683\n0.74\n42.47\n11.50\n25.68\n36.61\n39.89\n45.36\n107.66\n▇▃▁▁▁\n\n\nHbA1cIFCC3cat\n683\n0.74\n0.82\n0.73\n0.00\n0.00\n1.00\n1.00\n2.00\n▇▁▇▁▃\n\n\nfastingInsulin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nDiabetes\n495\n0.81\n0.32\n0.47\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\ndiab3cat\n495\n0.81\n1.03\n0.77\n0.00\n0.00\n1.00\n2.00\n2.00\n▆▁▇▁▆\n\n\nHOMA_B\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nHOMA_IR\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nldl\n899\n0.66\n122.08\n35.76\n44.00\n97.00\n120.00\n146.00\n243.00\n▂▇▆▂▁\n\n\nhdl\n886\n0.67\n53.47\n14.51\n27.00\n43.00\n51.00\n62.00\n109.00\n▅▇▅▁▁\n\n\ntrigs\n886\n0.67\n100.88\n59.46\n22.00\n62.00\n87.00\n124.00\n433.00\n▇▃▁▁▁\n\n\ntotchol\n886\n0.67\n195.47\n39.60\n106.00\n166.00\n193.00\n221.00\n330.00\n▂▇▇▂▁\n\n\nldl5cat\n899\n0.66\n1.31\n1.11\n0.00\n0.00\n1.00\n2.00\n4.00\n▇▇▆▂▁\n\n\nhdl3cat\n886\n0.67\n0.98\n0.78\n0.00\n0.00\n1.00\n2.00\n2.00\n▆▁▇▁▆\n\n\ntrigs4cat\n886\n0.67\n0.20\n0.53\n0.00\n0.00\n0.00\n0.00\n3.00\n▇▁▁▁▁\n\n\nhsCRP\n659\n0.75\n0.54\n0.72\n0.02\n0.12\n0.27\n0.64\n4.75\n▇▁▁▁▁\n\n\neSelectin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\npSelectin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nendothelin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nsCort\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nreninRIA\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nreninIRMA\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\naldosterone\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nleptin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nadiponectin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nCreatinineUSpot\n657\n0.75\n155.30\n81.60\n15.00\n96.00\n145.00\n202.00\n469.00\n▅▇▃▁▁\n\n\nAlbuminUSpot\n750\n0.72\n4.24\n16.29\n0.20\n0.60\n1.00\n1.90\n179.00\n▇▁▁▁▁\n\n\nSCrCC\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nSCrIDMS\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\neGFRmdrd\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\neGFRckdepi\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nCreatinineU24hr\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nAlbuminU24hr\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ndialysisEver\n0\n1.00\n0.01\n0.07\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDialysisDuration\n2644\n0.00\n7.33\n9.31\n1.00\n1.00\n3.00\n10.00\n27.00\n▇▁▁▁▁\n\n\nCKDHx\n2\n1.00\n0.03\n0.17\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nCAC\n612\n0.77\n142.56\n381.43\n0.00\n0.00\n0.00\n70.48\n2997.73\n▇▁▁▁▁\n\n\nAAC\n613\n0.77\n802.81\n1488.63\n0.00\n0.00\n85.48\n863.14\n8335.09\n▇▁▁▁▁\n\n\nVAT50mm\n614\n0.77\n830.19\n376.73\n112.67\n560.68\n776.89\n1050.17\n2157.67\n▃▇▅▁▁\n\n\nSAT50mm\n615\n0.77\n2348.85\n1024.66\n289.33\n1566.49\n2214.60\n3045.99\n5137.73\n▃▇▆▃▁\n\n\nanyCAC\n612\n0.77\n0.46\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nanyAAC\n613\n0.77\n0.65\n0.48\n0.00\n0.00\n1.00\n1.00\n1.00\n▅▁▁▁▇\n\n\nspeechLossEver\n0\n1.00\n0.03\n0.16\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nvisionLossEver\n0\n1.00\n0.05\n0.22\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ndoubleVisionEver\n0\n1.00\n0.02\n0.12\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nnumbnessEver\n0\n1.00\n0.08\n0.27\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nparalysisEver\n0\n1.00\n0.04\n0.19\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ndizzynessEver\n0\n1.00\n0.06\n0.23\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nstrokeHx\n1\n1.00\n0.04\n0.21\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMIHx\n1\n1.00\n0.01\n0.11\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nCardiacProcHx\n3\n1.00\n0.05\n0.21\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nCHDHx\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nCarotidAngioHx\n2594\n0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nCVDHx\n1\n1.00\n0.05\n0.21\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nSMK3cat\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nidealHealthSMK\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nBMI3cat\n43\n0.98\n0.55\n0.69\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▂\n\n\nidealHealthBMI\n43\n0.98\n0.12\n0.32\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nPA3cat\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nidealHealthPA\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nnutrition3cat\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nidealHealthNutrition\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ntotChol3cat\n969\n0.63\n1.21\n0.67\n0.00\n1.00\n1.00\n2.00\n2.00\n▂▁▇▁▆\n\n\nidealHealthChol\n969\n0.63\n0.35\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nBP3cat\n83\n0.97\n0.90\n0.58\n0.00\n1.00\n1.00\n1.00\n2.00\n▃▁▇▁▂\n\n\nidealHealthBP\n83\n0.97\n0.13\n0.33\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nglucose3cat\n796\n0.70\n1.04\n0.69\n0.00\n1.00\n1.00\n2.00\n2.00\n▃▁▇▁▃\n\n\nidealHealthDM\n796\n0.70\n0.26\n0.44\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nFakeCensusTractID\n78\n0.97\n133.27\n72.10\n2.00\n66.00\n135.00\n196.00\n268.00\n▅▇▇▇▅\n\n\nnbmedHHincome\n4\n1.00\n37671.27\n17910.41\n14502.00\n27521.00\n34103.00\n43782.00\n114583.00\n▇▆▁▁▁\n\n\nnbpctpoverty\n4\n1.00\n0.23\n0.14\n0.02\n0.12\n0.20\n0.30\n0.58\n▇▇▇▃▂\n\n\nnbpctBlackNH\n4\n1.00\n0.78\n0.28\n0.08\n0.56\n0.96\n0.98\n1.00\n▁▁▁▁▇\n\n\nnbpctWhiteNH\n4\n1.00\n0.19\n0.27\n0.00\n0.01\n0.02\n0.37\n0.88\n▇▁▁▁▁\n\n\nnbSESpc2score\n4\n1.00\n0.30\n0.67\n-2.33\n-0.09\n0.31\n0.79\n1.51\n▁▁▅▇▆\n\n\nnbSESanascore\n4\n1.00\n-2.15\n3.53\n-8.85\n-4.18\n-2.53\n0.28\n9.65\n▅▇▇▂▁\n\n\nnbProblems\n10\n1.00\n1.54\n0.18\n1.13\n1.37\n1.56\n1.71\n2.06\n▂▇▆▅▁\n\n\nnbCohesion\n10\n1.00\n3.02\n0.12\n2.63\n2.94\n3.03\n3.12\n3.21\n▁▂▆▇▇\n\n\nnbViolence\n11\n1.00\n1.25\n0.12\n1.03\n1.15\n1.24\n1.32\n1.64\n▇▅▇▂▁\n\n\nnbK3FavorFoodstore\n5\n1.00\n0.28\n0.26\n0.00\n0.04\n0.22\n0.48\n0.96\n▇▃▂▂▁\n\n\nnbK3paFacilities\n5\n1.00\n0.63\n0.50\n0.00\n0.20\n0.54\n0.98\n2.01\n▇▆▅▂▁\n\n\nnbpctResiden1mi\n419\n0.84\n0.32\n0.12\n0.04\n0.24\n0.32\n0.39\n0.63\n▂▆▇▃▁\n\n\nnbPopDensity1mi\n4\n1.00\n1768.67\n1088.93\n15.02\n946.59\n1744.26\n2701.66\n3839.42\n▇▇▇▇▅\n\n\n\n\n\n\n\n\n2.2. Data Processing\nWe will select the most relevant attributes that might contribute to increase Health care risk factor such as stroke and other cardiovascular diseases.\nsubjid: participant ID\nBPmeds; categorical variable with categories 1 = takes blood pressure lowering medication, 0 = does not take blood pressure lowering drugs\nwaist: waist circumference in cm.\nage: age in years\nHbA1c: Hemoglobin A1c, measures the average blood sugar (glucose)\nBPjnc7: categorical variable with categories where 0=normotensive, 1=pre-hypertensive, 2=Stage I hypertension, 3=Stage II hypertension\ncurrentSmoker: categorical variable with categories (1 = current smoker, 0 = not current smoker)\neverSmoker: categorical variable with categories (1 = has smoked at least 200 cigarettes in their lifetime, 0 = has not smoked more than 200 cigarettes)\nHTN: categorical variable for Hypertension Status where 1 = yes, 0 = no\ntotchol: Fasting Total Cholesterol\nBMI: Body mass index\nLVMecho: Left Ventricular Mass in grams by Echocardiography\nHSgrad: categorical variable for High School Graduate where 0=did not graduate high school, 1=graduated high school\nBMI3cat: categorize health status based on BMI where 0=poor health, 1=intermediate health, 2=ideal health\nnutrition3cat: health status as defined by nutrition from self-reported food inventories where 0=poor health, 1=intermediate health, 2=ideal health\nidealHealthBP blood pressure where 1=ideal health, 0=not ideal health\nidealHealthSMK: smoking status where 1=ideal health, 0=not ideal health\nidealHealthDM: diabetes where 1=ideal health, 0=not ideal health\nidealHealthNutrition: diet where 1=ideal health, 0=not ideal health\nidealHealthPA: physical activity where 1=ideal health, 0=not ideal health\nidealHealthBMI: obesity where 1=ideal health, 0=not ideal health\nidealHealthChol: high cholesterol where 1=ideal health, 0=not ideal health\nPrivatePublicIns: Indicator of health by Public or Private insurance where 0=uninsured, 1=private insurance only, 2=public insurance only, 3=private and public insurances\n\nkeeps1 &lt;- c(\"subjid\", \"HbA1c\" , \"age\", \"waist\",\"BPmeds\", \"everSmoker\", \"currentSmoker\", \"BPjnc7\", \"HTN\", \"totchol\", \"BMI\", \"LVMecho\", \"HSgrad\", \"BMI3cat\", \"nutrition3cat\", \"idealHealthBP\", \"idealHealthSMK\", \"idealHealthDM\", \"idealHealthNutrition\", \"idealHealthPA\", \"idealHealthBMI\", \"idealHealthChol\", \"PrivatePublicIns\")\nkeeps2 &lt;- c(\"subjid\", \"HbA1c\" , \"age\", \"waist\",\"BPmeds\", \"everSmoker\", \"currentSmoker\", \"BPjnc7\", \"HTN\", \"totchol\", \"BMI\", \"BMI3cat\", \"nutrition3cat\", \"idealHealthBP\", \"idealHealthSMK\", \"idealHealthDM\", \"idealHealthNutrition\", \"idealHealthPA\", \"idealHealthBMI\", \"idealHealthChol\")\n\n\njhs_data1 &lt;- jhs_analysis1 %&gt;% select(all_of(keeps1)) %&gt;% na.omit()\n\n# `LVMecho`, `HSgrad`, and `PrivatePublicIns` -- dont exist in jhs_analysis2\njhs_data2 &lt;- jhs_analysis2 %&gt;% select(all_of(keeps2)) %&gt;% na.omit()\n\nThis code chunk I left for debugging, might help if u have furhter questions.\n\n# jhs_analysis1_variables\n# jhs_data1 %&gt;%\n#   count(smoking)\n\n\n2.2.1 Create Variable Smoking (binning)\nCreate a variable called smoking that combines currentSmoker (1 = current smoker, 0 = not current smoker) and everSmoker (1 = has smoked at least 200 cigarettes in their lifetime, 0 = has not smoked more than 200 cigarettes) into a three level variable, where 1 = current smoker, 2 = former smoker, 3 = never smoker. This variable was only obtained at first patient visit (jhs_analysis1) so on the visit two (jhs_analysis2) it will be NaN.\n\njhs_data1 &lt;- jhs_data1 %&gt;% mutate(\n  smoking = case_when(currentSmoker==\"1\" & everSmoker==\"1\" ~ 1,\n                      currentSmoker==\"0\" & everSmoker==\"1\" ~ 2,\n                      currentSmoker==\"0\" & everSmoker==\"0\" ~ 3)\n) %&gt;%\n  dummy_cols(select_columns = \"smoking\")"
  },
  {
    "objectID": "posts/renan-blog-post-6/index.html#calculate-the-t-and-d-weights",
    "href": "posts/renan-blog-post-6/index.html#calculate-the-t-and-d-weights",
    "title": "The Jackson Heart Study (JHS) Dataset",
    "section": "3. Calculate the t and d weights",
    "text": "3. Calculate the t and d weights\nTypical-Weight (TW)\nThe Typical-Weight (\\(\\text{TW}\\)) measures how important or typical a pattern is to the class of interest. It is the ratio of the pattern’s frequency (support) in the target class to its overall frequency in the entire dataset:\n\\[\\text{TW}(P, C) = \\frac{\\text{Support}(P \\text{ in } C)}{\\text{Support}(P \\text{ in } D)}\\]\nA high Typical-Weight (close to \\(1\\)) means the pattern is highly typical of the class.\nFor example, the pattern BPmeds has a \\(\\text{Typical-Weight}\\) of \\(\\mathbf{0.740}\\). This means that \\(74.0\\%\\) of all individuals who take BP medication are in the Hypertension (\\(\\text{HTN}=1\\)) class.\nDiscriminative-Weight (DW)\nThe Discriminative-Weight (\\(\\text{DW}\\)) measures how well a pattern distinguishes the class of interest from all other classes. It is calculated as the difference between the pattern’s frequency in the target class and its maximum frequency in any other class:\n\\[\\text{DW}(P, C) = \\text{Support}(P \\text{ in } C) - \\max_{C' \\neq C} \\{\\text{Support}(P \\text{ in } C')\\}\\]\nA positive Discriminative-Weight means the pattern occurs more frequently with the target class than with any other class, making it a good discriminator.\nFor example, the pattern BPmeds has a \\(\\text{Discriminative-Weight}\\) of \\(\\mathbf{0.135}\\). This means that the fraction of the total population who are Hypertensive and take BP meds is \\(13.5\\) percentage points higher than the fraction of the total population who are not Hypertensive and take BP meds, making it the strongest positive indicator for \\(\\text{HTN}=1\\).\n\n# --- 1. Define the Target Class and Patterns ---\nTARGET_CLASS &lt;- 1  # HTN = 1 (Hypertension)\nPATTERNS &lt;- c(\"smoking_1\", \"smoking_2\", \"smoking_3\", \"BPmeds\", \"idealHealthBP\")\n\n# --- 2. Initialize a data frame to store results ---\nweights_df &lt;- data.frame(\n  Pattern = PATTERNS,\n  Typical_Weight = NA,\n  Discriminative_Weight = NA\n)\n\nN_total &lt;- nrow(jhs_data1)\n\n# --- 3. Loop through each pattern and calculate weights ---\nfor (i in 1:length(PATTERNS)) {\n  P &lt;- PATTERNS[i]\n\n  # Ensure the pattern column is numeric (0/1) for summation\n  jhs_data1[[P]] &lt;- as.numeric(as.character(jhs_data1[[P]]))\n\n  # A. Calculate Supports (normalized by total N)\n  # Total support for the pattern S(P)\n  count_P &lt;- sum(jhs_data1[[P]], na.rm = TRUE)\n  S_P &lt;- count_P / N_total\n\n  # Support for the pattern in the target class S(P in C)\n  count_P_in_class &lt;- sum(jhs_data1[[P]][jhs_data1$HTN == TARGET_CLASS], na.rm = TRUE)\n  S_P_in_class &lt;- count_P_in_class / N_total\n\n  # Support for the pattern in the non-target class S(P in not C)\n  count_P_in_not_class &lt;- sum(jhs_data1[[P]][jhs_data1$HTN != TARGET_CLASS], na.rm = TRUE)\n  S_P_in_not_class &lt;- count_P_in_not_class / N_total\n\n  # B. Calculate Typical-Weight (TW)\n  # TW(P, C) = S(P in C) / S(P)\n  TW &lt;- ifelse(S_P &gt; 0, S_P_in_class / S_P, 0)\n\n  # C. Calculate Discriminative-Weight (DW)\n  # DW(P, C) = S(P in C) - S(P in not C) (since only two classes)\n  DW &lt;- S_P_in_class - S_P_in_not_class\n\n  # D. Store Results\n  weights_df[i, \"Typical_Weight\"] &lt;- TW\n  weights_df[i, \"Discriminative_Weight\"] &lt;- DW\n}\n\n# --- 4. Display the results ---\nprint(weights_df)\n\n        Pattern Typical_Weight Discriminative_Weight\n1     smoking_1      0.4459459           -0.01107266\n2     smoking_2      0.5644444            0.02006920\n3     smoking_3      0.4589552           -0.06089965\n4        BPmeds      0.9379845            0.39100346\n5 idealHealthBP      0.0000000           -0.26228374\n\n\n\n# 1. Ensure the grouping variable is a factor\njhs_data1$BPjnc7 &lt;- as.factor(jhs_data1$BPjnc7)\n\n# 2. Standardize predictor variables for accurate typical-weights\njhs_data1_std &lt;- jhs_data1 %&gt;% \n  mutate(across(c(HbA1c, age, waist, totchol, BMI, LVMecho), scale))\n\n\n# 3. Perform LDA on standardized data to get the Typical-Weights (Canonical Coefficients)\nlda_model_std &lt;- lda(BPjnc7 ~ HbA1c + age + waist + totchol + BMI + LVMecho, data = jhs_data1_std)\ntypical_weights &lt;- as.data.frame(lda_model_std$scaling)\ncolnames(typical_weights) &lt;- paste0(\"LD\", 1:ncol(typical_weights), \"_Typical_Weight\")\ncat(\"\\n\\nTypical Weights (Standardized Canonical Coefficients):\\n\")\n\n\n\nTypical Weights (Standardized Canonical Coefficients):\n\nprint(typical_weights)\n\n        LD1_Typical_Weight LD2_Typical_Weight LD3_Typical_Weight\nHbA1c           0.02106858        -0.25500991        -0.64732344\nage            -0.61431913         0.62194047        -0.15678145\nwaist           0.21146869         1.16093565         0.05985403\ntotchol        -0.19628960        -0.03476905         0.02279955\nBMI            -0.17326490        -0.89166009        -0.68357268\nLVMecho        -0.80941649        -0.67261118         0.40405762\n\n\n```{r}\n# 4. Perform LDA on original data for Discriminative-Weights (Structure Coefficients)\nlda_model &lt;- lda(BPjnc7 ~ HbA1c + age + waist + totchol + BMI + LVMecho, data = jhs_data1)\ndiscriminative_weights &lt;- rstatix::get_discriminant_loadings(lda_model)\ncat(\"\\n\\nDiscriminative Weights (Structure Coefficients/Loadings):\\n\")\nprint(discriminative_weights)\n```\n```{r}\n# 1. Calculate the Discriminant Function Scores (DF scores) for each subject\n# The 'predict' function gives you the values for LD1, LD2, etc.\nld_scores &lt;- predict(lda_model, jhs_data1)$x\nld_scores &lt;- as.data.frame(ld_scores)\n\n# 2. Extract the continuous predictor variables\npredictor_vars &lt;- jhs_data1 %&gt;%\n  dplyr::select(HbA1c, age, waist, totchol, BMI, LVMecho)\n\n# 3. Calculate the Discriminative Weights (Structure Coefficients)\n# These are the correlations between the predictor variables and the LD scores.\ndiscriminative_weights &lt;- cor(predictor_vars, ld_scores)\n\ncat(\"\\n\\nDiscriminative Weights (Structure Coefficients/Loadings):\\n\")\nprint(discriminative_weights)\n```\n\n# Save the weights for the final explanation\n# Placeholder for actual data analysis\n# typical_weight_age &lt;- typical_weights[\"age\", \"LD1_Typical_Weight\"]\n# discriminative_weight_age &lt;- discriminative_weights %&gt;% \n#   filter(variable == \"age\") %&gt;% \n#   pull(LD1)\n\n\nExplanation with Placeholder Values\nThe typical-weights are the standardized coefficients used to calculate a subject’s discriminant score, showing the relative contribution of each variable. For instance, age has a typical-weight of 0.85 on the first discriminant function (LD1), meaning it is a strong contributor to the function, while the discriminative-weights are the correlations between the predictors and the discriminant functions, indicating shared variance. Age has a discriminative-weight (loading) of 0.91 on LD1, suggesting it is highly correlated with the primary dimension that separates the BPjnc7 groups.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences"
  },
  {
    "objectID": "posts/renan-blog-post-5/index.html",
    "href": "posts/renan-blog-post-5/index.html",
    "title": "The Jackson Heart Study (JHS) Dataset",
    "section": "",
    "text": "The Jackson Heart Study (JHS) dataset can be obtained by sending an access. I would like to thank Dr. Samantha Seals for being kind enough to provide me with the datasets for the JHS."
  },
  {
    "objectID": "posts/renan-blog-post-5/index.html#introduction",
    "href": "posts/renan-blog-post-5/index.html#introduction",
    "title": "The Jackson Heart Study (JHS) Dataset",
    "section": "1. Introduction",
    "text": "1. Introduction\nIn this project we will be evaluating variables that are statistically likely to increase the risk of causing some health diseases related to Heart.\n\n1.1 Load packages\n\npackages = c(\"brant\", \"car\", \"fastDummies\", \"fitdistrplus\", \"jtools\", \"plyr\", \"sas7bdat\", \"sjPlot\", \"skimr\", \"tidyverse\", \"rstatix\", \"ggplot2\", \"MASS\")\n\n# install.packages(packages)\nlapply(packages, library, character.only = TRUE)\n\n\n\n1.1. Load the Dataset\nThe code below automates loading the datasets from the folder datasets located at the root of the github repository. We will Import the data for the JHS study for visit 1 and for the JHS study for visit 2 from analysis1.sas7bdat and analysis2.sas7bdat.\n\n\nCode\nfind_git_root &lt;- function(start = getwd()) {\n  path &lt;- normalizePath(start, winslash = \"/\", mustWork = TRUE)\n  while (path != dirname(path)) {\n    if (dir.exists(file.path(path, \".git\"))) return(path)\n    path &lt;- dirname(path)\n  }\n  stop(\"No .git directory found — are you inside a Git repository?\")\n}\n\nrepo_root &lt;- find_git_root()\ndatasets_path &lt;- file.path(repo_root, \"datasets\")\nhealth_data_path &lt;- file.path(datasets_path, \"jhs_data\")\n\njhs_analysis1_path &lt;- file.path(health_data_path, \"analysis1.sas7bdat\")\njhs_analysis2_path &lt;- file.path(health_data_path, \"analysis2.sas7bdat\")\njhs_analysis1 &lt;- read.sas7bdat(jhs_analysis1_path)\njhs_analysis2 &lt;- read.sas7bdat(jhs_analysis2_path)"
  },
  {
    "objectID": "posts/renan-blog-post-5/index.html#data-exploration-and-processing",
    "href": "posts/renan-blog-post-5/index.html#data-exploration-and-processing",
    "title": "The Jackson Heart Study (JHS) Dataset",
    "section": "2. Data Exploration and Processing",
    "text": "2. Data Exploration and Processing\nIn this section which is intended for the Project 1 assignment we will be exploring the dataset and processing it to extract the most relevant Attributes.\n\n2.1. Dataset Features\nThe Jackson Heart Study (JHS) has a JHS Variable Explorer which helps to explore all the available variables, its characteristics, data types and the structure as some variables have extensive contextual information about they were obtained and their associated study.\nThe objects jhs_analysis1_variables and jhs_analysis2_variables have the individual features and I found useful when I need to quickly check if my variable is available.\n\n# Listing all the attributes in the Dataset\n# making a datatable is too slow and inneficient\n# DT::datatable(jhs_analysis1, options = list(pageLength = 5))\n# DT::datatable(jhs_analysis2, options = list(pageLength = 5))\njhs_analysis1_variables = knitr::kable(data.frame(Feature = names(jhs_analysis1)))\njhs_analysis2_variables = knitr::kable(data.frame(Feature = names(jhs_analysis2)))\n\nComparisson side by side of all the available variables in both Datasets.\n\n# Extract variable names as vectors\nvars1 &lt;- names(jhs_analysis1)\nvars2 &lt;- names(jhs_analysis2)\n\n# Compare variable names\ncommon_vars &lt;- intersect(vars1, vars2)\nonly_in_analysis1 &lt;- setdiff(vars1, vars2)\nonly_in_analysis2 &lt;- setdiff(vars2, vars1)\n\n# Create comparison table\ncomparison &lt;- data.frame(\n  Variable = union(vars1, vars2),\n  In_Analysis1 = union(vars1, vars2) %in% vars1,\n  In_Analysis2 = union(vars1, vars2) %in% vars2\n)\n\nknitr::kable(comparison, caption = \"Variable Comparison Between Datasets\")\n\n\nVariable Comparison Between Datasets\n\n\nVariable\nIn_Analysis1\nIn_Analysis2\n\n\n\n\nsubjid\nTRUE\nTRUE\n\n\nvisit\nTRUE\nTRUE\n\n\nVisitDate\nTRUE\nTRUE\n\n\nDaysFromV1\nTRUE\nTRUE\n\n\nYearsFromV1\nTRUE\nTRUE\n\n\nARIC\nTRUE\nTRUE\n\n\nrecruit\nTRUE\nTRUE\n\n\nageIneligible\nTRUE\nTRUE\n\n\nFastHours\nTRUE\nTRUE\n\n\nage\nTRUE\nTRUE\n\n\nbrthyr\nTRUE\nTRUE\n\n\nbrthmo\nTRUE\nTRUE\n\n\nsex\nTRUE\nTRUE\n\n\nmale\nTRUE\nTRUE\n\n\nmenopause\nTRUE\nTRUE\n\n\nalc\nTRUE\nTRUE\n\n\nalcw\nTRUE\nTRUE\n\n\ncurrentSmoker\nTRUE\nTRUE\n\n\neverSmoker\nTRUE\nTRUE\n\n\nweight\nTRUE\nTRUE\n\n\nheight\nTRUE\nTRUE\n\n\nwaist\nTRUE\nTRUE\n\n\nneck\nTRUE\nTRUE\n\n\nBMI\nTRUE\nTRUE\n\n\nhip\nTRUE\nTRUE\n\n\nbsa\nTRUE\nTRUE\n\n\nBPmeds\nTRUE\nTRUE\n\n\nDMmedsOral\nTRUE\nTRUE\n\n\nDMmedsIns\nTRUE\nTRUE\n\n\nDMmeds\nTRUE\nTRUE\n\n\nstatinMeds\nTRUE\nTRUE\n\n\nhrtMeds\nTRUE\nTRUE\n\n\nbetaBlkMeds\nTRUE\nTRUE\n\n\ncalBlkMeds\nTRUE\nTRUE\n\n\ndiureticMeds\nTRUE\nTRUE\n\n\nantiArythMeds\nTRUE\nTRUE\n\n\nmedAcct\nTRUE\nTRUE\n\n\nBPmedsSelf\nTRUE\nTRUE\n\n\nDMMedType\nTRUE\nTRUE\n\n\ndmMedsSelf\nTRUE\nTRUE\n\n\nstatinMedsSelf\nTRUE\nTRUE\n\n\nhrtMedsSelfEver\nTRUE\nTRUE\n\n\nhrtMedsSelf\nTRUE\nTRUE\n\n\nantiArythMedsSelf\nTRUE\nTRUE\n\n\nsbp\nTRUE\nTRUE\n\n\ndbp\nTRUE\nTRUE\n\n\nBPjnc7\nTRUE\nTRUE\n\n\nHTN\nTRUE\nTRUE\n\n\nabi\nTRUE\nTRUE\n\n\nHbA1c\nTRUE\nTRUE\n\n\nFPG\nTRUE\nTRUE\n\n\nFPG3cat\nTRUE\nTRUE\n\n\nHbA1c3cat\nTRUE\nTRUE\n\n\nHbA1cIFCC\nTRUE\nTRUE\n\n\nHbA1cIFCC3cat\nTRUE\nTRUE\n\n\nfastingInsulin\nTRUE\nTRUE\n\n\nDiabetes\nTRUE\nTRUE\n\n\ndiab3cat\nTRUE\nTRUE\n\n\nHOMA_B\nTRUE\nTRUE\n\n\nHOMA_IR\nTRUE\nTRUE\n\n\nldl\nTRUE\nTRUE\n\n\nhdl\nTRUE\nTRUE\n\n\ntrigs\nTRUE\nTRUE\n\n\ntotchol\nTRUE\nTRUE\n\n\nldl5cat\nTRUE\nTRUE\n\n\nhdl3cat\nTRUE\nTRUE\n\n\ntrigs4cat\nTRUE\nTRUE\n\n\nLEPTIN\nTRUE\nFALSE\n\n\nHSCRP\nTRUE\nFALSE\n\n\nENDOTHELIN\nTRUE\nFALSE\n\n\nALDOSTERONE\nTRUE\nFALSE\n\n\neSelectin\nTRUE\nTRUE\n\n\npSelectin\nTRUE\nTRUE\n\n\nsCort\nTRUE\nTRUE\n\n\nreninRIA\nTRUE\nTRUE\n\n\nreninIRMA\nTRUE\nTRUE\n\n\nadiponectin\nTRUE\nTRUE\n\n\nCreatinineU24hr\nTRUE\nTRUE\n\n\nAlbuminU24hr\nTRUE\nTRUE\n\n\nCreatinineUSpot\nTRUE\nTRUE\n\n\nAlbuminUSpot\nTRUE\nTRUE\n\n\nSCrCC\nTRUE\nTRUE\n\n\nSCrIDMS\nTRUE\nTRUE\n\n\neGFRmdrd\nTRUE\nTRUE\n\n\neGFRckdepi\nTRUE\nTRUE\n\n\nDialysisEver\nTRUE\nFALSE\n\n\nDialysisDuration\nTRUE\nTRUE\n\n\nCKDHx\nTRUE\nTRUE\n\n\nmaneuvers\nTRUE\nFALSE\n\n\nasthma\nTRUE\nFALSE\n\n\nFVC\nTRUE\nFALSE\n\n\nFEV1\nTRUE\nFALSE\n\n\nFEV6\nTRUE\nFALSE\n\n\nFEV1PP\nTRUE\nFALSE\n\n\nFVCPP\nTRUE\nFALSE\n\n\nEF\nTRUE\nFALSE\n\n\nDiastLVdia\nTRUE\nFALSE\n\n\nSystLVdia\nTRUE\nFALSE\n\n\nLVMecho\nTRUE\nFALSE\n\n\nLVMindex\nTRUE\nFALSE\n\n\nLVH\nTRUE\nFALSE\n\n\nEF3cat\nTRUE\nFALSE\n\n\nFS\nTRUE\nFALSE\n\n\nRWT\nTRUE\nFALSE\n\n\necgHR\nTRUE\nFALSE\n\n\nQRS\nTRUE\nFALSE\n\n\nQT\nTRUE\nFALSE\n\n\nConductionDefect\nTRUE\nFALSE\n\n\nMajorScarAnt\nTRUE\nFALSE\n\n\nMinorScarAnt\nTRUE\nFALSE\n\n\nRepolarAnt\nTRUE\nFALSE\n\n\nMIant\nTRUE\nFALSE\n\n\nMajorScarPost\nTRUE\nFALSE\n\n\nMinorScarPost\nTRUE\nFALSE\n\n\nRepolarPost\nTRUE\nFALSE\n\n\nMIpost\nTRUE\nFALSE\n\n\nMajorScarAntLat\nTRUE\nFALSE\n\n\nMinorScarAntLat\nTRUE\nFALSE\n\n\nRepolarAntLat\nTRUE\nFALSE\n\n\nMIAntLat\nTRUE\nFALSE\n\n\nMIecg\nTRUE\nFALSE\n\n\nAfib\nTRUE\nFALSE\n\n\nAflutter\nTRUE\nFALSE\n\n\nQTcFram\nTRUE\nFALSE\n\n\nQTcBaz\nTRUE\nFALSE\n\n\nQTcHod\nTRUE\nFALSE\n\n\nQTcFrid\nTRUE\nFALSE\n\n\nCV\nTRUE\nFALSE\n\n\nLVHcv\nTRUE\nFALSE\n\n\nspeechLossEver\nTRUE\nTRUE\n\n\nvisionLossEver\nTRUE\nTRUE\n\n\ndoubleVisionEver\nTRUE\nTRUE\n\n\nnumbnessEver\nTRUE\nTRUE\n\n\nparalysisEver\nTRUE\nTRUE\n\n\ndizzynessEver\nTRUE\nTRUE\n\n\nstrokeHx\nTRUE\nTRUE\n\n\nMIHx\nTRUE\nTRUE\n\n\nCardiacProcHx\nTRUE\nTRUE\n\n\nCHDHx\nTRUE\nTRUE\n\n\nCarotidAngioHx\nTRUE\nTRUE\n\n\nCVDHx\nTRUE\nTRUE\n\n\nPrivateIns\nTRUE\nFALSE\n\n\nMedicaidIns\nTRUE\nFALSE\n\n\nMedicareIns\nTRUE\nFALSE\n\n\nVAIns\nTRUE\nFALSE\n\n\nInsuranceType\nTRUE\nFALSE\n\n\nInsured\nTRUE\nFALSE\n\n\nPublicIns\nTRUE\nFALSE\n\n\nPublicInsType\nTRUE\nFALSE\n\n\nPrivatePublicIns\nTRUE\nFALSE\n\n\ndailyDiscr\nTRUE\nFALSE\n\n\nlifetimeDiscrm\nTRUE\nFALSE\n\n\ndiscrmBurden\nTRUE\nFALSE\n\n\ndepression\nTRUE\nFALSE\n\n\nperceivedStress\nTRUE\nFALSE\n\n\nweeklyStress\nTRUE\nFALSE\n\n\nfmlyinc\nTRUE\nFALSE\n\n\nIncome\nTRUE\nFALSE\n\n\noccupation\nTRUE\nFALSE\n\n\nedu3cat\nTRUE\nFALSE\n\n\nHSgrad\nTRUE\nFALSE\n\n\nPA3cat\nTRUE\nTRUE\n\n\nnutrition3cat\nTRUE\nTRUE\n\n\nSMK3cat\nTRUE\nTRUE\n\n\nidealHealthSMK\nTRUE\nTRUE\n\n\nBMI3cat\nTRUE\nTRUE\n\n\nidealHealthBMI\nTRUE\nTRUE\n\n\nidealHealthPA\nTRUE\nTRUE\n\n\nidealHealthNutrition\nTRUE\nTRUE\n\n\ntotChol3cat\nTRUE\nTRUE\n\n\nidealHealthChol\nTRUE\nTRUE\n\n\nBP3cat\nTRUE\nTRUE\n\n\nidealHealthBP\nTRUE\nTRUE\n\n\nglucose3cat\nTRUE\nTRUE\n\n\nidealHealthDM\nTRUE\nTRUE\n\n\nvitaminD2\nTRUE\nFALSE\n\n\nvitaminD3\nTRUE\nFALSE\n\n\nvitaminD3epimer\nTRUE\nFALSE\n\n\ndarkgrnVeg\nTRUE\nFALSE\n\n\neggs\nTRUE\nFALSE\n\n\nfish\nTRUE\nFALSE\n\n\nFakeCensusTractID\nTRUE\nTRUE\n\n\nnbmedHHincome\nTRUE\nTRUE\n\n\nnbpctpoverty\nTRUE\nTRUE\n\n\nnbpctBlackNH\nTRUE\nTRUE\n\n\nnbpctWhiteNH\nTRUE\nTRUE\n\n\nnbSESpc2score\nTRUE\nTRUE\n\n\nnbSESanascore\nTRUE\nTRUE\n\n\nnbProblems\nTRUE\nTRUE\n\n\nnbCohesion\nTRUE\nTRUE\n\n\nnbViolence\nTRUE\nTRUE\n\n\nnbK3FavorFoodstore\nTRUE\nTRUE\n\n\nnbK3paFacilities\nTRUE\nTRUE\n\n\nnbpctResiden1mi\nTRUE\nTRUE\n\n\nnbPopDensity1mi\nTRUE\nTRUE\n\n\nsportIndex\nTRUE\nFALSE\n\n\nhyIndex\nTRUE\nFALSE\n\n\nactiveIndex\nTRUE\nFALSE\n\n\nhsCRP\nFALSE\nTRUE\n\n\nendothelin\nFALSE\nTRUE\n\n\naldosterone\nFALSE\nTRUE\n\n\nleptin\nFALSE\nTRUE\n\n\ndialysisEver\nFALSE\nTRUE\n\n\nCAC\nFALSE\nTRUE\n\n\nAAC\nFALSE\nTRUE\n\n\nVAT50mm\nFALSE\nTRUE\n\n\nSAT50mm\nFALSE\nTRUE\n\n\nanyCAC\nFALSE\nTRUE\n\n\nanyAAC\nFALSE\nTRUE\n\n\n\n\n\n\n2.1.1. Dataset 1 jhs_analysis1 Data Summary\nBelow we use the Skimr package to create a more extension Data Summary of jhs_analysis1\n\nskim(jhs_analysis1)\n\n\nData summary\n\n\nName\njhs_analysis1\n\n\nNumber of rows\n2653\n\n\nNumber of columns\n198\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n195\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsubjid\n0\n1\n1\n4\n0\n2653\n0\n\n\nsex\n0\n1\n4\n6\n0\n2\n0\n\n\nfmlyinc\n0\n1\n0\n1\n374\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nvisit\n0\n1.00\n1.00\n0.00\n1.00\n1.00\n1.00\n1.00\n1.00\n▁▁▇▁▁\n\n\nVisitDate\n0\n1.00\n15602.94\n345.53\n14877.00\n15335.00\n15609.00\n15873.00\n16168.00\n▅▅▇▇▇\n\n\nDaysFromV1\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nYearsFromV1\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nARIC\n0\n1.00\n0.29\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nrecruit\n0\n1.00\n3.12\n1.58\n1.00\n1.00\n3.00\n5.00\n5.00\n▇▂▅▆▇\n\n\nageIneligible\n3\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nFastHours\n6\n1.00\n12.82\n3.29\n0.75\n12.00\n13.00\n14.22\n25.25\n▁▁▇▁▁\n\n\nage\n0\n1.00\n54.28\n11.74\n22.91\n45.33\n54.20\n63.16\n83.21\n▁▆▇▇▂\n\n\nbrthyr\n0\n1.00\n1947.95\n12.01\n1919.00\n1939.00\n1948.00\n1957.00\n1980.00\n▂▇▇▆▁\n\n\nbrthmo\n0\n1.00\n6.40\n3.49\n1.00\n3.00\n7.00\n9.00\n12.00\n▇▅▅▅▇\n\n\nmale\n0\n1.00\n0.37\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nmenopause\n1354\n0.49\n0.85\n0.36\n0.00\n1.00\n1.00\n1.00\n1.00\n▂▁▁▁▇\n\n\nalc\n10\n1.00\n0.48\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nalcw\n71\n0.97\n1.61\n5.07\n0.00\n0.00\n0.00\n0.50\n42.00\n▇▁▁▁▁\n\n\ncurrentSmoker\n18\n0.99\n0.12\n0.32\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\neverSmoker\n4\n1.00\n0.31\n0.46\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nweight\n4\n1.00\n91.29\n20.86\n49.10\n77.00\n88.00\n102.80\n174.60\n▃▇▃▁▁\n\n\nheight\n3\n1.00\n169.33\n9.37\n149.00\n162.00\n169.00\n176.00\n192.00\n▃▇▇▆▂\n\n\nwaist\n5\n1.00\n100.65\n15.88\n67.00\n90.00\n99.00\n110.00\n152.00\n▂▇▆▂▁\n\n\nneck\n4\n1.00\n38.59\n3.72\n31.00\n36.00\n38.00\n41.00\n49.00\n▃▇▆▃▁\n\n\nBMI\n4\n1.00\n31.86\n6.97\n18.53\n27.02\n30.77\n35.56\n59.57\n▃▇▃▁▁\n\n\nhip\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nbsa\n4\n1.00\n2.01\n0.23\n1.49\n1.84\n2.00\n2.15\n2.74\n▂▇▇▃▁\n\n\nBPmeds\n20\n0.99\n0.51\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nDMmedsOral\n183\n0.93\n0.10\n0.30\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDMmedsIns\n190\n0.93\n0.05\n0.21\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDMmeds\n23\n0.99\n0.13\n0.34\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nstatinMeds\n25\n0.99\n0.13\n0.33\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nhrtMeds\n144\n0.95\n0.19\n0.40\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nbetaBlkMeds\n179\n0.93\n0.11\n0.31\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ncalBlkMeds\n176\n0.93\n0.20\n0.40\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\ndiureticMeds\n156\n0.94\n0.33\n0.47\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nantiArythMeds\n24\n0.99\n0.04\n0.19\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nmedAcct\n20\n0.99\n1.71\n0.58\n0.00\n2.00\n2.00\n2.00\n2.00\n▁▁▂▁▇\n\n\nBPmedsSelf\n45\n0.98\n0.48\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nDMMedType\n193\n0.93\n0.19\n0.57\n0.00\n0.00\n0.00\n0.00\n3.00\n▇▁▁▁▁\n\n\ndmMedsSelf\n47\n0.98\n0.13\n0.34\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nstatinMedsSelf\n51\n0.98\n0.12\n0.33\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nhrtMedsSelfEver\n995\n0.62\n0.43\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nhrtMedsSelf\n1944\n0.27\n0.58\n0.49\n0.00\n0.00\n1.00\n1.00\n1.00\n▆▁▁▁▇\n\n\nantiArythMedsSelf\n47\n0.98\n0.04\n0.19\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nsbp\n4\n1.00\n126.11\n15.58\n93.57\n114.66\n124.75\n134.83\n188.93\n▃▇▅▁▁\n\n\ndbp\n4\n1.00\n75.82\n8.56\n52.63\n70.07\n75.88\n81.69\n99.95\n▁▅▇▃▁\n\n\nBPjnc7\n4\n1.00\n0.87\n0.77\n0.00\n0.00\n1.00\n1.00\n3.00\n▆▇▁▂▁\n\n\nHTN\n0\n1.00\n0.53\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nabi\n244\n0.91\n1.22\n0.15\n0.63\n1.13\n1.21\n1.31\n1.83\n▁▃▇▂▁\n\n\nHbA1c\n88\n0.97\n5.88\n1.11\n4.20\n5.30\n5.60\n6.10\n12.00\n▇▅▁▁▁\n\n\nFPG\n185\n0.93\n97.58\n26.48\n70.00\n85.00\n91.00\n100.00\n302.00\n▇▁▁▁▁\n\n\nFPG3cat\n185\n0.93\n0.33\n0.61\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▂▁▁\n\n\nHbA1c3cat\n88\n0.97\n0.65\n0.74\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▂\n\n\nHbA1cIFCC\n88\n0.97\n40.72\n12.11\n22.41\n34.43\n37.71\n43.17\n107.66\n▇▅▁▁▁\n\n\nHbA1cIFCC3cat\n88\n0.97\n0.65\n0.74\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▂\n\n\nfastingInsulin\n195\n0.93\n17.62\n11.63\n3.00\n11.00\n15.00\n21.00\n92.00\n▇▂▁▁▁\n\n\nDiabetes\n21\n0.99\n0.19\n0.40\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\ndiab3cat\n21\n0.99\n0.73\n0.76\n0.00\n0.00\n1.00\n1.00\n2.00\n▇▁▆▁▃\n\n\nHOMA_B\n613\n0.77\n225.03\n123.63\n35.06\n143.77\n193.64\n271.92\n855.76\n▇▆▁▁▁\n\n\nHOMA_IR\n613\n0.77\n3.54\n2.04\n0.53\n2.16\n3.01\n4.42\n12.99\n▇▆▂▁▁\n\n\nldl\n210\n0.92\n126.49\n35.72\n42.00\n101.50\n124.00\n148.00\n240.00\n▂▇▇▃▁\n\n\nhdl\n197\n0.93\n51.68\n13.99\n25.00\n41.00\n50.00\n60.00\n102.00\n▅▇▅▁▁\n\n\ntrigs\n197\n0.93\n104.20\n61.08\n27.00\n64.00\n90.00\n126.00\n485.00\n▇▃▁▁▁\n\n\ntotchol\n197\n0.93\n199.04\n38.55\n111.00\n173.00\n196.00\n222.00\n327.00\n▂▇▇▂▁\n\n\nldl5cat\n210\n0.92\n1.43\n1.12\n0.00\n1.00\n1.00\n2.00\n4.00\n▆▇▆▃▁\n\n\nhdl3cat\n197\n0.93\n0.88\n0.78\n0.00\n0.00\n1.00\n2.00\n2.00\n▇▁▇▁▆\n\n\ntrigs4cat\n197\n0.93\n0.22\n0.55\n0.00\n0.00\n0.00\n0.00\n3.00\n▇▁▁▁▁\n\n\nLEPTIN\n58\n0.98\n27.92\n22.09\n1.30\n10.20\n23.20\n39.45\n125.60\n▇▅▂▁▁\n\n\nHSCRP\n43\n0.98\n0.49\n0.67\n0.01\n0.11\n0.26\n0.55\n4.78\n▇▁▁▁▁\n\n\nENDOTHELIN\n44\n0.98\n1.31\n0.54\n0.40\n0.90\n1.20\n1.60\n4.10\n▇▇▂▁▁\n\n\nALDOSTERONE\n44\n0.98\n5.61\n4.35\n1.90\n2.60\n4.40\n7.20\n32.00\n▇▂▁▁▁\n\n\neSelectin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\npSelectin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nsCort\n44\n0.98\n9.57\n3.84\n2.20\n6.80\n9.00\n11.70\n24.80\n▅▇▃▁▁\n\n\nreninRIA\n1468\n0.45\n1.47\n4.04\n0.19\n0.20\n0.40\n1.00\n34.40\n▇▁▁▁▁\n\n\nreninIRMA\n1236\n0.53\n11.39\n23.17\n4.90\n5.10\n6.70\n9.60\n321.30\n▇▁▁▁▁\n\n\nadiponectin\n80\n0.97\n5079.41\n3626.80\n694.65\n2652.82\n4045.90\n6356.52\n24487.00\n▇▃▁▁▁\n\n\nCreatinineU24hr\n2032\n0.23\n1.47\n0.51\n0.40\n1.10\n1.40\n1.70\n3.20\n▂▇▃▂▁\n\n\nAlbuminU24hr\n2034\n0.23\n31.32\n148.48\n1.60\n5.10\n7.20\n14.10\n2318.00\n▇▁▁▁▁\n\n\nCreatinineUSpot\n1361\n0.49\n151.57\n74.64\n21.00\n95.00\n145.50\n198.00\n416.00\n▆▇▅▁▁\n\n\nAlbuminUSpot\n1361\n0.49\n3.45\n12.51\n0.20\n0.51\n0.87\n1.66\n156.00\n▇▁▁▁▁\n\n\nSCrCC\n33\n0.99\n1.00\n0.23\n0.66\n0.85\n0.94\n1.13\n3.28\n▇▂▁▁▁\n\n\nSCrIDMS\n33\n0.99\n0.91\n0.24\n0.56\n0.75\n0.85\n1.04\n3.27\n▇▂▁▁▁\n\n\neGFRmdrd\n33\n0.99\n87.27\n16.85\n22.25\n76.79\n86.98\n97.72\n136.57\n▁▂▇▆▁\n\n\neGFRckdepi\n33\n0.99\n96.14\n20.05\n18.67\n83.10\n97.37\n110.41\n142.14\n▁▁▆▇▂\n\n\nDialysisEver\n30\n0.99\n0.00\n0.06\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDialysisDuration\n2648\n0.00\n10.60\n11.50\n1.00\n1.00\n6.00\n18.00\n27.00\n▇▁▁▂▂\n\n\nCKDHx\n9\n1.00\n0.04\n0.19\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nmaneuvers\n103\n0.96\n4.52\n1.43\n2.00\n3.00\n4.00\n5.00\n9.00\n▇▇▇▁▂\n\n\nasthma\n15\n0.99\n0.14\n0.48\n0.00\n0.00\n0.00\n0.00\n2.00\n▇▁▁▁▁\n\n\nFVC\n117\n0.96\n3.03\n0.82\n1.22\n2.43\n2.91\n3.54\n5.63\n▂▇▆▂▁\n\n\nFEV1\n117\n0.96\n2.44\n0.68\n0.86\n1.98\n2.37\n2.86\n4.53\n▂▇▇▃▁\n\n\nFEV6\n117\n0.96\n2.97\n0.80\n1.18\n2.39\n2.86\n3.47\n5.43\n▂▇▆▃▁\n\n\nFEV1PP\n117\n0.96\n92.74\n16.58\n39.90\n82.83\n93.32\n103.39\n140.31\n▁▃▇▅▁\n\n\nFVCPP\n117\n0.96\n91.33\n15.93\n44.77\n81.84\n91.08\n100.67\n158.76\n▁▇▇▁▁\n\n\nEF\n85\n0.97\n62.03\n6.93\n30.00\n55.00\n65.00\n65.00\n75.00\n▁▁▃▇▂\n\n\nDiastLVdia\n879\n0.67\n49.51\n4.29\n39.60\n46.60\n49.50\n52.10\n66.90\n▂▇▆▁▁\n\n\nSystLVdia\n880\n0.67\n30.10\n4.68\n21.70\n26.70\n30.00\n32.80\n50.70\n▆▇▃▁▁\n\n\nLVMecho\n881\n0.67\n147.05\n38.35\n79.17\n119.44\n142.05\n167.37\n308.46\n▆▇▃▁▁\n\n\nLVMindex\n883\n0.67\n35.51\n8.94\n19.20\n29.32\n33.97\n40.17\n74.53\n▅▇▂▁▁\n\n\nLVH\n883\n0.67\n0.06\n0.24\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nEF3cat\n85\n0.97\n0.29\n0.46\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▃▁▁\n\n\nFS\n880\n0.67\n0.04\n0.21\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nRWT\n881\n0.67\n0.35\n0.05\n0.23\n0.31\n0.34\n0.38\n0.56\n▃▇▃▁▁\n\n\necgHR\n6\n1.00\n63.57\n10.10\n43.00\n57.00\n63.00\n70.00\n98.00\n▃▇▅▂▁\n\n\nQRS\n49\n0.98\n93.64\n13.46\n72.00\n84.00\n92.00\n100.00\n162.00\n▇▇▁▁▁\n\n\nQT\n6\n1.00\n414.94\n30.60\n340.00\n394.00\n412.00\n434.00\n516.00\n▂▇▇▂▁\n\n\nConductionDefect\n0\n1.00\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMajorScarAnt\n0\n1.00\n0.01\n0.08\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMinorScarAnt\n0\n1.00\n0.01\n0.11\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nRepolarAnt\n0\n1.00\n0.04\n0.20\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMIant\n0\n1.00\n0.01\n0.08\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMajorScarPost\n0\n1.00\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMinorScarPost\n0\n1.00\n0.02\n0.15\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nRepolarPost\n0\n1.00\n0.02\n0.12\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMIpost\n0\n1.00\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMajorScarAntLat\n0\n1.00\n0.00\n0.04\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMinorScarAntLat\n0\n1.00\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nRepolarAntLat\n0\n1.00\n0.04\n0.20\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMIAntLat\n0\n1.00\n0.00\n0.05\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMIecg\n0\n1.00\n0.02\n0.15\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nAfib\n6\n1.00\n0.00\n0.04\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nAflutter\n7\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nQTcFram\n6\n1.00\n419.93\n23.12\n360.97\n404.89\n419.00\n433.85\n502.00\n▂▇▇▂▁\n\n\nQTcBaz\n6\n1.00\n424.12\n26.18\n359.67\n406.02\n423.17\n441.21\n508.91\n▂▇▇▃▁\n\n\nQTcHod\n6\n1.00\n421.20\n23.15\n367.25\n406.00\n419.50\n435.00\n507.25\n▂▇▆▂▁\n\n\nQTcFrid\n6\n1.00\n420.75\n23.01\n365.79\n405.41\n419.50\n434.51\n502.05\n▂▇▇▂▁\n\n\nCV\n6\n1.00\n1472.56\n632.70\n12.00\n1064.50\n1408.00\n1834.00\n3890.00\n▂▇▅▁▁\n\n\nLVHcv\n6\n1.00\n0.09\n0.28\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nspeechLossEver\n13\n1.00\n0.02\n0.13\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nvisionLossEver\n11\n1.00\n0.03\n0.18\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ndoubleVisionEver\n15\n0.99\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nnumbnessEver\n14\n0.99\n0.06\n0.23\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nparalysisEver\n14\n0.99\n0.03\n0.16\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ndizzynessEver\n15\n0.99\n0.04\n0.19\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nstrokeHx\n0\n1.00\n0.03\n0.16\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMIHx\n0\n1.00\n0.04\n0.20\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nCardiacProcHx\n35\n0.99\n0.06\n0.24\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nCHDHx\n0\n1.00\n0.06\n0.24\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nCarotidAngioHx\n1\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nCVDHx\n0\n1.00\n0.08\n0.27\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nPrivateIns\n9\n1.00\n0.72\n0.45\n0.00\n0.00\n1.00\n1.00\n1.00\n▃▁▁▁▇\n\n\nMedicaidIns\n10\n1.00\n0.08\n0.27\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMedicareIns\n5\n1.00\n0.22\n0.41\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nVAIns\n19\n0.99\n0.07\n0.26\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nInsuranceType\n10\n1.00\n2.33\n2.77\n0.00\n1.00\n1.00\n3.00\n15.00\n▇▂▁▁▁\n\n\nInsured\n6\n1.00\n0.87\n0.33\n0.00\n1.00\n1.00\n1.00\n1.00\n▁▁▁▁▇\n\n\nPublicIns\n5\n1.00\n0.13\n0.33\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nPublicInsType\n2317\n0.13\n2.14\n0.68\n1.00\n2.00\n2.00\n3.00\n3.00\n▃▁▇▁▅\n\n\nPrivatePublicIns\n6\n1.00\n1.25\n0.82\n0.00\n1.00\n1.00\n1.00\n3.00\n▂▇▁▂▂\n\n\ndailyDiscr\n40\n0.98\n2.12\n1.01\n1.00\n1.33\n1.89\n2.67\n5.67\n▇▅▂▁▁\n\n\nlifetimeDiscrm\n80\n0.97\n3.15\n2.10\n0.00\n1.00\n3.00\n5.00\n8.00\n▆▇▃▅▂\n\n\ndiscrmBurden\n404\n0.85\n2.34\n0.77\n1.00\n1.83\n2.17\n2.83\n4.00\n▆▇▅▇▂\n\n\ndepression\n817\n0.69\n10.49\n7.69\n0.00\n5.00\n9.00\n14.00\n41.00\n▇▆▂▁▁\n\n\nperceivedStress\n26\n0.99\n5.32\n4.37\n0.00\n2.00\n5.00\n8.00\n19.00\n▇▆▃▂▁\n\n\nweeklyStress\n1041\n0.61\n81.79\n78.63\n0.00\n27.00\n55.00\n109.00\n405.00\n▇▃▁▁▁\n\n\nIncome\n386\n0.85\n2.92\n1.01\n1.00\n2.00\n3.00\n4.00\n4.00\n▂▅▁▇▇\n\n\noccupation\n19\n0.99\n2.46\n1.70\n1.00\n1.00\n2.00\n3.00\n6.00\n▇▂▁▁▂\n\n\nedu3cat\n334\n0.87\n1.78\n0.41\n1.00\n2.00\n2.00\n2.00\n2.00\n▂▁▁▁▇\n\n\nHSgrad\n3\n1.00\n0.86\n0.35\n0.00\n1.00\n1.00\n1.00\n1.00\n▁▁▁▁▇\n\n\nPA3cat\n2\n1.00\n0.75\n0.78\n0.00\n0.00\n1.00\n1.00\n2.00\n▇▁▆▁▃\n\n\nnutrition3cat\n241\n0.91\n0.44\n0.52\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▆▁▁\n\n\nSMK3cat\n41\n0.98\n1.76\n0.65\n0.00\n2.00\n2.00\n2.00\n2.00\n▁▁▁▁▇\n\n\nidealHealthSMK\n41\n0.98\n0.87\n0.33\n0.00\n1.00\n1.00\n1.00\n1.00\n▁▁▁▁▇\n\n\nBMI3cat\n4\n1.00\n0.59\n0.71\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▂\n\n\nidealHealthBMI\n4\n1.00\n0.13\n0.34\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nidealHealthPA\n2\n1.00\n0.21\n0.41\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nidealHealthNutrition\n241\n0.91\n0.01\n0.10\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ntotChol3cat\n210\n0.92\n1.32\n0.70\n0.00\n1.00\n1.00\n2.00\n2.00\n▂▁▇▁▇\n\n\nidealHealthChol\n210\n0.92\n0.46\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nBP3cat\n10\n1.00\n1.04\n0.62\n0.00\n1.00\n1.00\n1.00\n2.00\n▂▁▇▁▃\n\n\nidealHealthBP\n10\n1.00\n0.21\n0.41\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nglucose3cat\n119\n0.96\n1.28\n0.73\n0.00\n1.00\n1.00\n2.00\n2.00\n▃▁▇▁▇\n\n\nidealHealthDM\n119\n0.96\n0.45\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▆\n\n\nvitaminD2\n1344\n0.49\n2.57\n3.84\n0.00\n0.20\n0.50\n3.50\n17.80\n▇▁▁▁▁\n\n\nvitaminD3\n60\n0.98\n13.18\n6.07\n3.30\n8.60\n11.90\n16.70\n33.30\n▆▇▅▂▁\n\n\nvitaminD3epimer\n806\n0.70\n0.52\n0.38\n0.00\n0.30\n0.40\n0.70\n2.20\n▇▅▂▁▁\n\n\ndarkgrnVeg\n107\n0.96\n0.75\n0.73\n0.01\n0.25\n0.50\n1.03\n4.27\n▇▂▁▁▁\n\n\neggs\n107\n0.96\n0.54\n0.62\n0.00\n0.09\n0.32\n0.64\n3.36\n▇▂▁▁▁\n\n\nfish\n107\n0.96\n0.18\n0.38\n0.00\n0.00\n0.09\n0.17\n2.99\n▇▁▁▁▁\n\n\nFakeCensusTractID\n65\n0.98\n134.02\n73.83\n2.00\n66.00\n135.00\n200.00\n268.00\n▅▇▇▇▅\n\n\nnbmedHHincome\n6\n1.00\n33963.00\n15406.32\n13110.00\n25229.00\n28889.00\n43510.00\n104707.00\n▇▃▂▁▁\n\n\nnbpctpoverty\n6\n1.00\n0.23\n0.12\n0.01\n0.11\n0.22\n0.32\n0.53\n▇▇▇▆▂\n\n\nnbpctBlackNH\n6\n1.00\n0.79\n0.27\n0.06\n0.64\n0.93\n0.98\n0.99\n▁▁▁▁▇\n\n\nnbpctWhiteNH\n6\n1.00\n0.20\n0.27\n0.00\n0.01\n0.06\n0.34\n0.91\n▇▁▁▁▁\n\n\nnbSESpc2score\n6\n1.00\n0.60\n0.65\n-1.74\n0.22\n0.75\n1.06\n1.62\n▁▁▅▇▇\n\n\nnbSESanascore\n6\n1.00\n-2.49\n3.72\n-8.91\n-5.48\n-3.14\n-0.40\n8.42\n▆▇▅▃▁\n\n\nnbProblems\n9\n1.00\n1.56\n0.18\n1.19\n1.37\n1.56\n1.71\n2.07\n▂▇▇▅▁\n\n\nnbCohesion\n9\n1.00\n3.02\n0.12\n2.63\n2.93\n3.03\n3.12\n3.21\n▁▂▇▇▇\n\n\nnbViolence\n10\n1.00\n1.26\n0.12\n1.03\n1.15\n1.26\n1.33\n1.76\n▆▇▃▁▁\n\n\nnbK3FavorFoodstore\n6\n1.00\n0.26\n0.21\n0.00\n0.06\n0.23\n0.43\n0.72\n▇▅▃▃▂\n\n\nnbK3paFacilities\n6\n1.00\n0.44\n0.36\n0.00\n0.18\n0.37\n0.62\n1.74\n▇▆▂▁▁\n\n\nnbpctResiden1mi\n360\n0.86\n0.31\n0.11\n0.05\n0.24\n0.32\n0.39\n0.62\n▂▆▇▃▁\n\n\nnbPopDensity1mi\n6\n1.00\n2066.75\n1262.43\n16.32\n1070.28\n2098.44\n3191.84\n4336.95\n▇▇▇▇▅\n\n\nsportIndex\n117\n0.96\n2.21\n1.24\n1.00\n1.00\n2.25\n3.25\n4.75\n▇▂▃▃▁\n\n\nhyIndex\n26\n0.99\n2.31\n0.61\n1.00\n1.86\n2.29\n2.71\n4.14\n▃▇▇▂▁\n\n\nactiveIndex\n14\n0.99\n2.12\n0.79\n1.00\n1.50\n2.00\n2.75\n4.50\n▇▇▆▂▁\n\n\n\n\n\n\n\n2.1.2. Dataset 2 jhs_analysis2 Data Summary\nBelow we use the Skimr package to create a more extension Data Summary of jhs_analysis2\n\nskim(jhs_analysis2)\n\n\nData summary\n\n\nName\njhs_analysis2\n\n\nNumber of rows\n2653\n\n\nNumber of columns\n134\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n132\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsubjid\n0\n1\n1\n4\n0\n2653\n0\n\n\nsex\n0\n1\n4\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nvisit\n0\n1.00\n2.00\n0.00\n2.00\n2.00\n2.00\n2.00\n2.00\n▁▁▇▁▁\n\n\nVisitDate\n0\n1.00\n17318.60\n324.40\n16711.00\n17023.00\n17336.00\n17590.00\n17896.00\n▆▇▆▇▆\n\n\nDaysFromV1\n0\n1.00\n1715.64\n209.55\n1351.00\n1584.00\n1664.00\n1808.00\n2707.00\n▇▇▂▁▁\n\n\nYearsFromV1\n0\n1.00\n4.70\n0.57\n3.70\n4.34\n4.56\n4.95\n7.41\n▇▇▂▁▁\n\n\nARIC\n0\n1.00\n0.29\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nrecruit\n0\n1.00\n3.12\n1.58\n1.00\n1.00\n3.00\n5.00\n5.00\n▇▂▅▆▇\n\n\nageIneligible\n3\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nFastHours\n15\n0.99\n12.27\n4.38\n1.13\n11.70\n13.32\n14.71\n21.57\n▂▁▆▇▁\n\n\nage\n0\n1.00\n58.98\n11.72\n28.21\n50.18\n58.87\n67.89\n87.76\n▁▆▇▇▂\n\n\nbrthyr\n0\n1.00\n1947.95\n12.00\n1920.00\n1939.00\n1948.00\n1957.00\n1980.00\n▂▇▇▆▁\n\n\nbrthmo\n0\n1.00\n6.40\n3.49\n1.00\n3.00\n7.00\n9.00\n12.00\n▇▅▅▅▇\n\n\nmale\n0\n1.00\n0.37\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nmenopause\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nalc\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nalcw\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ncurrentSmoker\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\neverSmoker\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nweight\n43\n0.98\n91.85\n20.88\n48.62\n77.09\n89.07\n103.24\n167.53\n▃▇▅▁▁\n\n\nheight\n30\n0.99\n169.06\n9.46\n147.32\n162.56\n167.64\n175.26\n193.04\n▂▇▆▆▂\n\n\nwaist\n3\n1.00\n102.45\n15.88\n68.58\n91.44\n101.60\n111.76\n157.48\n▃▇▆▂▁\n\n\nhip\n3\n1.00\n114.57\n14.33\n86.36\n104.14\n111.76\n121.92\n166.37\n▃▇▃▁▁\n\n\nBMI\n43\n0.98\n32.15\n6.95\n18.40\n27.27\n31.10\n35.76\n58.80\n▃▇▃▁▁\n\n\nneck\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nbsa\n43\n0.98\n2.01\n0.23\n1.47\n1.85\n2.00\n2.16\n2.75\n▂▇▇▃▁\n\n\nBPmeds\n212\n0.92\n0.69\n0.46\n0.00\n0.00\n1.00\n1.00\n1.00\n▃▁▁▁▇\n\n\nDMmedsOral\n521\n0.80\n0.18\n0.39\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nDMmedsIns\n556\n0.79\n0.08\n0.27\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDMmeds\n212\n0.92\n0.22\n0.41\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nstatinMeds\n211\n0.92\n0.32\n0.47\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nhrtMeds\n549\n0.79\n0.08\n0.28\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nbetaBlkMeds\n527\n0.80\n0.18\n0.39\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\ncalBlkMeds\n512\n0.81\n0.28\n0.45\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\ndiureticMeds\n436\n0.84\n0.52\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nantiArythMeds\n214\n0.92\n0.06\n0.24\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nmedAcct\n566\n0.79\n2.00\n0.00\n2.00\n2.00\n2.00\n2.00\n2.00\n▁▁▇▁▁\n\n\nBPmedsSelf\n218\n0.92\n0.66\n0.47\n0.00\n0.00\n1.00\n1.00\n1.00\n▅▁▁▁▇\n\n\nDMMedType\n562\n0.79\n0.31\n0.70\n0.00\n0.00\n0.00\n0.00\n3.00\n▇▂▁▁▁\n\n\ndmMedsSelf\n219\n0.92\n0.21\n0.41\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▂\n\n\nstatinMedsSelf\n223\n0.92\n0.31\n0.46\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nhrtMedsSelfEver\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nhrtMedsSelf\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nantiArythMedsSelf\n228\n0.91\n0.06\n0.23\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nsbp\n2\n1.00\n126.92\n18.03\n90.82\n114.66\n124.75\n136.67\n200.00\n▃▇▃▁▁\n\n\ndbp\n2\n1.00\n74.18\n9.83\n49.00\n67.58\n74.00\n80.03\n106.00\n▂▇▇▃▁\n\n\nBPjnc7\n2\n1.00\n0.92\n0.85\n0.00\n0.00\n1.00\n1.00\n3.00\n▆▇▁▃▁\n\n\nHTN\n0\n1.00\n0.67\n0.47\n0.00\n0.00\n1.00\n1.00\n1.00\n▃▁▁▁▇\n\n\nabi\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nHbA1c\n683\n0.74\n6.04\n1.05\n4.50\n5.50\n5.80\n6.30\n12.00\n▇▃▁▁▁\n\n\nFPG\n886\n0.67\n104.75\n30.95\n66.00\n90.00\n97.00\n107.00\n305.00\n▇▁▁▁▁\n\n\nFPG3cat\n886\n0.67\n0.52\n0.68\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▂\n\n\nHbA1c3cat\n683\n0.74\n0.82\n0.73\n0.00\n0.00\n1.00\n1.00\n2.00\n▇▁▇▁▃\n\n\nHbA1cIFCC\n683\n0.74\n42.47\n11.50\n25.68\n36.61\n39.89\n45.36\n107.66\n▇▃▁▁▁\n\n\nHbA1cIFCC3cat\n683\n0.74\n0.82\n0.73\n0.00\n0.00\n1.00\n1.00\n2.00\n▇▁▇▁▃\n\n\nfastingInsulin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nDiabetes\n495\n0.81\n0.32\n0.47\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\ndiab3cat\n495\n0.81\n1.03\n0.77\n0.00\n0.00\n1.00\n2.00\n2.00\n▆▁▇▁▆\n\n\nHOMA_B\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nHOMA_IR\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nldl\n899\n0.66\n122.08\n35.76\n44.00\n97.00\n120.00\n146.00\n243.00\n▂▇▆▂▁\n\n\nhdl\n886\n0.67\n53.47\n14.51\n27.00\n43.00\n51.00\n62.00\n109.00\n▅▇▅▁▁\n\n\ntrigs\n886\n0.67\n100.88\n59.46\n22.00\n62.00\n87.00\n124.00\n433.00\n▇▃▁▁▁\n\n\ntotchol\n886\n0.67\n195.47\n39.60\n106.00\n166.00\n193.00\n221.00\n330.00\n▂▇▇▂▁\n\n\nldl5cat\n899\n0.66\n1.31\n1.11\n0.00\n0.00\n1.00\n2.00\n4.00\n▇▇▆▂▁\n\n\nhdl3cat\n886\n0.67\n0.98\n0.78\n0.00\n0.00\n1.00\n2.00\n2.00\n▆▁▇▁▆\n\n\ntrigs4cat\n886\n0.67\n0.20\n0.53\n0.00\n0.00\n0.00\n0.00\n3.00\n▇▁▁▁▁\n\n\nhsCRP\n659\n0.75\n0.54\n0.72\n0.02\n0.12\n0.27\n0.64\n4.75\n▇▁▁▁▁\n\n\neSelectin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\npSelectin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nendothelin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nsCort\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nreninRIA\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nreninIRMA\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\naldosterone\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nleptin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nadiponectin\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nCreatinineUSpot\n657\n0.75\n155.30\n81.60\n15.00\n96.00\n145.00\n202.00\n469.00\n▅▇▃▁▁\n\n\nAlbuminUSpot\n750\n0.72\n4.24\n16.29\n0.20\n0.60\n1.00\n1.90\n179.00\n▇▁▁▁▁\n\n\nSCrCC\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nSCrIDMS\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\neGFRmdrd\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\neGFRckdepi\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nCreatinineU24hr\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nAlbuminU24hr\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ndialysisEver\n0\n1.00\n0.01\n0.07\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nDialysisDuration\n2644\n0.00\n7.33\n9.31\n1.00\n1.00\n3.00\n10.00\n27.00\n▇▁▁▁▁\n\n\nCKDHx\n2\n1.00\n0.03\n0.17\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nCAC\n612\n0.77\n142.56\n381.43\n0.00\n0.00\n0.00\n70.48\n2997.73\n▇▁▁▁▁\n\n\nAAC\n613\n0.77\n802.81\n1488.63\n0.00\n0.00\n85.48\n863.14\n8335.09\n▇▁▁▁▁\n\n\nVAT50mm\n614\n0.77\n830.19\n376.73\n112.67\n560.68\n776.89\n1050.17\n2157.67\n▃▇▅▁▁\n\n\nSAT50mm\n615\n0.77\n2348.85\n1024.66\n289.33\n1566.49\n2214.60\n3045.99\n5137.73\n▃▇▆▃▁\n\n\nanyCAC\n612\n0.77\n0.46\n0.50\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nanyAAC\n613\n0.77\n0.65\n0.48\n0.00\n0.00\n1.00\n1.00\n1.00\n▅▁▁▁▇\n\n\nspeechLossEver\n0\n1.00\n0.03\n0.16\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nvisionLossEver\n0\n1.00\n0.05\n0.22\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ndoubleVisionEver\n0\n1.00\n0.02\n0.12\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nnumbnessEver\n0\n1.00\n0.08\n0.27\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nparalysisEver\n0\n1.00\n0.04\n0.19\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\ndizzynessEver\n0\n1.00\n0.06\n0.23\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nstrokeHx\n1\n1.00\n0.04\n0.21\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nMIHx\n1\n1.00\n0.01\n0.11\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nCardiacProcHx\n3\n1.00\n0.05\n0.21\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nCHDHx\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nCarotidAngioHx\n2594\n0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▁▇▁▁\n\n\nCVDHx\n1\n1.00\n0.05\n0.21\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nSMK3cat\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nidealHealthSMK\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nBMI3cat\n43\n0.98\n0.55\n0.69\n0.00\n0.00\n0.00\n1.00\n2.00\n▇▁▅▁▂\n\n\nidealHealthBMI\n43\n0.98\n0.12\n0.32\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nPA3cat\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nidealHealthPA\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nnutrition3cat\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\nidealHealthNutrition\n2653\n0.00\nNaN\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\ntotChol3cat\n969\n0.63\n1.21\n0.67\n0.00\n1.00\n1.00\n2.00\n2.00\n▂▁▇▁▆\n\n\nidealHealthChol\n969\n0.63\n0.35\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nBP3cat\n83\n0.97\n0.90\n0.58\n0.00\n1.00\n1.00\n1.00\n2.00\n▃▁▇▁▂\n\n\nidealHealthBP\n83\n0.97\n0.13\n0.33\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nglucose3cat\n796\n0.70\n1.04\n0.69\n0.00\n1.00\n1.00\n2.00\n2.00\n▃▁▇▁▃\n\n\nidealHealthDM\n796\n0.70\n0.26\n0.44\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▃\n\n\nFakeCensusTractID\n78\n0.97\n133.27\n72.10\n2.00\n66.00\n135.00\n196.00\n268.00\n▅▇▇▇▅\n\n\nnbmedHHincome\n4\n1.00\n37671.27\n17910.41\n14502.00\n27521.00\n34103.00\n43782.00\n114583.00\n▇▆▁▁▁\n\n\nnbpctpoverty\n4\n1.00\n0.23\n0.14\n0.02\n0.12\n0.20\n0.30\n0.58\n▇▇▇▃▂\n\n\nnbpctBlackNH\n4\n1.00\n0.78\n0.28\n0.08\n0.56\n0.96\n0.98\n1.00\n▁▁▁▁▇\n\n\nnbpctWhiteNH\n4\n1.00\n0.19\n0.27\n0.00\n0.01\n0.02\n0.37\n0.88\n▇▁▁▁▁\n\n\nnbSESpc2score\n4\n1.00\n0.30\n0.67\n-2.33\n-0.09\n0.31\n0.79\n1.51\n▁▁▅▇▆\n\n\nnbSESanascore\n4\n1.00\n-2.15\n3.53\n-8.85\n-4.18\n-2.53\n0.28\n9.65\n▅▇▇▂▁\n\n\nnbProblems\n10\n1.00\n1.54\n0.18\n1.13\n1.37\n1.56\n1.71\n2.06\n▂▇▆▅▁\n\n\nnbCohesion\n10\n1.00\n3.02\n0.12\n2.63\n2.94\n3.03\n3.12\n3.21\n▁▂▆▇▇\n\n\nnbViolence\n11\n1.00\n1.25\n0.12\n1.03\n1.15\n1.24\n1.32\n1.64\n▇▅▇▂▁\n\n\nnbK3FavorFoodstore\n5\n1.00\n0.28\n0.26\n0.00\n0.04\n0.22\n0.48\n0.96\n▇▃▂▂▁\n\n\nnbK3paFacilities\n5\n1.00\n0.63\n0.50\n0.00\n0.20\n0.54\n0.98\n2.01\n▇▆▅▂▁\n\n\nnbpctResiden1mi\n419\n0.84\n0.32\n0.12\n0.04\n0.24\n0.32\n0.39\n0.63\n▂▆▇▃▁\n\n\nnbPopDensity1mi\n4\n1.00\n1768.67\n1088.93\n15.02\n946.59\n1744.26\n2701.66\n3839.42\n▇▇▇▇▅\n\n\n\n\n\n\n\n\n2.2. Data Processing\nWe will select the most relevant attributes that might contribute to increase Health care risk factor such as stroke and other cardiovascular diseases.\nsubjid: participant ID\nBPmeds; categorical variable with categories 1 = takes blood pressure lowering medication, 0 = does not take blood pressure lowering drugs\nwaist: waist circumference in cm.\nage: age in years\nHbA1c: Hemoglobin A1c, measures the average blood sugar (glucose)\nBPjnc7: categorical variable with categories where 0=normotensive, 1=pre-hypertensive, 2=Stage I hypertension, 3=Stage II hypertension\ncurrentSmoker: categorical variable with categories (1 = current smoker, 0 = not current smoker)\neverSmoker: categorical variable with categories (1 = has smoked at least 200 cigarettes in their lifetime, 0 = has not smoked more than 200 cigarettes)\nHTN: categorical variable for Hypertension Status where 1 = yes, 0 = no\ntotchol: Fasting Total Cholesterol\nBMI: Body mass index\nLVMecho: Left Ventricular Mass in grams by Echocardiography\nHSgrad: categorical variable for High School Graduate where 0=did not graduate high school, 1=graduated high school\nBMI3cat: categorize health status based on BMI where 0=poor health, 1=intermediate health, 2=ideal health\nnutrition3cat: health status as defined by nutrition from self-reported food inventories where 0=poor health, 1=intermediate health, 2=ideal health\nidealHealthBP blood pressure where 1=ideal health, 0=not ideal health\nidealHealthSMK: smoking status where 1=ideal health, 0=not ideal health\nidealHealthDM: diabetes where 1=ideal health, 0=not ideal health\nidealHealthNutrition: diet where 1=ideal health, 0=not ideal health\nidealHealthPA: physical activity where 1=ideal health, 0=not ideal health\nidealHealthBMI: obesity where 1=ideal health, 0=not ideal health\nidealHealthChol: high cholesterol where 1=ideal health, 0=not ideal health\nPrivatePublicIns: Indicator of health by Public or Private insurance where 0=uninsured, 1=private insurance only, 2=public insurance only, 3=private and public insurances\n\nkeeps1 &lt;- c(\"subjid\", \"HbA1c\" , \"age\", \"waist\",\"BPmeds\", \"everSmoker\", \"currentSmoker\", \"BPjnc7\", \"HTN\", \"totchol\", \"BMI\", \"LVMecho\", \"HSgrad\", \"BMI3cat\", \"nutrition3cat\", \"idealHealthBP\", \"idealHealthSMK\", \"idealHealthDM\", \"idealHealthNutrition\", \"idealHealthPA\", \"idealHealthBMI\", \"idealHealthChol\", \"PrivatePublicIns\")\nkeeps2 &lt;- c(\"subjid\", \"HbA1c\" , \"age\", \"waist\",\"BPmeds\", \"everSmoker\", \"currentSmoker\", \"BPjnc7\", \"HTN\", \"totchol\", \"BMI\", \"BMI3cat\", \"nutrition3cat\", \"idealHealthBP\", \"idealHealthSMK\", \"idealHealthDM\", \"idealHealthNutrition\", \"idealHealthPA\", \"idealHealthBMI\", \"idealHealthChol\")\n\n\njhs_data1 &lt;- jhs_analysis1 %&gt;% select(all_of(keeps1)) %&gt;% na.omit()\n\n# `LVMecho`, `HSgrad`, and `PrivatePublicIns` -- dont exist in jhs_analysis2\njhs_data2 &lt;- jhs_analysis2 %&gt;% select(all_of(keeps2)) %&gt;% na.omit()\n\nThis code chunk I left for debugging, might help if u have furhter questions.\n\n# jhs_analysis1_variables\n# jhs_data1 %&gt;%\n#   count(smoking)\n\n\n2.2.1 Create Variable Smoking (binning)\nCreate a variable called smoking that combines currentSmoker (1 = current smoker, 0 = not current smoker) and everSmoker (1 = has smoked at least 200 cigarettes in their lifetime, 0 = has not smoked more than 200 cigarettes) into a three level variable, where 1 = current smoker, 2 = former smoker, 3 = never smoker. This variable was only obtained at first patient visit (jhs_analysis1) so on the visit two (jhs_analysis2) it will be NaN.\n\njhs_data1 &lt;- jhs_data1 %&gt;% mutate(\n  smoking = case_when(currentSmoker==\"1\" & everSmoker==\"1\" ~ 1,\n                      currentSmoker==\"0\" & everSmoker==\"1\" ~ 2,\n                      currentSmoker==\"0\" & everSmoker==\"0\" ~ 3)\n) %&gt;%\n  dummy_cols(select_columns = \"smoking\")"
  },
  {
    "objectID": "posts/renan-blog-post-5/index.html#calculate-the-t-and-d-weights",
    "href": "posts/renan-blog-post-5/index.html#calculate-the-t-and-d-weights",
    "title": "The Jackson Heart Study (JHS) Dataset",
    "section": "3. Calculate the t and d weights",
    "text": "3. Calculate the t and d weights\nTypical-Weight (TW)\nThe Typical-Weight (\\(\\text{TW}\\)) measures how important or typical a pattern is to the class of interest. It is the ratio of the pattern’s frequency (support) in the target class to its overall frequency in the entire dataset:\n\\[\\text{TW}(P, C) = \\frac{\\text{Support}(P \\text{ in } C)}{\\text{Support}(P \\text{ in } D)}\\]\nA high Typical-Weight (close to \\(1\\)) means the pattern is highly typical of the class.\nFor example, the pattern BPmeds has a \\(\\text{Typical-Weight}\\) of \\(\\mathbf{0.937}\\). This means that \\(93.7\\%\\) of all individuals who take BP medication are in the Hypertension (\\(\\text{HTN}=1\\)) class.\nDiscriminative-Weight (DW)\nThe Discriminative-Weight (\\(\\text{DW}\\)) measures how well a pattern distinguishes the class of interest from all other classes. It is calculated as the difference between the pattern’s frequency in the target class and its maximum frequency in any other class:\n\\[\\text{DW}(P, C) = \\text{Support}(P \\text{ in } C) - \\max_{C' \\neq C} \\{\\text{Support}(P \\text{ in } C')\\}\\]\nA positive Discriminative-Weight means the pattern occurs more frequently with the target class than with any other class, making it a good discriminator.\nFor example, the pattern BPmeds has a \\(\\text{Discriminative-Weight}\\) of \\(\\mathbf{0.39}\\). This means that the fraction of the total population who are Hypertensive and take BP meds is \\(39%\\) percentage points higher than the fraction of the total population who are not Hypertensive and take BP meds, making it the strongest positive indicator for \\(\\text{HTN}=1\\).\n\n\n\nPattern\nTypical_Weight\nDiscriminative_Weight\n\n\n\n\nsmoking_1\n0.4459459\n-0.01107266\n\n\nsmoking_2\n0.5644444\n0.02006920\n\n\nsmoking_3\n0.4589552\n-0.06089965\n\n\nBPmeds\n0.9379845\n0.39100346\n\n\nidealHealthBP\n0.0000000\n-0.26228374\n\n\n\n\n# --- 1. Define the Target Class and Patterns ---\nTARGET_CLASS &lt;- 1  # HTN = 1 (Hypertension)\nPATTERNS &lt;- c(\"smoking_1\", \"smoking_2\", \"smoking_3\", \"BPmeds\", \"idealHealthBP\")\n\n# --- 2. Initialize a data frame to store results ---\nweights_df &lt;- data.frame(\n  Pattern = PATTERNS,\n  Typical_Weight = NA,\n  Discriminative_Weight = NA\n)\n\nN_total &lt;- nrow(jhs_data1)\n\n# --- 3. Loop through each pattern and calculate weights ---\nfor (i in 1:length(PATTERNS)) {\n  P &lt;- PATTERNS[i]\n\n  # Ensure the pattern column is numeric (0/1) for summation\n  jhs_data1[[P]] &lt;- as.numeric(as.character(jhs_data1[[P]]))\n\n  # A. Calculate Supports (normalized by total N)\n  # Total support for the pattern S(P)\n  count_P &lt;- sum(jhs_data1[[P]], na.rm = TRUE)\n  S_P &lt;- count_P / N_total\n\n  # Support for the pattern in the target class S(P in C)\n  count_P_in_class &lt;- sum(jhs_data1[[P]][jhs_data1$HTN == TARGET_CLASS], na.rm = TRUE)\n  S_P_in_class &lt;- count_P_in_class / N_total\n\n  # Support for the pattern in the non-target class S(P in not C)\n  count_P_in_not_class &lt;- sum(jhs_data1[[P]][jhs_data1$HTN != TARGET_CLASS], na.rm = TRUE)\n  S_P_in_not_class &lt;- count_P_in_not_class / N_total\n\n  # B. Calculate Typical-Weight (TW)\n  # TW(P, C) = S(P in C) / S(P)\n  TW &lt;- ifelse(S_P &gt; 0, S_P_in_class / S_P, 0)\n\n  # C. Calculate Discriminative-Weight (DW)\n  # DW(P, C) = S(P in C) - S(P in not C) (since only two classes)\n  DW &lt;- S_P_in_class - S_P_in_not_class\n\n  # D. Store Results\n  weights_df[i, \"Typical_Weight\"] &lt;- TW\n  weights_df[i, \"Discriminative_Weight\"] &lt;- DW\n}\n\n# --- 4. Display the results ---\nprint(weights_df)\n\n        Pattern Typical_Weight Discriminative_Weight\n1     smoking_1      0.4459459           -0.01107266\n2     smoking_2      0.5644444            0.02006920\n3     smoking_3      0.4589552           -0.06089965\n4        BPmeds      0.9379845            0.39100346\n5 idealHealthBP      0.0000000           -0.26228374\n\n\n\nReferences"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Literature Review",
    "section": "",
    "text": "These are the literature review done by all the students during this semester.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdding Binning\n\n\n\nbinita\n\n\n\nAdding Binning to dataset to decrease Granularity\n\n\n\n\n\nBinita Subedi\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Risk Dataset\n\n\n\nrenan\n\n\n\nSwitched datasets so in this project will be replicating all assingments\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nHouse Dataset Exploration\n\n\n\nrenan\n\n\n\nExploring House price datasets for CAP-6771 (Fall 2025)\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nProject 2 - Data Cubbing and Binning\n\n\n\nrenan\n\n\n\nIn this post apply binning to pre-process the dataset\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nThe Jackson Heart Study (JHS) Dataset\n\n\n\nrenan\n\n\n\nAfter further evaluation decided to switch to The Jackson Heart Study (JHS) dataset\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nThe Jackson Heart Study (JHS) Dataset\n\n\n\nrenan\n\n\n\nAfter further evaluation decided to switch to The Jackson Heart Study (JHS) dataset\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the Project\n\n\n\nrenan\n\n\n\nFirst post for course CAP-6771 (Fall 2025)\n\n\n\n\n\nRenan monteiro barbosa\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "House Price Forecasting",
    "section": "",
    "text": "The report goes here need some work\nSlides: slides.html"
  },
  {
    "objectID": "report.html#introduction",
    "href": "report.html#introduction",
    "title": "House Price Forecasting",
    "section": "Introduction",
    "text": "Introduction\nNeed to add report stuff\n\nReferences"
  },
  {
    "objectID": "people/index.html",
    "href": "people/index.html",
    "title": "Meet the Group",
    "section": "",
    "text": "Graduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | University of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Cyber Security\n\n\n\n\n\neducation\n\n\nMSC Cyber Security | University of West Florida\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people/index.html#graduate-students",
    "href": "people/index.html#graduate-students",
    "title": "Meet the Group",
    "section": "",
    "text": "Graduate Student, Masters Data Science\n\n\n\n\n\neducation\n\n\nB.S. Mechanical Engineering | University of West Florida\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduate Student, Masters Cyber Security\n\n\n\n\n\neducation\n\n\nMSC Cyber Security | University of West Florida\n\n\n\n\n\n\n\n\nNo matching items"
  }
]